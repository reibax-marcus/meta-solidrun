From 6d7b9331836c8e134763869f57d974ca2349fe9d Mon Sep 17 00:00:00 2001
From: Sergey Temerkhanov <s.temerkhanov@gmail.com>
Date: Wed, 4 Nov 2015 17:54:48 -0800
Subject: [PATCH 0339/1239] cavium: vnic: Update VNIC for Pass 2.0

This commit adds support for Pass 2.0 ThunderX chips
---
 drivers/net/cavium/nic.h          | 321 +++++++++++------
 drivers/net/cavium/nic_main.c     | 414 +++++++++++++--------
 drivers/net/cavium/nic_reg.h      |  63 ++--
 drivers/net/cavium/nicvf_main.c   | 386 +++++++++-----------
 drivers/net/cavium/nicvf_queues.c | 579 +++++++++++++-----------------
 drivers/net/cavium/nicvf_queues.h | 182 +++++-----
 drivers/net/cavium/q_struct.h     | 574 ++++++++++++++---------------
 drivers/net/cavium/thunder_bgx.c  | 504 +++++++++++++-------------
 drivers/net/cavium/thunder_bgx.h  | 154 ++++----
 drivers/net/cavium/thunderx_smi.c |   4 +
 10 files changed, 1636 insertions(+), 1545 deletions(-)

diff --git a/drivers/net/cavium/nic.h b/drivers/net/cavium/nic.h
index 32ed44148b..5f02d3fbc7 100644
--- a/drivers/net/cavium/nic.h
+++ b/drivers/net/cavium/nic.h
@@ -12,18 +12,19 @@
 
 #include <linux/netdevice.h>
 #include "thunder_bgx.h"
-#include <asm/io.h>
 
 /**
  * Macro to get the physical address of a CSR on a node
  */
-#define CSR_PA(csr, node) ((csr) | ((uint64_t)(node) << 44))
+#define CSR_PA(node, csr) ((csr) | ((uint64_t)(node) << 44))
 
 /* PCI device IDs */
 #define	PCI_DEVICE_ID_THUNDER_NIC_PF	0xA01E
 #define	PCI_DEVICE_ID_THUNDER_NIC_VF	0x0011
 #define	PCI_DEVICE_ID_THUNDER_BGX	0xA026
 
+#define	IS_PASS1(rev)	(rev == 0x0 || rev == 0x1 || rev == 0x2)
+
 /* PCI BAR nos */
 #define	PCI_CFG_REG_BAR_NUM		0
 #define	PCI_MSIX_REG_BAR_NUM		4
@@ -58,8 +59,7 @@
 
 /* Min/Max packet size */
 #define	NIC_HW_MIN_FRS			64
-#define	NIC_HW_MAX_FRS			9200
-/* 9216 max packet including FCS */
+#define	NIC_HW_MAX_FRS			9200 /* 9216 max packet including FCS */
 
 /* Max pkinds */
 #define	NIC_MAX_PKIND			16
@@ -74,25 +74,24 @@
  * ...
  * BGX1-LMAC3-CHAN0 - VNIC CHAN174
  */
-#define	NIC_INF_COUNT			2	/* No of interfaces */
+#define	NIC_INF_COUNT			2  /* No of interfaces */
 #define	NIC_CHANS_PER_INF		128
 #define	NIC_MAX_CHANS			(NIC_INF_COUNT * NIC_CHANS_PER_INF)
-/* No of channel parse indices */
-#define	NIC_CPI_COUNT			2048
+#define	NIC_CPI_COUNT			2048 /* No of channel parse indices */
 
 /* TNS bypass mode: 1-1 mapping between VNIC and BGX:LMAC */
 #define NIC_MAX_BGX				MAX_BGX_PER_CN88XX
 #define	NIC_CPI_PER_BGX			(NIC_CPI_COUNT / NIC_MAX_BGX)
-#define	NIC_MAX_CPI_PER_LMAC	64	/* Max when CPI_ALG is IP diffserv */
+#define	NIC_MAX_CPI_PER_LMAC	64 /* Max when CPI_ALG is IP diffserv */
 #define	NIC_RSSI_PER_BGX		(NIC_RSSI_COUNT / NIC_MAX_BGX)
 
 /* Tx scheduling */
 #define	NIC_MAX_TL4				1024
-#define	NIC_MAX_TL4_SHAPERS		256	/* 1 shaper for 4 TL4s */
+#define	NIC_MAX_TL4_SHAPERS		256 /* 1 shaper for 4 TL4s */
 #define	NIC_MAX_TL3				256
-#define	NIC_MAX_TL3_SHAPERS		64	/* 1 shaper for 4 TL3s */
+#define	NIC_MAX_TL3_SHAPERS		64  /* 1 shaper for 4 TL3s */
 #define	NIC_MAX_TL2				64
-#define	NIC_MAX_TL2_SHAPERS		2	/* 1 shaper for 32 TL2s */
+#define	NIC_MAX_TL2_SHAPERS		2  /* 1 shaper for 32 TL2s */
 #define	NIC_MAX_TL1				2
 
 /* TNS bypass mode */
@@ -148,13 +147,13 @@
 #define NICPF_CLK_PER_INT_TICK		43750
 
 struct nicvf_cq_poll {
-	uint8_t cq_idx;		/* Completion queue index */
+	uint8_t	cq_idx;		/* Completion queue index */
 };
 
-#define	NIC_RSSI_COUNT			4096	/* Total no of RSS indices */
+#define	NIC_RSSI_COUNT			4096 /* Total no of RSS indices */
 #define NIC_MAX_RSS_HASH_BITS		8
 #define NIC_MAX_RSS_IDR_TBL_SIZE	(1 << NIC_MAX_RSS_HASH_BITS)
-#define RSS_HASH_KEY_SIZE		5	/* 320 bit key */
+#define RSS_HASH_KEY_SIZE		5 /* 320 bit key */
 
 #ifdef VNIC_RSS_SUPPORT
 struct nicvf_rss_info {
@@ -169,9 +168,9 @@ struct nicvf_rss_info {
 #define	RSS_L3_BI_DIRECTION_ENA		(1 << 7)
 #define	RSS_L4_BI_DIRECTION_ENA		(1 << 8)
 	uint64_t cfg;
-	uint8_t hash_bits;
+	uint8_t  hash_bits;
 	uint16_t rss_size;
-	uint8_t ind_tbl[NIC_MAX_RSS_IDR_TBL_SIZE];
+	uint8_t  ind_tbl[NIC_MAX_RSS_IDR_TBL_SIZE];
 	uint64_t key[RSS_HASH_KEY_SIZE];
 };
 #endif
@@ -243,46 +242,72 @@ struct nicvf_drv_stats {
 	u64 tx_tso;
 };
 
+
 struct nicpf {
-	struct eth_device *netdev;
+	struct eth_device	*netdev;
 #define NIC_NODE_ID_MASK	0x300000000000
 #define NIC_NODE_ID(x)		((x & NODE_ID_MASK) >> 44)
-	uint8_t node;
-	unsigned int flags;
-	uint16_t total_vf_cnt;	/* Total num of VF supported */
-	uint16_t num_vf_en;	/* No of VF enabled */
-	uint64_t reg_base;	/* Register start address */
-	struct pkind_cfg pkind;
-	uint8_t bgx_cnt;
+	uint8_t			node;
+	unsigned int		flags;
+	uint16_t		total_vf_cnt;   /* Total num of VF supported */
+	uint16_t		num_vf_en;      /* No of VF enabled */
+	uint64_t		reg_base;       /* Register start address */
+	struct pkind_cfg	pkind;
+	uint8_t			bgx_cnt;
+	uint8_t			rev_id;
 #define	NIC_SET_VF_LMAC_MAP(bgx, lmac)	(((bgx & 0xF) << 4) | (lmac & 0xF))
 #define	NIC_GET_BGX_FROM_VF_LMAC_MAP(map)	((map >> 4) & 0xF)
 #define	NIC_GET_LMAC_FROM_VF_LMAC_MAP(map)	(map & 0xF)
-	uint8_t vf_lmac_map[MAX_LMAC];
-	uint16_t cpi_base[MAX_NUM_VFS_SUPPORTED];
-	uint16_t rss_ind_tbl_size;
-	uint64_t mac[MAX_NUM_VFS_SUPPORTED];
+	uint8_t			vf_lmac_map[MAX_LMAC];
+	uint16_t		cpi_base[MAX_NUM_VFS_SUPPORTED];
+	uint16_t		rss_ind_tbl_size;
+	uint64_t		mac[MAX_NUM_VFS_SUPPORTED];
+	bool			mbx_lock[MAX_NUM_VFS_SUPPORTED];
+	uint8_t			link[MAX_LMAC];
+	uint8_t			duplex[MAX_LMAC];
+	uint32_t		speed[MAX_LMAC];
+	bool			vf_enabled[MAX_NUM_VFS_SUPPORTED];
+	uint16_t		rssi_base[MAX_NUM_VFS_SUPPORTED];
+	uint8_t			lmac_cnt;
 };
 
 struct nicvf {
-	struct eth_device *netdev;
-	uint8_t vf_id;
-	uint8_t tns_mode;
-	uint8_t node;
-	uint16_t mtu;
-	struct queue_set *qs;
-	uint8_t num_qs;
-	void *addnl_qs;
-	uint16_t vf_mtu;
-	uint64_t reg_base;
-#ifdef VNIC_RSS_SUPPORT
-	struct nicvf_rss_info rss_info;
-#endif
-	uint8_t cpi_alg;
-
-	struct nicvf_hw_stats stats;
-	struct nicvf_drv_stats drv_stats;
-
-	struct nicpf *nicpf;
+	struct eth_device	*netdev;
+	uint8_t			vf_id;
+	bool                    sqs_mode:1;
+	bool			loopback_supported:1;
+	uint8_t			tns_mode;
+	uint8_t			node;
+	uint16_t		mtu;
+	struct queue_set	*qs;
+	uint8_t			num_qs;
+	void			*addnl_qs;
+	uint16_t		vf_mtu;
+	uint64_t		reg_base;
+	struct nicvf_cq_poll	*napi[8];
+
+	uint8_t			cpi_alg;
+
+	struct nicvf_hw_stats   stats;
+	struct nicvf_drv_stats  drv_stats;
+
+	struct nicpf		*nicpf;
+
+	/* VF <-> PF mailbox communication */
+	bool			pf_acked;
+	bool			pf_nacked;
+	bool			set_mac_pending;
+
+	bool			link_up;
+	uint8_t			duplex;
+	uint32_t		speed;
+	uint8_t			rev_id;
+	uint8_t			rx_queues;
+	uint8_t			tx_queues;
+
+	bool			open;
+	bool			rb_alloc_fail;
+	void 			*rcv_buf;
 };
 
 /* PF <--> VF Mailbox communication
@@ -292,126 +317,184 @@ struct nicvf {
  */
 
 /* PF <--> VF mailbox communication */
-#define	NIC_PF_VF_MAILBOX_SIZE		8
-#define	NIC_PF_VF_MBX_TIMEOUT		2000	/* ms */
+#define	NIC_PF_VF_MAILBOX_SIZE		2
+#define	NIC_PF_VF_MBX_TIMEOUT		2000 /* ms */
 
 /* Mailbox message types */
-/* Is PF ready to rcv msgs */
-#define	NIC_PF_VF_MSG_READY		0x01	/* Is PF ready to rcv msgs */
-#define	NIC_PF_VF_MSG_ACK		0x02	/* ACK the message received */
-#define	NIC_PF_VF_MSG_NACK		0x03	/* NACK the message received */
-#define	NIC_PF_VF_MSG_QS_CFG		0x04	/* Configure Qset */
-#define	NIC_PF_VF_MSG_RQ_CFG		0x05	/* Configure receive queue */
-#define	NIC_PF_VF_MSG_SQ_CFG		0x06	/* Configure Send queue */
-#define	NIC_PF_VF_MSG_RQ_DROP_CFG	0x07	/* Configure receive queue */
-#define	NIC_PF_VF_MSG_SET_MAC		0x08	/* Add MAC ID to DMAC filter */
-#define	NIC_PF_VF_MSG_SET_MAX_FRS	0x09	/* Set max frame size */
-#define	NIC_PF_VF_MSG_CPI_CFG		0x0A	/* Config CPI, RSSI */
-#define	NIC_PF_VF_MSG_RSS_SIZE		0x0B	/* Get RSS indir_tbl size */
-#define	NIC_PF_VF_MSG_RSS_CFG		0x0C	/* Config RSS table */
-#define	NIC_PF_VF_MSG_RSS_CFG_CONT	0x0D	/* RSS config continuation */
-#define	NIC_PF_VF_MSG_RQ_BP_CFG		0x0E
-#define	NIC_PF_VF_MSG_RQ_SW_SYNC	0x0F
+#define	NIC_MBOX_MSG_READY		0x01	/* Is PF ready to rcv msgs */
+#define	NIC_MBOX_MSG_ACK		0x02	/* ACK the message received */
+#define	NIC_MBOX_MSG_NACK		0x03	/* NACK the message received */
+#define	NIC_MBOX_MSG_QS_CFG		0x04	/* Configure Qset */
+#define	NIC_MBOX_MSG_RQ_CFG		0x05	/* Configure receive queue */
+#define	NIC_MBOX_MSG_SQ_CFG		0x06	/* Configure Send queue */
+#define	NIC_MBOX_MSG_RQ_DROP_CFG	0x07	/* Configure receive queue */
+#define	NIC_MBOX_MSG_SET_MAC		0x08	/* Add MAC ID to DMAC filter */
+#define	NIC_MBOX_MSG_SET_MAX_FRS	0x09	/* Set max frame size */
+#define	NIC_MBOX_MSG_CPI_CFG		0x0A	/* Config CPI, RSSI */
+#define	NIC_MBOX_MSG_RSS_SIZE		0x0B	/* Get RSS indir_tbl size */
+#define	NIC_MBOX_MSG_RSS_CFG		0x0C	/* Config RSS table */
+#define	NIC_MBOX_MSG_RSS_CFG_CONT	0x0D	/* RSS config continuation */
+#define	NIC_MBOX_MSG_RQ_BP_CFG		0x0E	/* RQ backpressure config */
+#define	NIC_MBOX_MSG_RQ_SW_SYNC		0x0F	/* Flush inflight pkts to RQ */
+#define	NIC_MBOX_MSG_BGX_STATS		0x10	/* Get stats from BGX */
+#define	NIC_MBOX_MSG_BGX_LINK_CHANGE	0x11	/* BGX:LMAC link status */
+#define	NIC_MBOX_MSG_ALLOC_SQS		0x12	/* Allocate secondary Qset */
+#define	NIC_MBOX_MSG_NICVF_PTR		0x13	/* Send nicvf ptr to PF */
+#define	NIC_MBOX_MSG_PNICVF_PTR		0x14	/* Get primary qset nicvf ptr */
+#define	NIC_MBOX_MSG_SNICVF_PTR		0x15	/* Send sqet nicvf ptr to PVF */
+#define	NIC_MBOX_MSG_LOOPBACK		0x16	/* Set interface in loopback */
+#define	NIC_MBOX_MSG_CFG_DONE		0xF0	/* VF configuration done */
+#define	NIC_MBOX_MSG_SHUTDOWN		0xF1	/* VF is being shutdown */
 
 struct nic_cfg_msg {
-	uint64_t vf_id;
-	uint64_t tns_mode;
-	uint64_t mac_addr;
-	uint64_t node_id;
+	u8    msg;
+	u8    vf_id;
+	u8    node_id;
+	bool  tns_mode:1;
+	bool  sqs_mode:1;
+	bool  loopback_supported:1;
+	u8    mac_addr[6];
 };
 
 /* Qset configuration */
 struct qs_cfg_msg {
-	uint64_t num;
-	uint64_t cfg;
+	u8    msg;
+	u8    num;
+	u8    sqs_count;
+	u64   cfg;
 };
 
 /* Receive queue configuration */
 struct rq_cfg_msg {
-	uint64_t qs_num;
-	uint64_t rq_num;
-	uint64_t cfg;
+	u8    msg;
+	u8    qs_num;
+	u8    rq_num;
+	u64   cfg;
 };
 
 /* Send queue configuration */
 struct sq_cfg_msg {
-	uint64_t qs_num;
-	uint64_t sq_num;
-	uint64_t cfg;
+	u8    msg;
+	u8    qs_num;
+	u8    sq_num;
+	bool  sqs_mode;
+	u64   cfg;
 };
 
 /* Set VF's MAC address */
 struct set_mac_msg {
-	uint64_t vf_id;
-	uint64_t addr;
+	u8    msg;
+	u8    vf_id;
+	u8    mac_addr[6];
 };
 
 /* Set Maximum frame size */
 struct set_frs_msg {
-	uint64_t vf_id;
-	uint64_t max_frs;
+	u8    msg;
+	u8    vf_id;
+	u16   max_frs;
 };
 
 /* Set CPI algorithm type */
 struct cpi_cfg_msg {
-	uint64_t vf_id;
-	uint64_t rq_cnt;
-	uint64_t cpi_alg;
+	u8    msg;
+	u8    vf_id;
+	u8    rq_cnt;
+	u8    cpi_alg;
 };
 
-#ifdef VNIC_RSS_SUPPORT
 /* Get RSS table size */
 struct rss_sz_msg {
-	uint64_t vf_id;
-	uint64_t ind_tbl_size;
+	u8    msg;
+	u8    vf_id;
+	u16   ind_tbl_size;
 };
 
 /* Set RSS configuration */
 struct rss_cfg_msg {
-	uint8_t vf_id;
-	uint8_t hash_bits;
-	uint16_t tbl_len;
-	uint16_t tbl_offset;
-#define RSS_IND_TBL_LEN_PER_MBX_MSG	42
-	uint8_t ind_tbl[RSS_IND_TBL_LEN_PER_MBX_MSG];
+	u8    msg;
+	u8    vf_id;
+	u8    hash_bits;
+	u8    tbl_len;
+	u8    tbl_offset;
+#define RSS_IND_TBL_LEN_PER_MBX_MSG	8
+	u8    ind_tbl[RSS_IND_TBL_LEN_PER_MBX_MSG];
+};
+
+struct bgx_stats_msg {
+	u8    msg;
+	u8    vf_id;
+	u8    rx;
+	u8    idx;
+	u64   stats;
+};
+
+/* Physical interface link status */
+struct bgx_link_status {
+	u8    msg;
+	u8    link_up;
+	u8    duplex;
+	u32   speed;
+};
+
+#ifdef VNIC_MULTI_QSET_SUPPORT
+/* Get Extra Qset IDs */
+struct sqs_alloc {
+	u8    msg;
+	u8    vf_id;
+	u8    qs_count;
+};
+
+struct nicvf_ptr {
+	u8    msg;
+	u8    vf_id;
+	bool  sqs_mode;
+	u8    sqs_id;
+	u64   nicvf;
 };
 #endif
 
-/* Maximum 8 64bit locations */
-struct nic_mbx {
-#define	NIC_PF_VF_MBX_MSG_MASK		0xFFFF
-	uint16_t msg;
-#define	NIC_PF_VF_MBX_LOCK_OFFSET	0
-#define	NIC_PF_VF_MBX_LOCK_VAL(x)	((x >> 16) & 0xFFFF)
-#define	NIC_PF_VF_MBX_LOCK_CLEAR(x)	(x & ~(0xFFFF0000))
-#define	NIC_PF_VF_MBX_LOCK_SET(x)\
-	(NIC_PF_VF_MBX_LOCK_CLEAR(x) | (1 << 16))
-	uint16_t mbx_lock;
-	uint32_t unused;
-	union {
-		struct nic_cfg_msg nic_cfg;
-		struct qs_cfg_msg qs;
-		struct rq_cfg_msg rq;
-		struct sq_cfg_msg sq;
-		struct set_mac_msg mac;
-		struct set_frs_msg frs;
-		struct cpi_cfg_msg cpi_cfg;
-#ifdef VNIC_RSS_SUPPORT
-		struct rss_sz_msg rss_size;
-		struct rss_cfg_msg rss_cfg;
+/* Set interface in loopback mode */
+struct set_loopback {
+	u8    msg;
+	u8    vf_id;
+	bool  enable;
+};
+/* 128 bit shared memory between PF and each VF */
+union nic_mbx {
+	struct { u8 msg; }	msg;
+	struct nic_cfg_msg	nic_cfg;
+	struct qs_cfg_msg	qs;
+	struct rq_cfg_msg	rq;
+	struct sq_cfg_msg	sq;
+	struct set_mac_msg	mac;
+	struct set_frs_msg	frs;
+	struct cpi_cfg_msg	cpi_cfg;
+	struct rss_sz_msg	rss_size;
+	struct rss_cfg_msg	rss_cfg;
+	struct bgx_stats_msg    bgx_stats;
+	struct bgx_link_status  link_status;
+#ifdef VNIC_MULTI_QSET_SUPPORT
+	struct sqs_alloc        sqs_alloc;
+	struct nicvf_ptr	nicvf;
 #endif
-		uint64_t rsvd[6];
-	} data;
-	uint64_t mbx_trigger_intr;
+	struct set_loopback	lbk;
 };
 
 int nicvf_set_real_num_queues(struct eth_device *netdev,
 			      int tx_queues, int rx_queues);
-int nicvf_send_msg_to_pf(struct nicvf *vf, struct nic_mbx *mbx);
+int nicvf_open(struct eth_device *netdev, bd_t *bis);
+void nicvf_stop(struct eth_device *netdev);
+int nicvf_send_msg_to_pf(struct nicvf *vf, union nic_mbx *mbx);
 void nicvf_free_pkt(struct nicvf *nic, void *pkt);
 void nicvf_update_stats(struct nicvf *nic);
 
 void nic_handle_mbx_intr(struct nicpf *nic, int vf);
 
+const u8 *bgx_get_lmac_mac(int node, int bgx_idx, int lmacid);
+void bgx_set_lmac_mac(int node, int bgx_idx, int lmacid, const u8 *mac);
+void bgx_lmac_rx_tx_enable(int node, int bgx_idx, int lmacid, bool enable);
+void bgx_lmac_internal_loopback(int node, int bgx_idx,
+				int lmac_idx, bool enable);
 
 #endif /* NIC_H */
diff --git a/drivers/net/cavium/nic_main.c b/drivers/net/cavium/nic_main.c
index a7167d0c0e..f0e190a832 100644
--- a/drivers/net/cavium/nic_main.c
+++ b/drivers/net/cavium/nic_main.c
@@ -34,11 +34,12 @@ unsigned long rounddown_pow_of_two(unsigned long n)
 	n |= n >> 16;
 	n |= n >> 32;
 
-	return n + 1;
+	return(n + 1);
 }
 
 static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg);
-static void nic_tx_channel_cfg(struct nicpf *nic, int vnic, int sq_idx);
+static void nic_tx_channel_cfg(struct nicpf *nic, u8 vnic,
+			       struct sq_cfg_msg *sq);
 static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf);
 static int nic_rcv_queue_sw_sync(struct nicpf *nic);
 
@@ -57,188 +58,265 @@ static uint64_t nic_reg_read(struct nicpf *nic, uint64_t offset)
 	return readq((void *)addr);
 }
 
-static uint64_t nic_get_mbx_addr(int vf)
+static u64 nic_get_mbx_addr(int vf)
 {
-	return NIC_PF_VF_0_127_MAILBOX_0_7 + (vf << NIC_VF_NUM_SHIFT);
+	return NIC_PF_VF_0_127_MAILBOX_0_1 + (vf << NIC_VF_NUM_SHIFT);
 }
 
-static int nic_lock_mbox(struct nicpf *nic, int vf)
+
+static void nic_send_msg_to_vf(struct nicpf *nic, int vf, union nic_mbx *mbx)
 {
-	int timeout = NIC_PF_VF_MBX_TIMEOUT;
-	int sleep = 10;
-	uint64_t lock, mbx_addr;
-
-	mbx_addr = nic_get_mbx_addr(vf) + NIC_PF_VF_MBX_LOCK_OFFSET;
-	lock = NIC_PF_VF_MBX_LOCK_VAL(nic_reg_read(nic, mbx_addr));
-	while (lock) {
-		mdelay(sleep);
-		lock = NIC_PF_VF_MBX_LOCK_VAL(nic_reg_read(nic, mbx_addr));
-		timeout -= sleep;
-		if (!timeout) {
-			printf("PF couldn't lock mailbox\n");
-			return 0;
-		}
+	void __iomem *mbx_addr = (void *)(nic->reg_base + nic_get_mbx_addr(vf));
+	u64 *msg = (u64 *)mbx;
+
+	/* In first revision HW, mbox interrupt is triggerred
+	 * when PF writes to MBOX(1), in next revisions when
+	 * PF writes to MBOX(0)
+	 */
+	if (IS_PASS1(nic->rev_id)) {
+		/* see the comment for nic_reg_write()/nic_reg_read()
+		 * functions above
+		 */
+		writeq(msg[0], mbx_addr);
+		writeq(msg[1], mbx_addr + 8);
+	} else {
+		writeq(msg[1], mbx_addr + 8);
+		writeq(msg[0], mbx_addr);
 	}
-	lock = nic_reg_read(nic, mbx_addr);
-	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MBX_LOCK_SET(lock));
-	return 1;
 }
 
-void nic_release_mbx(struct nicpf *nic, int vf)
+static void nic_mbx_send_ready(struct nicpf *nic, int vf)
 {
-	uint64_t mbx_addr, lock;
+	union nic_mbx mbx = {};
+	int bgx_idx, lmac;
+	const u8 *mac;
 
-	mbx_addr = nic_get_mbx_addr(vf) + NIC_PF_VF_MBX_LOCK_OFFSET;
-	lock = nic_reg_read(nic, mbx_addr);
-	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MBX_LOCK_CLEAR(lock));
-}
+	mbx.nic_cfg.msg = NIC_MBOX_MSG_READY;
+	mbx.nic_cfg.vf_id = vf;
 
-static int nic_send_msg_to_vf(struct nicpf *nic, int vf,
-			      struct nic_mbx *mbx, bool lock_needed)
-{
-	int i;
-	uint64_t *msg;
-	uint64_t mbx_addr;
+	if (nic->flags & NIC_TNS_ENABLED)
+		mbx.nic_cfg.tns_mode = NIC_TNS_MODE;
+	else
+		mbx.nic_cfg.tns_mode = NIC_TNS_BYPASS_MODE;
 
-	if (lock_needed && (!nic_lock_mbox(nic, vf)))
-		return -1;
+	if (vf < MAX_LMAC) {
+		bgx_idx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
 
-	mbx->mbx_trigger_intr = 1;
-	msg = (uint64_t *)mbx;
-	mbx_addr = nic->reg_base + nic_get_mbx_addr(vf);
+		mac = bgx_get_lmac_mac(nic->node, bgx_idx, lmac);
+		if (mac)
+			memcpy((u8 *)&mbx.nic_cfg.mac_addr, mac, 6);
+	}
+#ifdef VNIC_MULTI_QSET_SUPPORT
+	mbx.nic_cfg.sqs_mode = (vf >= nic->num_vf_en) ? true : false;
+#endif
+	mbx.nic_cfg.node_id = nic->node;
 
-	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++)
-		writeq(*(msg + i), (void *)(mbx_addr + (i * 8)));
+	mbx.nic_cfg.loopback_supported = vf < MAX_LMAC;
 
-	if (lock_needed)
-		nic_release_mbx(nic, vf);
-	return 0;
+	nic_send_msg_to_vf(nic, vf, &mbx);
 }
 
-static void nic_mbx_send_ready(struct nicpf *nic, int vf)
-{
-	struct nic_mbx mbx = { };
-
-	/* Respond with VNIC ID */
-	mbx.msg = NIC_PF_VF_MSG_READY;
-	mbx.data.nic_cfg.vf_id = vf;
 
-	if (nic->flags & NIC_TNS_ENABLED)
-		mbx.data.nic_cfg.tns_mode = NIC_TNS_MODE;
-	else
-		mbx.data.nic_cfg.tns_mode = NIC_TNS_BYPASS_MODE;
-
-/*
-	memcpy(&mbx.data.nic_cfg.mac_addr,
-	       &nic->mac[vf], 6);
-*/
-	mbx.data.nic_cfg.node_id = nic->node;
+/* ACKs VF's mailbox message
+ * @vf: VF to which ACK to be sent
+ */
+static void nic_mbx_send_ack(struct nicpf *nic, int vf)
+{
+	union nic_mbx mbx = {};
 
-	nic_send_msg_to_vf(nic, vf, &mbx, false);
+	mbx.msg.msg = NIC_MBOX_MSG_ACK;
+	nic_send_msg_to_vf(nic, vf, &mbx);
 }
 
-static void nic_mbx_send_ack(struct nicpf *nic, int vf)
+/* NACKs VF's mailbox message that PF is not able to
+ * complete the action
+ * @vf: VF to which ACK to be sent
+ */
+static void nic_mbx_send_nack(struct nicpf *nic, int vf)
 {
-	struct nic_mbx mbx = { };
+	union nic_mbx mbx = {};
 
-	mbx.msg = NIC_PF_VF_MSG_ACK;
-	nic_send_msg_to_vf(nic, vf, &mbx, false);
+	mbx.msg.msg = NIC_MBOX_MSG_NACK;
+	nic_send_msg_to_vf(nic, vf, &mbx);
 }
 
-static void nic_mbx_send_nack(struct nicpf *nic, int vf)
+static int nic_config_loopback(struct nicpf *nic, struct set_loopback *lbk)
 {
-	struct nic_mbx mbx = { };
+	int bgx_idx, lmac_idx;
+
+	if (lbk->vf_id > MAX_LMAC)
+		return -1;
+
+	bgx_idx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lbk->vf_id]);
+	lmac_idx = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lbk->vf_id]);
 
-	mbx.msg = NIC_PF_VF_MSG_NACK;
-	nic_send_msg_to_vf(nic, vf, &mbx, false);
+	bgx_lmac_internal_loopback(nic->node, bgx_idx, lmac_idx, lbk->enable);
+
+	return 0;
 }
 
-/* Handle Mailbox messages from VF and ack the message. */
+/* Interrupt handler to handle mailbox messages from VFs */
 void nic_handle_mbx_intr(struct nicpf *nic, int vf)
 {
-	struct nic_mbx mbx = { };
-	uint64_t *mbx_data;
-	uint64_t mbx_addr;
-	uint64_t reg_addr;
-	int lmac;
+	union nic_mbx mbx = {};
+	u64 *mbx_data;
+	u64 mbx_addr;
+	u64 reg_addr;
+	u64 cfg;
+	int bgx, lmac;
 	int i;
 	int ret = 0;
 
+	nic->mbx_lock[vf] = true;
+
 	mbx_addr = nic_get_mbx_addr(vf);
-	mbx_data = (uint64_t *)&mbx;
+	mbx_data = (u64 *)&mbx;
 
 	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
 		*mbx_data = nic_reg_read(nic, mbx_addr);
 		mbx_data++;
-		mbx_addr += NIC_PF_VF_MAILBOX_SIZE;
+		mbx_addr += sizeof(u64);
 	}
 
-	mbx.msg &= NIC_PF_VF_MBX_MSG_MASK;
-
-	debug("%s: Mailbox msg %d from VF%d\n", __func__, mbx.msg, vf);
-
-	switch (mbx.msg) {
-	case NIC_PF_VF_MSG_READY:
+	debug("%s: Mailbox msg %d from VF%d\n",
+		__func__, mbx.msg.msg, vf);
+	switch (mbx.msg.msg) {
+	case NIC_MBOX_MSG_READY:
 		nic_mbx_send_ready(nic, vf);
+		if (vf < MAX_LMAC) {
+			nic->link[vf] = 0;
+			nic->duplex[vf] = 0;
+			nic->speed[vf] = 0;
+		}
 		ret = 1;
 		break;
-	case NIC_PF_VF_MSG_QS_CFG:
+	case NIC_MBOX_MSG_QS_CFG:
 		reg_addr = NIC_PF_QSET_0_127_CFG |
-		    (mbx.data.qs.num << NIC_QS_ID_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx.data.qs.cfg);
+			   (mbx.qs.num << NIC_QS_ID_SHIFT);
+		cfg = mbx.qs.cfg;
+#ifdef VNIC_MULTI_QSET_SUPPORT
+		/* Check if its a secondary Qset */
+		if (vf >= nic->num_vf_en) {
+			cfg = cfg & (~0x7FULL);
+			/* Assign this Qset to primary Qset's VF */
+			cfg |= nic->pqs_vf[vf];
+		}
+#endif
+		nic_reg_write(nic, reg_addr, cfg);
 		break;
-	case NIC_PF_VF_MSG_RQ_CFG:
+	case NIC_MBOX_MSG_RQ_CFG:
 		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_CFG |
-		    (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
-		    (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+			   (mbx.rq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.rq.cfg);
 		break;
-	case NIC_PF_VF_MSG_RQ_BP_CFG:
+	case NIC_MBOX_MSG_RQ_BP_CFG:
 		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_BP_CFG |
-		    (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
-		    (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+			   (mbx.rq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.rq.cfg);
 		break;
-	case NIC_PF_VF_MSG_RQ_SW_SYNC:
+	case NIC_MBOX_MSG_RQ_SW_SYNC:
 		ret = nic_rcv_queue_sw_sync(nic);
 		break;
-	case NIC_PF_VF_MSG_RQ_DROP_CFG:
+	case NIC_MBOX_MSG_RQ_DROP_CFG:
 		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG |
-		    (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
-		    (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+			   (mbx.rq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.rq.cfg);
 		break;
-	case NIC_PF_VF_MSG_SQ_CFG:
+	case NIC_MBOX_MSG_SQ_CFG:
 		reg_addr = NIC_PF_QSET_0_127_SQ_0_7_CFG |
-		    (mbx.data.sq.qs_num << NIC_QS_ID_SHIFT) |
-		    (mbx.data.sq.sq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx.data.sq.cfg);
-		nic_tx_channel_cfg(nic, mbx.data.qs.num, mbx.data.sq.sq_num);
+			   (mbx.sq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.sq.sq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.sq.cfg);
+		nic_tx_channel_cfg(nic, mbx.qs.num, (struct sq_cfg_msg*)&mbx.sq);
 		break;
-	case NIC_PF_VF_MSG_SET_MAC:
-		lmac = mbx.data.mac.vf_id;
-		debug("LMAC: %d, map: %x\n", lmac, nic->vf_lmac_map[lmac]);
+	case NIC_MBOX_MSG_SET_MAC:
+#ifdef VNIC_MULTI_QSET_SUPPORT
+		if (vf >= nic->num_vf_en)
+			break;
+#endif
+		lmac = mbx.mac.vf_id;
+		bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lmac]);
 		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lmac]);
+		bgx_set_lmac_mac(nic->node, bgx, lmac, mbx.mac.mac_addr);
+		break;
+	case NIC_MBOX_MSG_SET_MAX_FRS:
+		ret = nic_update_hw_frs(nic, mbx.frs.max_frs,
+					mbx.frs.vf_id);
+		break;
+	case NIC_MBOX_MSG_CPI_CFG:
+		nic_config_cpi(nic, &mbx.cpi_cfg);
+		break;
+#ifdef VNIC_RSS_SUPPORT
+	case NIC_MBOX_MSG_RSS_SIZE:
+		nic_send_rss_size(nic, vf);
+		goto unlock;
+	case NIC_MBOX_MSG_RSS_CFG:
+	case NIC_MBOX_MSG_RSS_CFG_CONT:
+		nic_config_rss(nic, &mbx.rss_cfg);
+		break;
+#endif
+	case NIC_MBOX_MSG_CFG_DONE:
+		/* Last message of VF config msg sequence */
+		nic->vf_enabled[vf] = true;
+		if (vf >= nic->lmac_cnt)
+			goto unlock;
+
+		bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+
+		bgx_lmac_rx_tx_enable(nic->node, bgx, lmac, true);
+		goto unlock;
+	case NIC_MBOX_MSG_SHUTDOWN:
+		/* First msg in VF teardown sequence */
+		nic->vf_enabled[vf] = false;
+#ifdef VNIC_MULTI_QSET_SUPPORT
+		if (vf >= nic->num_vf_en)
+			nic->sqs_used[vf - nic->num_vf_en] = false;
+		nic->pqs_vf[vf] = 0;
+#endif
+		if (vf >= nic->lmac_cnt)
+			break;
 
+		bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+
+		bgx_lmac_rx_tx_enable(nic->node, bgx, lmac, false);
 		break;
-	case NIC_PF_VF_MSG_SET_MAX_FRS:
-		ret = nic_update_hw_frs(nic, mbx.data.frs.max_frs,
-					mbx.data.frs.vf_id);
+#ifdef VNIC_MULTI_QSET_SUPPORT
+	case NIC_MBOX_MSG_ALLOC_SQS:
+		nic_alloc_sqs(nic, &mbx.sqs_alloc);
+		goto unlock;
+	case NIC_MBOX_MSG_NICVF_PTR:
+		nic->nicvf[vf] = mbx.nicvf.nicvf;
 		break;
-	case NIC_PF_VF_MSG_CPI_CFG:
-		nic_config_cpi(nic, &mbx.data.cpi_cfg);
+	case NIC_MBOX_MSG_PNICVF_PTR:
+		nic_send_pnicvf(nic, vf);
+		goto unlock;
+	case NIC_MBOX_MSG_SNICVF_PTR:
+		nic_send_snicvf(nic, &mbx.nicvf);
+		goto unlock;
+#endif
+	case NIC_MBOX_MSG_LOOPBACK:
+		ret = nic_config_loopback(nic, &mbx.lbk);
 		break;
 	default:
-		printf("Invalid msg from VF%d, msg 0x%x\n", vf, mbx.msg);
+		printf("Invalid msg from VF%d, msg 0x%x\n", vf, mbx.msg.msg);
 		break;
 	}
 
 	if (!ret)
 		nic_mbx_send_ack(nic, vf);
-	else if (mbx.msg != NIC_PF_VF_MSG_READY)
+	else if (mbx.msg.msg != NIC_MBOX_MSG_READY)
 		nic_mbx_send_nack(nic, vf);
+unlock:
+	nic->mbx_lock[vf] = false;
 }
 
+
 static int nic_rcv_queue_sw_sync(struct nicpf *nic)
 {
 	int timeout = 20;
@@ -262,8 +340,8 @@ static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf)
 {
 	uint64_t *pkind = (uint64_t *)&nic->pkind;
 	if ((new_frs > NIC_HW_MAX_FRS) || (new_frs < NIC_HW_MIN_FRS)) {
-		printf("MTU from VF%d rejected, out of range %d..%d\n",
-		       vf, NIC_HW_MIN_FRS, NIC_HW_MAX_FRS);
+		printf("Invalid MTU setting from VF%d rejected, should be between %d and %d\n",
+				vf, NIC_HW_MIN_FRS, NIC_HW_MAX_FRS);
 		return 1;
 	}
 	new_frs += ETH_HLEN;
@@ -313,6 +391,7 @@ static void nic_set_lmac_vf_mapping(struct nicpf *nic)
 	debug("bgx_count: %d\n", bgx_count);
 
 	for (bgx = 0; bgx < NIC_MAX_BGX; bgx++) {
+
 		if (!(bgx_count & (1 << bgx)))
 			continue;
 		nic->bgx_cnt++;
@@ -320,16 +399,16 @@ static void nic_set_lmac_vf_mapping(struct nicpf *nic)
 		debug("lmac_cnt: %d\n", lmac_cnt);
 		for (lmac = 0; lmac < lmac_cnt; lmac++)
 			nic->vf_lmac_map[next_bgx_lmac++] =
-			    NIC_SET_VF_LMAC_MAP(bgx, lmac);
+						NIC_SET_VF_LMAC_MAP(bgx, lmac);
 		nic->num_vf_en += lmac_cnt;
 
 		/* Program LMAC credits */
-		lmac_credit = (1ull << 1);	/* chennel credit enable */
+		lmac_credit = (1ull << 1); /* chennel credit enable */
 		lmac_credit |= (0x1ff << 2);
 		lmac_credit |= (((((48 * 1024) / lmac_cnt) -
-				  NIC_HW_MAX_FRS) / 16) << 12);
+				NIC_HW_MAX_FRS) / 16) << 12);
 		nic_reg_write(nic,
-			      NIC_PF_LMAC_0_7_CREDIT + (lmac * 8), lmac_credit);
+				NIC_PF_LMAC_0_7_CREDIT + (lmac * 8), lmac_credit);
 	}
 }
 
@@ -339,17 +418,13 @@ static void nic_init_hw(struct nicpf *nic)
 	uint64_t reg;
 	uint64_t *pkind = (uint64_t *)&nic->pkind;
 
-	/* Reset NIC, incase if driver is repeatedly inserted and removed */
-	nic_reg_write(nic, NIC_PF_SOFT_RESET, 1);
-
 	/* Enable NIC HW block */
 	nic_reg_write(nic, NIC_PF_CFG, 0x3);
 
 	/* Enable backpressure */
 	nic_reg_write(nic, NIC_PF_BP_CFG, (1ULL << 6) | 0x03);
 	nic_reg_write(nic, NIC_PF_INTF_0_1_BP_CFG, (1ULL << 63) | 0x08);
-	nic_reg_write(nic, NIC_PF_INTF_0_1_BP_CFG + (1 << 8),
-		      (1ULL << 63) | 0x09);
+	nic_reg_write(nic, NIC_PF_INTF_0_1_BP_CFG + (1 << 8), (1ULL << 63) | 0x09);
 
 	for (i = 0; i < NIC_MAX_CHANS; i++)
 		nic_reg_write(nic, NIC_PF_CHAN_0_255_TX_CFG | (i << 3), 1);
@@ -364,7 +439,7 @@ static void nic_init_hw(struct nicpf *nic)
 	} else {
 		/* Disable TNS mode on both interfaces */
 		reg = NIC_TNS_BYPASS_MODE << 7;
-		reg |= 0x08;	/* Block identifier */
+		reg |= 0x08; /* Block identifier */
 		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, reg);
 		reg &= ~0xFull;
 		reg |= 0x09;
@@ -387,34 +462,35 @@ static void nic_init_hw(struct nicpf *nic)
 	nic_reg_write(nic, NIC_PF_INTR_TIMER_CFG, NICPF_CLK_PER_INT_TICK);
 }
 
+/* Channel parse index configuration */
 static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
 {
-	uint32_t vnic, bgx, lmac, chan;
-	uint32_t padd, cpi_count = 0;
-	uint64_t cpi_base, cpi, rssi_base, rssi;
-	uint8_t qset, rq_idx = 0;
+	u32 vnic, bgx, lmac, chan;
+	u32 padd, cpi_count = 0;
+	u64 cpi_base, cpi, rssi_base, rssi;
+	u8  qset, rq_idx = 0;
 
 	vnic = cfg->vf_id;
 	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
 	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
 
-	debug("%s: vnic: %d, bgx: %d, lmac: %d\n", __func__, vnic, bgx, lmac);
-
 	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_INF);
 	cpi_base = (lmac * NIC_MAX_CPI_PER_LMAC) + (bgx * NIC_CPI_PER_BGX);
 	rssi_base = (lmac * nic->rss_ind_tbl_size) + (bgx * NIC_RSSI_PER_BGX);
 
 	/* Rx channel configuration */
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_BP_CFG | (chan << 3),
+		      (1ull << 63) | (vnic << 0));
 	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_CFG | (chan << 3),
-		      (cfg->cpi_alg << 62) | (cpi_base << 48));
+		      ((u64)cfg->cpi_alg << 62) | (cpi_base << 48));
 
 	if (cfg->cpi_alg == CPI_ALG_NONE)
 		cpi_count = 1;
-	else if (cfg->cpi_alg == CPI_ALG_VLAN)	/* 3 bits of PCP */
+	else if (cfg->cpi_alg == CPI_ALG_VLAN) /* 3 bits of PCP */
 		cpi_count = 8;
-	else if (cfg->cpi_alg == CPI_ALG_VLAN16)	/* 3 bits PCP + DEI */
+	else if (cfg->cpi_alg == CPI_ALG_VLAN16) /* 3 bits PCP + DEI */
 		cpi_count = 16;
-	else if (cfg->cpi_alg == CPI_ALG_DIFF)	/* 6bits DSCP */
+	else if (cfg->cpi_alg == CPI_ALG_DIFF) /* 6bits DSCP */
 		cpi_count = NIC_MAX_CPI_PER_LMAC;
 
 	/* RSS Qset, Qidx mapping */
@@ -423,6 +499,7 @@ static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
 	for (; rssi < (rssi_base + cfg->rq_cnt); rssi++) {
 		nic_reg_write(nic, NIC_PF_RSSI_0_4097_RQ | (rssi << 3),
 			      (qset << 3) | rq_idx);
+		rq_idx++;
 	}
 
 	rssi = 0;
@@ -432,11 +509,21 @@ static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
 		if (cfg->cpi_alg != CPI_ALG_DIFF)
 			padd = cpi % cpi_count;
 		else
-			padd = cpi % 8;	/* 3 bits CS out of 6bits DSCP */
+			padd = cpi % 8; /* 3 bits CS out of 6bits DSCP */
 
 		/* Leave RSS_SIZE as '0' to disable RSS */
-		nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi << 3),
-			      (vnic << 24) | (padd << 16) | (rssi_base + rssi));
+		if (IS_PASS1(nic->rev_id)) {
+			nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi << 3),
+				      (vnic << 24) | (padd << 16) |
+				      (rssi_base + rssi));
+		} else {
+			/* Set MPI_ALG to '0' to disable MCAM parsing */
+			nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi << 3),
+				      (padd << 16));
+			/* MPI index is same as CPI if MPI_ALG is not enabled */
+			nic_reg_write(nic, NIC_PF_MPI_0_2047_CFG | (cpi << 3),
+				      (vnic << 24) | (rssi_base + rssi));
+		}
 
 		if ((rssi + 1) >= cfg->rq_cnt)
 			continue;
@@ -449,8 +536,10 @@ static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
 			rssi = ((cpi - cpi_base) & 0x38) >> 3;
 	}
 	nic->cpi_base[cfg->vf_id] = cpi_base;
+	nic->rssi_base[cfg->vf_id] = rssi_base;
 }
 
+
 /* Transmit channel configuration (TL4 -> TL3 -> Chan)
  * VNIC0-SQ0 -> TL4(0)  -> TL4A(0) -> TL3[0] -> BGX0/LMAC0/Chan0
  * VNIC1-SQ0 -> TL4(8)  -> TL4A(2) -> TL3[2] -> BGX0/LMAC1/Chan0
@@ -461,33 +550,41 @@ static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
  * VNIC6-SQ0 -> TL4(528)  -> TL4A(132) -> TL3[132] -> BGX1/LMAC2/Chan0
  * VNIC7-SQ0 -> TL4(536)  -> TL4A(134) -> TL3[134] -> BGX1/LMAC3/Chan0
  */
-static void nic_tx_channel_cfg(struct nicpf *nic, int vnic, int sq_idx)
+static void nic_tx_channel_cfg(struct nicpf *nic, u8 vnic,
+			       struct sq_cfg_msg *sq)
 {
-	uint32_t bgx, lmac;
-	uint32_t tl2, tl3, tl4;
+	u32 bgx, lmac, chan;
+	u32 tl2, tl3, tl4;
+	u32 rr_quantum;
+	u8 sq_idx = sq->sq_num;
+	u8 pqs_vnic = vnic;
 
-	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
-	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[pqs_vnic]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[pqs_vnic]);
 
-	debug("%s: bgx: %u, lmac: %u\n", __func__, bgx, lmac);
+	/* 24 bytes for FCS, IPG and preamble */
+	rr_quantum = ((NIC_HW_MAX_FRS + 24) / 4);
 
 	tl4 = (lmac * NIC_TL4_PER_LMAC) + (bgx * NIC_TL4_PER_BGX);
 	tl4 += sq_idx;
+
 	tl3 = tl4 / (NIC_MAX_TL4 / NIC_MAX_TL3);
 	nic_reg_write(nic, NIC_PF_QSET_0_127_SQ_0_7_CFG2 |
-		      (vnic << NIC_QS_ID_SHIFT) |
-		      (sq_idx << NIC_Q_NUM_SHIFT), tl4);
+		      ((u64)vnic << NIC_QS_ID_SHIFT) |
+		      ((u32)sq_idx << NIC_Q_NUM_SHIFT), tl4);
 	nic_reg_write(nic, NIC_PF_TL4_0_1023_CFG | (tl4 << 3),
-		      (vnic << 27) | (sq_idx << 24) | (NIC_HW_MAX_FRS / 4));
+		      ((u64)vnic << 27) | ((u32)sq_idx << 24) | rr_quantum);
 
-	nic_reg_write(nic, NIC_PF_TL3_0_255_CFG | (tl3 << 3),
-		      NIC_HW_MAX_FRS / 4);
-	nic_reg_write(nic, NIC_PF_TL3_0_255_CHAN | (tl3 << 3), lmac << 4);
+	nic_reg_write(nic, NIC_PF_TL3_0_255_CFG | (tl3 << 3), rr_quantum);
+	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_INF);
+	nic_reg_write(nic, NIC_PF_TL3_0_255_CHAN | (tl3 << 3), chan);
+	/* Enable backpressure on the channel */
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_TX_CFG | (chan << 3), 1);
 
 	tl2 = tl3 >> 2;
 	nic_reg_write(nic, NIC_PF_TL3A_0_63_CFG | (tl2 << 3), tl2);
-	nic_reg_write(nic, NIC_PF_TL2_0_63_CFG | (tl2 << 3),
-		      NIC_HW_MAX_FRS / 4);
+	nic_reg_write(nic, NIC_PF_TL2_0_63_CFG | (tl2 << 3), rr_quantum);
+	/* No priorities as of now */
 	nic_reg_write(nic, NIC_PF_TL2_0_63_PRI | (tl2 << 3), 0x00);
 }
 
@@ -500,13 +597,18 @@ struct nicpf *nic_initialize(unsigned int node)
 		return nic;
 
 	/* MAP PF's configuration registers */
-	nic->reg_base = CSR_PA(NIC_PF_BAR0, node);
+	nic->reg_base = CSR_PA(node, NIC_PF_BAR0);
 	if (!nic->reg_base) {
 		printf("Cannot map config register space, aborting\n");
 		goto exit;
 	}
 
+#if 0
+	nic->node = NIC_NODE_ID(pci_resource_start(pdev, PCI_CFG_REG_BAR_NUM));
+#endif
+
 	nic->node = node;
+	nic->rev_id = 3;
 
 	/* By default set NIC in TNS bypass mode */
 	nic->flags &= ~NIC_TNS_ENABLED;
@@ -526,3 +628,5 @@ exit:
 	free(nic);
 	return NULL;
 }
+
+
diff --git a/drivers/net/cavium/nic_reg.h b/drivers/net/cavium/nic_reg.h
index 8324a14c71..c0b10e2a9a 100644
--- a/drivers/net/cavium/nic_reg.h
+++ b/drivers/net/cavium/nic_reg.h
@@ -10,6 +10,7 @@
 #ifndef NIC_REG_H
 #define NIC_REG_H
 
+
 #define   NIC_PF_REG_COUNT			29573
 #define   NIC_VF_REG_COUNT			249
 
@@ -17,8 +18,8 @@
 #define   NIC_PF_CFG				(0x0000)
 #define   NIC_PF_STATUS				(0x0010)
 
-#define   NIC_PF_INTR_TIMER_CFG		(0x0030)
-#define   NIC_PF_BIST_STATUS		(0x0040)
+#define   NIC_PF_INTR_TIMER_CFG			(0x0030)
+#define   NIC_PF_BIST_STATUS			(0x0040)
 #define   NIC_PF_SOFT_RESET			(0x0050)
 
 #define   NIC_PF_TCP_TIMER			(0x0060)
@@ -32,7 +33,7 @@
 
 #define   NIC_PF_INTF_0_1_SEND_CFG		(0x0200)
 #define   NIC_PF_INTF_0_1_BP_CFG		(0x0208)
-#define   NIC_PF_INTF_0_1_BP_DIS_0_1	(0x0210)
+#define   NIC_PF_INTF_0_1_BP_DIS_0_1		(0x0210)
 #define   NIC_PF_INTF_0_1_BP_SW_0_1		(0x0220)
 #define   NIC_PF_RBDR_BP_STATE_0_3		(0x0240)
 
@@ -98,6 +99,7 @@
 #define   NIC_PF_ECC3_DBE_ENA_W1S		(0x2718)
 
 #define   NIC_PF_CPI_0_2047_CFG			(0x200000)
+#define   NIC_PF_MPI_0_2047_CFG			(0x210000)
 #define   NIC_PF_RSSI_0_4097_RQ			(0x220000)
 #define   NIC_PF_LMAC_0_7_CFG			(0x240000)
 #define   NIC_PF_LMAC_0_7_SW_XOFF		(0x242000)
@@ -106,9 +108,9 @@
 #define   NIC_PF_CHAN_0_255_RX_CFG		(0x420000)
 #define   NIC_PF_CHAN_0_255_SW_XOFF		(0x440000)
 #define   NIC_PF_CHAN_0_255_CREDIT		(0x460000)
-#define   NIC_PF_CHAN_0_255_RX_BP_CFG	(0x480000)
+#define   NIC_PF_CHAN_0_255_RX_BP_CFG		(0x480000)
 
-#define   NIC_PF_SW_SYNC_RX				(0x490000)
+#define   NIC_PF_SW_SYNC_RX			(0x490000)
 
 #define   NIC_PF_SW_SYNC_RX_DONE		(0x490008)
 #define   NIC_PF_TL2_0_63_CFG			(0x500000)
@@ -120,18 +122,19 @@
 #define   NIC_PF_TL3_0_255_PIR			(0x640000)
 #define   NIC_PF_TL3_0_255_SW_XOFF		(0x660000)
 #define   NIC_PF_TL3_0_255_CNM_RATE		(0x680000)
-#define   NIC_PF_TL3_0_255_SH_STATUS	(0x6A0000)
+#define   NIC_PF_TL3_0_255_SH_STATUS		(0x6A0000)
 #define   NIC_PF_TL4A_0_255_CFG			(0x6F0000)
 #define   NIC_PF_TL4_0_1023_CFG			(0x800000)
 #define   NIC_PF_TL4_0_1023_SW_XOFF		(0x820000)
 #define   NIC_PF_TL4_0_1023_SH_STATUS		(0x840000)
 #define   NIC_PF_TL4A_0_1023_CNM_RATE		(0x880000)
 #define   NIC_PF_TL4A_0_1023_CNM_STATUS		(0x8A0000)
-#define   NIC_PF_VF_0_127_MAILBOX_0_7		(0x20002000)
+
+#define   NIC_PF_VF_0_127_MAILBOX_0_1		(0x20002030)
 #define   NIC_PF_VNIC_0_127_TX_STAT_0_4		(0x20004000)
 #define   NIC_PF_VNIC_0_127_RX_STAT_0_13	(0x20004100)
 #define   NIC_PF_QSET_0_127_LOCK_0_15		(0x20006000)
-#define   NIC_PF_QSET_0_127_CFG				(0x20010000)
+#define   NIC_PF_QSET_0_127_CFG			(0x20010000)
 #define   NIC_PF_QSET_0_127_RQ_0_7_CFG		(0x20010400)
 #define   NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG	(0x20010420)
 #define   NIC_PF_QSET_0_127_RQ_0_7_BP_CFG	(0x20010500)
@@ -142,35 +145,35 @@
 
 #define   NIC_PF_MSIX_VEC_0_18_ADDR		(0x000000)
 #define   NIC_PF_MSIX_VEC_0_CTL			(0x000008)
-#define   NIC_PF_MSIX_PBA_0				(0x0F0000)
+#define   NIC_PF_MSIX_PBA_0			(0x0F0000)
 
 /* Virtual function register offsets */
 #define   NIC_VNIC_CFG				(0x000020)
-#define   NIC_VF_PF_MAILBOX_0_7		(0x000100)
+#define   NIC_VF_PF_MAILBOX_0_1			(0x000130)
 #define   NIC_VF_INT				(0x000200)
 #define   NIC_VF_INT_W1S			(0x000220)
 #define   NIC_VF_ENA_W1C			(0x000240)
 #define   NIC_VF_ENA_W1S			(0x000260)
 
 #define   NIC_VNIC_RSS_CFG			(0x0020E0)
-#define   NIC_VNIC_RSS_KEY_0_4		(0x002200)
-#define   NIC_VNIC_TX_STAT_0_4		(0x004000)
-#define   NIC_VNIC_RX_STAT_0_13		(0x004100)
-#define   NIC_QSET_RQ_GEN_CFG		(0x010010)
-
-#define   NIC_QSET_CQ_0_7_CFG		(0x010400)
-#define   NIC_QSET_CQ_0_7_CFG2		(0x010408)
-#define   NIC_QSET_CQ_0_7_THRESH	(0x010410)
-#define   NIC_QSET_CQ_0_7_BASE		(0x010420)
-#define   NIC_QSET_CQ_0_7_HEAD		(0x010428)
-#define   NIC_QSET_CQ_0_7_TAIL		(0x010430)
-#define   NIC_QSET_CQ_0_7_DOOR		(0x010438)
-#define   NIC_QSET_CQ_0_7_STATUS	(0x010440)
-#define   NIC_QSET_CQ_0_7_STATUS2	(0x010448)
-#define   NIC_QSET_CQ_0_7_DEBUG		(0x010450)
-
-#define   NIC_QSET_RQ_0_7_CFG		(0x010600)
-#define   NIC_QSET_RQ_0_7_STAT_0_1	(0x010700)
+#define   NIC_VNIC_RSS_KEY_0_4			(0x002200)
+#define   NIC_VNIC_TX_STAT_0_4			(0x004000)
+#define   NIC_VNIC_RX_STAT_0_13			(0x004100)
+#define   NIC_QSET_RQ_GEN_CFG			(0x010010)
+
+#define   NIC_QSET_CQ_0_7_CFG			(0x010400)
+#define   NIC_QSET_CQ_0_7_CFG2			(0x010408)
+#define   NIC_QSET_CQ_0_7_THRESH		(0x010410)
+#define   NIC_QSET_CQ_0_7_BASE			(0x010420)
+#define   NIC_QSET_CQ_0_7_HEAD			(0x010428)
+#define   NIC_QSET_CQ_0_7_TAIL			(0x010430)
+#define   NIC_QSET_CQ_0_7_DOOR			(0x010438)
+#define   NIC_QSET_CQ_0_7_STATUS		(0x010440)
+#define   NIC_QSET_CQ_0_7_STATUS2		(0x010448)
+#define   NIC_QSET_CQ_0_7_DEBUG			(0x010450)
+
+#define   NIC_QSET_RQ_0_7_CFG			(0x010600)
+#define   NIC_QSET_RQ_0_7_STAT_0_1		(0x010700)
 
 #define   NIC_QSET_SQ_0_7_CFG			(0x010800)
 #define   NIC_QSET_SQ_0_7_THRESH		(0x010810)
@@ -224,24 +227,24 @@ struct pkind_cfg {
 #endif
 };
 
+static inline uint64_t BGXX_PF_BAR0(unsigned long param1) __attribute__ ((pure, always_inline));
 static inline uint64_t BGXX_PF_BAR0(unsigned long param1)
 {
 	assert(param1 <= 1);
 	return 0x87E0E0000000 + (param1 << 24);
 }
-
 #define BGXX_PF_BAR0_SIZE 0x400000
 
 #define NIC_PF_BAR0 0x843000000000
 #define NIC_PF_BAR0_SIZE 0x40000000
 
+static inline uint64_t NIC_VFX_BAR0(unsigned long param1) __attribute__ ((pure, always_inline));
 static inline uint64_t NIC_VFX_BAR0(unsigned long param1)
 {
 	assert(param1 <= 127);
 
 	return 0x8430A0000000 + (param1 << 21);
 }
-
 #define NIC_VFX_BAR0_SIZE 0x200000
 
 #endif /* NIC_REG_H */
diff --git a/drivers/net/cavium/nicvf_main.c b/drivers/net/cavium/nicvf_main.c
index 24d10ff495..135e0e8c87 100644
--- a/drivers/net/cavium/nicvf_main.c
+++ b/drivers/net/cavium/nicvf_main.c
@@ -23,110 +23,73 @@
 
 #define ETH_ALEN 6
 
+
 /* Register read/write APIs */
-void nicvf_reg_write(struct nicvf *nicvf, uint64_t offset, uint64_t val)
+void nicvf_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val)
 {
-	uint64_t addr = nicvf->reg_base + offset;
+	uint64_t addr = nic->reg_base + offset;
 
 	writeq(val, (void *)addr);
 }
 
-uint64_t nicvf_reg_read(struct nicvf *nicvf, uint64_t offset)
+uint64_t nicvf_reg_read(struct nicvf *nic, uint64_t offset)
 {
-	uint64_t addr = nicvf->reg_base + offset;
+	uint64_t addr = nic->reg_base + offset;
 
 	return readq((void *)addr);
 }
 
-void nicvf_queue_reg_write(struct nicvf *nicvf, uint64_t offset,
+void nicvf_queue_reg_write(struct nicvf *nic, uint64_t offset,
 			   uint64_t qidx, uint64_t val)
 {
-	uint64_t addr = nicvf->reg_base + offset;
+	uint64_t addr = nic->reg_base + offset;
 
 	writeq(val, (void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
 }
 
-uint64_t nicvf_queue_reg_read(struct nicvf *nicvf, uint64_t offset, uint64_t qidx)
+uint64_t nicvf_queue_reg_read(struct nicvf *nic, uint64_t offset, uint64_t qidx)
 {
-	uint64_t addr = nicvf->reg_base + offset;
+	uint64_t addr = nic->reg_base + offset;
 
 	return readq((void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
 }
 
-/* VF -> PF mailbox communication */
-static bool pf_ready_to_rcv_msg;
-static bool pf_acked;
-static bool pf_nacked;
-
-int nicvf_lock_mbox(struct nicvf *nicvf)
-{
-	int timeout = NIC_PF_VF_MBX_TIMEOUT;
-	int sleep = 10;
-	uint64_t lock, mbx_addr;
-
-	mbx_addr = NIC_VF_PF_MAILBOX_0_7 + NIC_PF_VF_MBX_LOCK_OFFSET;
-	lock = NIC_PF_VF_MBX_LOCK_VAL(nicvf_reg_read(nicvf, mbx_addr));
-	while (lock) {
-		mdelay(sleep);
-		lock = NIC_PF_VF_MBX_LOCK_VAL(nicvf_reg_read(nicvf, mbx_addr));
-		timeout -= sleep;
-		if (!timeout) {
-			printf("VF%d Couldn't lock mailbox\n", nicvf->vf_id);
-			return 0;
-		}
-	}
-	nicvf_reg_write(nicvf, mbx_addr, NIC_PF_VF_MBX_LOCK_SET(lock));
-	return 1;
-}
+static void  nicvf_handle_mbx_intr(struct nicvf *nic);
 
-void nicvf_release_mbx(struct nicvf *nicvf)
+/* VF -> PF mailbox communication */
+static void nicvf_write_to_mbx(struct nicvf *nic, union nic_mbx *mbx)
 {
-	uint64_t mbx_addr, lock;
+	u64 *msg = (u64 *)mbx;
 
-	mbx_addr = NIC_VF_PF_MAILBOX_0_7 + NIC_PF_VF_MBX_LOCK_OFFSET;
-	lock = nicvf_reg_read(nicvf, mbx_addr);
-	nicvf_reg_write(nicvf, mbx_addr, NIC_PF_VF_MBX_LOCK_CLEAR(lock));
+	nicvf_reg_write(nic, NIC_VF_PF_MAILBOX_0_1 + 0, msg[0]);
+	nicvf_reg_write(nic, NIC_VF_PF_MAILBOX_0_1 + 8, msg[1]);
 }
 
-static void  nicvf_handle_mbx_intr(struct nicvf *nicvf);
-
-int nicvf_send_msg_to_pf(struct nicvf *nicvf, struct nic_mbx *mbx)
+int nicvf_send_msg_to_pf(struct nicvf *nic, union nic_mbx *mbx)
 {
-	int i, timeout = NIC_PF_VF_MBX_TIMEOUT;
+	int timeout = NIC_PF_VF_MBX_TIMEOUT;
 	int sleep = 10;
-	uint64_t *msg;
-	uint64_t mbx_addr;
-
-	if (!nicvf_lock_mbox(nicvf))
-		return -1;
-
-	pf_acked = false;
-	pf_nacked = false;
-	mbx->mbx_trigger_intr = 1;
-	msg = (uint64_t *)mbx;
 
-	mbx_addr = nicvf->reg_base + NIC_VF_PF_MAILBOX_0_7;
+	nic->pf_acked = false;
+	nic->pf_nacked = false;
 
-	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++)
-		writeq(*(msg + i), (void *)(mbx_addr + (i * 8)));
+	nicvf_write_to_mbx(nic, mbx);
 
-	nicvf_release_mbx(nicvf);
-
-	nic_handle_mbx_intr(nicvf->nicpf, nicvf->vf_id);
+	nic_handle_mbx_intr(nic->nicpf, nic->vf_id);
 
 	/* Wait for previous message to be acked, timeout 2sec */
-	while (!pf_acked) {
-		if (pf_nacked)
+	while (!nic->pf_acked) {
+		if (nic->pf_nacked)
 			return -1;
 		mdelay(sleep);
-		nicvf_handle_mbx_intr(nicvf);
+		nicvf_handle_mbx_intr(nic);
 
-		if (pf_acked)
+		if (nic->pf_acked)
 			break;
 		timeout -= sleep;
 		if (!timeout) {
 			printf("PF didn't ack to mbox msg %d from VF%d\n",
-			       (mbx->msg & 0xFF), nicvf->vf_id);
+			       (mbx->msg.msg & 0xFF), nic->vf_id);
 			return -1;
 		}
 	}
@@ -134,117 +97,119 @@ int nicvf_send_msg_to_pf(struct nicvf *nicvf, struct nic_mbx *mbx)
 	return 0;
 }
 
+
 /* Checks if VF is able to comminicate with PF
 * and also gets the VNIC number this VF is associated to.
 */
-static int nicvf_check_pf_ready(struct nicvf *nicvf)
+static int nicvf_check_pf_ready(struct nicvf *nic)
 {
-	int timeout = 5000, sleep = 20;
-	uint64_t mbx_addr = NIC_VF_PF_MAILBOX_0_7;
-
-	pf_ready_to_rcv_msg = false;
-
-	nicvf_reg_write(nicvf, mbx_addr, NIC_PF_VF_MSG_READY);
+	union nic_mbx mbx = {};
 
-	mbx_addr += (NIC_PF_VF_MAILBOX_SIZE - 1) * 8;
-	nicvf_reg_write(nicvf, mbx_addr, 1ULL);
-
-	nic_handle_mbx_intr(nicvf->nicpf, nicvf->vf_id);
-
-	while (!pf_ready_to_rcv_msg) {
-		mdelay(sleep);
-		nicvf_handle_mbx_intr(nicvf);
-
-		if (pf_ready_to_rcv_msg)
-			break;
-		timeout -= sleep;
-		if (!timeout) {
-			printf("PF didn't respond to READY msg\n");
-			return 0;
-		}
+	mbx.msg.msg = NIC_MBOX_MSG_READY;
+	if (nicvf_send_msg_to_pf(nic, &mbx)) {
+		printf("PF didn't respond to READY msg\n");
+		return 0;
 	}
+
 	return 1;
 }
 
-static void nicvf_handle_mbx_intr(struct nicvf *nicvf)
+static void  nicvf_handle_mbx_intr(struct nicvf *nic)
 {
-	struct nic_mbx mbx = { };
-	uint64_t *mbx_data;
-	uint64_t mbx_addr;
+	union nic_mbx mbx = {};
+	u64 *mbx_data;
+	u64 mbx_addr;
 	int i;
 
-	mbx_addr = NIC_VF_PF_MAILBOX_0_7;
-	mbx_data = (uint64_t *)&mbx;
+	mbx_addr = NIC_VF_PF_MAILBOX_0_1;
+	mbx_data = (u64 *)&mbx;
 
 	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
-		*mbx_data = nicvf_reg_read(nicvf, mbx_addr);
-		asm volatile("dsb sy");
-
+		*mbx_data = nicvf_reg_read(nic, mbx_addr);
 		mbx_data++;
-		mbx_addr += NIC_PF_VF_MAILBOX_SIZE;
+		mbx_addr += sizeof(u64);
 	}
 
-	debug("Mbox message from PF, msg 0x%x\n", mbx.msg);
-
-	switch (mbx.msg & NIC_PF_VF_MBX_MSG_MASK) {
-	case NIC_PF_VF_MSG_READY:
-		pf_ready_to_rcv_msg = true;
-		nicvf->vf_id = mbx.data.nic_cfg.vf_id & 0x7F;
-		nicvf->tns_mode = mbx.data.nic_cfg.tns_mode & 0x7F;
-		nicvf->node = mbx.data.nic_cfg.node_id;
-
-		debug("MAC: %pM\n", nicvf->netdev->enetaddr);
+	debug("Mbox message: msg: 0x%x\n", mbx.msg.msg);
+	switch (mbx.msg.msg) {
+	case NIC_MBOX_MSG_READY:
+		nic->pf_acked = true;
+		nic->vf_id = mbx.nic_cfg.vf_id & 0x7F;
+		nic->tns_mode = mbx.nic_cfg.tns_mode & 0x7F;
+		nic->node = mbx.nic_cfg.node_id;
+		if (!nic->set_mac_pending)
+			memcpy(nic->netdev->enetaddr,
+					mbx.nic_cfg.mac_addr, 6);
+		nic->loopback_supported = mbx.nic_cfg.loopback_supported;
+		nic->link_up = false;
+		nic->duplex = 0;
+		nic->speed = 0;
 		break;
-	case NIC_PF_VF_MSG_ACK:
-		pf_acked = true;
+	case NIC_MBOX_MSG_ACK:
+		nic->pf_acked = true;
 		break;
-	case NIC_PF_VF_MSG_NACK:
-		pf_nacked = true;
+	case NIC_MBOX_MSG_NACK:
+		nic->pf_nacked = true;
+		break;
+	case NIC_MBOX_MSG_BGX_LINK_CHANGE:
+		nic->pf_acked = true;
+		nic->link_up = mbx.link_status.link_up;
+		nic->duplex = mbx.link_status.duplex;
+		nic->speed = mbx.link_status.speed;
+		if (nic->link_up) {
+			printf("%s: Link is Up %d Mbps %s\n",
+				    nic->netdev->name, nic->speed,
+				    nic->duplex == 1 ?
+				"Full duplex" : "Half duplex");
+		} else {
+			printf("%s: Link is Down\n",
+				    nic->netdev->name);
+		}
 		break;
 	default:
-		printf("Invalid message from PF, msg 0x%x\n", mbx.msg);
+		printf("Invalid message from PF, msg 0x%x\n", mbx.msg.msg);
 		break;
 	}
-	nicvf_clear_intr(nicvf, NICVF_INTR_MBOX, 0);
+
+	nicvf_clear_intr(nic, NICVF_INTR_MBOX, 0);
 }
 
-static int nicvf_hw_set_mac_addr(struct nicvf *nicvf, struct eth_device *netdev)
+static int nicvf_hw_set_mac_addr(struct nicvf *nic, struct eth_device *netdev)
 {
-	struct nic_mbx mbx = { };
-	int i;
+	union nic_mbx mbx = {};
 
-	mbx.msg = NIC_PF_VF_MSG_SET_MAC;
-	mbx.data.mac.vf_id = nicvf->vf_id;
-	for (i = 0; i < ETH_ALEN; i++)
-		mbx.data.mac.addr =
-		    (mbx.data.mac.addr << 8) | netdev->enetaddr[i];
+	mbx.mac.msg = NIC_MBOX_MSG_SET_MAC;
+	mbx.mac.vf_id = nic->vf_id;
+	memcpy(mbx.mac.mac_addr, netdev->enetaddr, 6);
 
-	return nicvf_send_msg_to_pf(nicvf, &mbx);
+	return nicvf_send_msg_to_pf(nic, &mbx);
 }
 
-void nicvf_config_cpi(struct nicvf *nicvf)
+static void nicvf_config_cpi(struct nicvf *nic)
 {
-	struct nic_mbx mbx = { };
+	union nic_mbx mbx = {};
 
-	mbx.msg = NIC_PF_VF_MSG_CPI_CFG;
-	mbx.data.cpi_cfg.vf_id = nicvf->vf_id;
-	mbx.data.cpi_cfg.cpi_alg = nicvf->cpi_alg;
-	mbx.data.cpi_cfg.rq_cnt = nicvf->qs->rq_cnt;
+	mbx.cpi_cfg.msg = NIC_MBOX_MSG_CPI_CFG;
+	mbx.cpi_cfg.vf_id = nic->vf_id;
+	mbx.cpi_cfg.cpi_alg = nic->cpi_alg;
+	mbx.cpi_cfg.rq_cnt = nic->qs->rq_cnt;
 
-	nicvf_send_msg_to_pf(nicvf, &mbx);
+	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
-static int nicvf_init_resources(struct nicvf *nicvf)
+
+static int nicvf_init_resources(struct nicvf *nic)
 {
 	int err;
 
-	nicvf->num_qs = 1;
+	nic->num_qs = 1;
 
 	/* Enable Qset */
-	nicvf_qset_config(nicvf, true);
+	nicvf_qset_config(nic, true);
 
 	/* Initialize queues and HW for data transfer */
-	err = nicvf_config_data_transfer(nicvf, true);
+	err = nicvf_config_data_transfer(nic, true);
+
 	if (err) {
 		printf("Failed to alloc/config VF's QSet resources\n");
 		return err;
@@ -252,12 +217,12 @@ static int nicvf_init_resources(struct nicvf *nicvf)
 	return 0;
 }
 
-void nicvf_free_pkt(struct nicvf *nicvf, void *pkt)
+void nicvf_free_pkt(struct nicvf *nic, void *pkt)
 {
 	free(pkt);
 }
 
-static void nicvf_snd_pkt_handler(struct nicvf *nicvf,
+static void nicvf_snd_pkt_handler(struct nicvf *nic,
 				  struct cmp_queue *cq,
 				  void *cq_desc, int cqe_type)
 {
@@ -266,21 +231,17 @@ static void nicvf_snd_pkt_handler(struct nicvf *nicvf,
 	struct sq_hdr_subdesc *hdr;
 
 	cqe_tx = (struct cqe_send_t *)cq_desc;
-	sq = &nicvf->qs->sq[cqe_tx->sq_idx];
+	sq = &nic->qs->sq[cqe_tx->sq_idx];
 
 	hdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, cqe_tx->sqe_ptr);
 	if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER)
 		return;
 
-	debug("%s Qset #%d SQ #%d SQ ptr #%d subdesc count %d\n",
-	      __func__, cqe_tx->sq_qs, cqe_tx->sq_idx,
-	      cqe_tx->sqe_ptr, hdr->subdesc_cnt);
-
-	nicvf_check_cqe_tx_errs(nicvf, cq, cq_desc);
+	nicvf_check_cqe_tx_errs(nic, cq, cq_desc);
 	nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
 }
 
-static int nicvf_rcv_pkt_handler(struct nicvf *nicvf,
+static int nicvf_rcv_pkt_handler(struct nicvf *nic,
 				 struct cmp_queue *cq, void *cq_desc,
 				 void **ppkt, int cqe_type)
 {
@@ -291,11 +252,11 @@ static int nicvf_rcv_pkt_handler(struct nicvf *nicvf,
 	int err = 0;
 
 	/* Check for errors */
-	err = nicvf_check_cqe_rx_errs(nicvf, cq, cq_desc);
+	err = nicvf_check_cqe_rx_errs(nic, cq, cq_desc);
 	if (err && !cqe_rx->rb_cnt)
 		return -1;
 
-	pkt = nicvf_get_rcv_pkt(nicvf, cq_desc, &pkt_len);
+	pkt = nicvf_get_rcv_pkt(nic, cq_desc, &pkt_len);
 	if (!pkt) {
 		debug("Packet not received\n");
 		return -1;
@@ -307,7 +268,7 @@ static int nicvf_rcv_pkt_handler(struct nicvf *nicvf,
 	return pkt_len;
 }
 
-int nicvf_cq_handler(struct nicvf *nicvf, void **ppkt, int *pkt_len)
+int nicvf_cq_handler(struct nicvf *nic, void **ppkt, int *pkt_len)
 {
 	int cq_qnum = 0;
 	int processed_sq_cqe = 0;
@@ -315,50 +276,49 @@ int nicvf_cq_handler(struct nicvf *nicvf, void **ppkt, int *pkt_len)
 	int processed_cqe = 0;
 
 	unsigned long cqe_count, cqe_head;
-	struct queue_set *qs = nicvf->qs;
+	struct queue_set *qs = nic->qs;
 	struct cmp_queue *cq = &qs->cq[cq_qnum];
 	struct cqe_rx_t *cq_desc;
 
 	/* Get num of valid CQ entries expect next one to be SQ completion */
-	cqe_count = nicvf_queue_reg_read(nicvf, NIC_QSET_CQ_0_7_STATUS, cq_qnum);
+	cqe_count = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS, cq_qnum);
 	cqe_count &= 0xFFFF;
 	if (!cqe_count)
 		return 0;
 
 	/* Get head of the valid CQ entries */
-	cqe_head =
-	    nicvf_queue_reg_read(nicvf, NIC_QSET_CQ_0_7_HEAD, cq_qnum) >> 9;
+	cqe_head = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD, cq_qnum) >> 9;
 	cqe_head &= 0xFFFF;
 
 	if (cqe_count) {
 		/* Get the CQ descriptor */
 		cq_desc = (struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head);
+		cqe_head++;
+		cqe_head &= (cq->dmem.q_len - 1);
+		/* Initiate prefetch for next descriptor */
+		prefetch((struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head));
 
 		switch (cq_desc->cqe_type) {
 		case CQE_TYPE_RX:
-			debug("%s: Got Rx CQE\n", nicvf->netdev->name);
-			*pkt_len =
-			    nicvf_rcv_pkt_handler(nicvf, cq, cq_desc, ppkt,
-						  CQE_TYPE_RX);
+			debug("%s: Got Rx CQE\n", nic->netdev->name);
+			*pkt_len = nicvf_rcv_pkt_handler(nic, cq, cq_desc, ppkt, CQE_TYPE_RX);
 			processed_rq_cqe++;
 			break;
 		case CQE_TYPE_SEND:
-			debug("%s: Got Tx CQE\n", nicvf->netdev->name);
-			nicvf_snd_pkt_handler(nicvf, cq, cq_desc, CQE_TYPE_SEND);
+			debug("%s: Got Tx CQE\n", nic->netdev->name);
+			nicvf_snd_pkt_handler(nic, cq, cq_desc, CQE_TYPE_SEND);
 			processed_sq_cqe++;
 			break;
 		default:
-			debug("%s: Got CQ type %u\n", nicvf->netdev->name,
-			      cq_desc->cqe_type);
+			debug("%s: Got CQ type %u\n", nic->netdev->name, cq_desc->cqe_type);
 			break;
 		}
 		processed_cqe++;
-		cqe_head++;
-		cqe_head &= (cq->dmem.q_len - 1);
 	}
 
+
 	/* Dequeue CQE */
-	nicvf_queue_reg_write(nicvf, NIC_QSET_CQ_0_7_DOOR,
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_DOOR,
 			      cq_qnum, processed_cqe);
 
 	asm volatile ("dsb sy");
@@ -370,47 +330,47 @@ int nicvf_cq_handler(struct nicvf *nicvf, void **ppkt, int *pkt_len)
  *
  * As of now only CQ errors are handled
  */
-void nicvf_handle_qs_err(struct nicvf *nicvf)
+void nicvf_handle_qs_err(struct nicvf *nic)
 {
-	struct queue_set *qs = nicvf->qs;
+	struct queue_set *qs = nic->qs;
 	int qidx;
 	uint64_t status;
 
 	/* Check if it is CQ err */
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
-		status = nicvf_queue_reg_read(nicvf, NIC_QSET_CQ_0_7_STATUS,
+		status = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS,
 					      qidx);
 		if (!(status & CQ_ERR_MASK))
 			continue;
 		/* Process already queued CQEs and reconfig CQ */
-		nicvf_sq_disable(nicvf, qidx);
-		nicvf_cmp_queue_config(nicvf, qs, qidx, true);
-		nicvf_sq_free_used_descs(nicvf->netdev, &qs->sq[qidx], qidx);
-		nicvf_sq_enable(nicvf, &qs->sq[qidx], qidx);
-
-		nicvf_enable_intr(nicvf, NICVF_INTR_CQ, qidx);
+		nicvf_sq_disable(nic, qidx);
+		nicvf_cmp_queue_config(nic, qs, qidx, true);
+		nicvf_sq_free_used_descs(nic->netdev, &qs->sq[qidx], qidx);
+		nicvf_sq_enable(nic, &qs->sq[qidx], qidx);
 	}
 }
 
 static int nicvf_xmit(struct eth_device *netdev, void *pkt, int pkt_len)
 {
-	struct nicvf *nicvf = netdev->priv;
+	struct nicvf *nic = netdev->priv;
 	int ret = 0;
 	int rcv_len = 0;
 	unsigned int timeout = 5000;
 	void *rpkt = NULL;
 
-	if (!nicvf_sq_append_pkt(nicvf, pkt, pkt_len)) {
-		printf("VF%d: TX ring full\n", nicvf->vf_id);
+	if (!nicvf_sq_append_pkt(nic, pkt, pkt_len)) {
+		printf("VF%d: TX ring full\n", nic->vf_id);
 		return -1;
 	}
 
 	/* check and update CQ for pkt sent */
 	while (!ret && timeout--) {
-		ret = nicvf_cq_handler(nicvf, &rpkt, &rcv_len);
+		ret = nicvf_cq_handler(nic, &rpkt, &rcv_len);
 		if (!ret)
-			debug("%s: %d, Not sent\n", __func__, __LINE__);
-		udelay(1);
+		{
+			debug("%s: %d, Not sent\n", __FUNCTION__, __LINE__);
+			udelay(10);
+		}
 	}
 
 	return 0;
@@ -418,29 +378,29 @@ static int nicvf_xmit(struct eth_device *netdev, void *pkt, int pkt_len)
 
 static int nicvf_recv(struct eth_device *netdev)
 {
-	struct nicvf *nicvf = netdev->priv;
+	struct nicvf *nic = netdev->priv;
 	void *pkt;
 	int pkt_len = 0;
-#ifdef DEBUG
+#if 0
 	u8 *dpkt;
 	int i, j;
 #endif
 
-	nicvf_cq_handler(nicvf, &pkt, &pkt_len);
+	nicvf_cq_handler(nic, &pkt, &pkt_len);
 
 	if (pkt_len) {
-#ifdef DEBUG
+#if 0
 		dpkt = pkt;
 		printf("RX packet contents:\n");
 		for (i = 0; i < 8; i++) {
 			puts("\t");
-			for (j = 0; j < 10; j++)
+			for (j = 0; j < 10; j++) {
 				printf("%02x ", dpkt[i * 10 + j]);
+			}
 			puts("\n");
 		}
 #endif
 		net_process_received_packet(pkt, pkt_len);
-		nicvf_refill_rbdr(nicvf);
 	}
 
 	return pkt_len;
@@ -448,44 +408,44 @@ static int nicvf_recv(struct eth_device *netdev)
 
 void nicvf_stop(struct eth_device *netdev)
 {
-	struct nicvf *nicvf = netdev->priv;
+	struct nicvf *nic = netdev->priv;
+
+	if (!nic->open)
+		return;
 
 	/* Free resources */
-	nicvf_config_data_transfer(nicvf, false);
+	nicvf_config_data_transfer(nic, false);
 
 	/* Disable HW Qset */
-	nicvf_qset_config(nicvf, false);
+	nicvf_qset_config(nic, false);
+
+	nic->open = false;
 }
 
 int nicvf_open(struct eth_device *netdev, bd_t *bis)
 {
 	int err;
-	struct nicvf *nicvf = netdev->priv;
-	struct nicpf *nicpf = nicvf->nicpf;
-	int vnic = nicvf->vf_id;
-
-	int bgx, lmac;
+	struct nicvf *nic = netdev->priv;
 
-	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nicpf->vf_lmac_map[vnic]);
-	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nicpf->vf_lmac_map[vnic]);
-
-	bgx_lmac_enable(bgx, lmac);
-	nicvf_hw_set_mac_addr(nicvf, netdev);
+	nicvf_hw_set_mac_addr(nic, netdev);
 
 	/* Configure CPI alorithm */
-	nicvf->cpi_alg = CPI_ALG_NONE;
-	nicvf_config_cpi(nicvf);
+	nic->cpi_alg = CPI_ALG_NONE;
+	nicvf_config_cpi(nic);
 
 	/* Initialize the queues */
-	err = nicvf_init_resources(nicvf);
+	err = nicvf_init_resources(nic);
 	if (err)
 		return -1;
 
-	if (!nicvf_check_pf_ready(nicvf))
+	if (!nicvf_check_pf_ready(nic)) {
 		return -1;
+	}
+
+	nic->open = true;
 
 	/* Make sure queue initialization is written */
-	asm volatile ("dsb sy");
+	asm volatile("dsb sy");
 
 	return 0;
 }
@@ -493,7 +453,7 @@ int nicvf_open(struct eth_device *netdev, bd_t *bis)
 int nicvf_initialize(struct nicpf *nicpf, int vf_num, unsigned int node)
 {
 	struct eth_device *netdev = NULL;
-	struct nicvf *nicvf = NULL;
+	struct nicvf *nic = NULL;
 	int    err;
 
 	netdev = malloc(sizeof(struct eth_device));
@@ -503,27 +463,29 @@ int nicvf_initialize(struct nicpf *nicpf, int vf_num, unsigned int node)
 		goto fail;
 	}
 
-	nicvf = malloc(sizeof(struct nicvf));
+	nic = malloc(sizeof(struct nicvf));
 
-	if (!nicvf) {
+	if (!nic) {
 		err = -1;
 		goto fail;
 	}
 
-	netdev->priv = nicvf;
-	nicvf->netdev = netdev;
-	nicvf->nicpf = nicpf;
-	nicvf->vf_id = vf_num;
+	netdev->priv = nic;
+	nic->netdev = netdev;
+	nic->nicpf = nicpf;
+	nic->vf_id = vf_num;
+
+	nic->rev_id = 3;
 
 	/* MAP VF's configuration registers */
-	nicvf->reg_base = CSR_PA(NIC_VFX_BAR0(vf_num), node);
-	if (!nicvf->reg_base) {
+	nic->reg_base = CSR_PA(node, NIC_VFX_BAR0(vf_num));
+	if (!nic->reg_base) {
 		printf("Cannot map config register space, aborting\n");
 		err = -1;
 		goto fail;
 	}
 
-	err = nicvf_set_qset_resources(nicvf);
+	err = nicvf_set_qset_resources(nic);
 	if (err)
 		return -1;
 
@@ -546,11 +508,13 @@ int nicvf_initialize(struct nicpf *nicpf, int vf_num, unsigned int node)
 		return -1;
 	}
 
+
 	return 0;
 fail:
-	if (nicvf)
-		free(nicvf);
-	if (netdev)
+	if (nic)
+		free(nic);
+	if(netdev)
 		free(netdev);
 	return err;
 }
+
diff --git a/drivers/net/cavium/nicvf_queues.c b/drivers/net/cavium/nicvf_queues.c
index 5eab33dbb6..c4a3217842 100644
--- a/drivers/net/cavium/nicvf_queues.c
+++ b/drivers/net/cavium/nicvf_queues.c
@@ -18,11 +18,6 @@
 #include "q_struct.h"
 #include "nicvf_queues.h"
 
-struct rcv_buffer {
-	void *alloc;
-	void *data;
-};
-
 static int nicvf_poll_reg(struct nicvf *nic, int qidx,
 			  uint64_t reg, int bit_pos, int bits, int val)
 {
@@ -49,14 +44,17 @@ static int nicvf_alloc_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem,
 {
 	dmem->q_len = q_len;
 	dmem->size = (desc_size * q_len) + align_bytes;
+	/* Save address, need it while freeing */
 	dmem->unalign_base = malloc(dmem->size);
+	dmem->dma = (uintptr_t)dmem->unalign_base;
+
 	if (!dmem->unalign_base)
 		return -1;
 
-	dmem->dma = (uintptr_t) dmem->unalign_base;
-	dmem->phys_base = NICVF_ALIGNED_ADDR((uint64_t) dmem->dma, align_bytes);
-	dmem->base = (void *)((u8 *)dmem->unalign_base +
-			      (dmem->phys_base - dmem->dma));
+	/* Align memory address for 'align_bytes' */
+	dmem->phys_base = NICVF_ALIGNED_ADDR((u64)dmem->dma, align_bytes);
+	dmem->base = dmem->unalign_base + (dmem->phys_base - dmem->dma);
+
 	return 0;
 }
 
@@ -66,73 +64,22 @@ static void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem)
 		return;
 
 	free(dmem->unalign_base);
+
 	dmem->unalign_base = NULL;
 	dmem->base = NULL;
 }
 
-static int nicvf_alloc_rcv_buffer(struct nicvf *nic,
-				  uint64_t buf_len, void **rbuf)
-{
-	uintptr_t buf = 0;
-	void *alloc;
-	struct rcv_buffer *bufdata;
-
-	buf_len += NICVF_RCV_BUF_ALIGN_BYTES + sizeof(void *);
-
-	alloc = malloc(buf_len);
-
-	buf = (uintptr_t) alloc;
-
-	if (!buf) {
-		printf("Failed to allocate new rcv buffer\n");
-		return -1;
-	}
-
-	/* Reserve bytes for storing skb address */
-	buf += sizeof(void *);
-
-	/* Align buffer addr to cache line i.e 128 bytes */
-	buf += NICVF_RCV_BUF_ALIGN_LEN(buf);
-
-	/* Store skb address */
-	bufdata = (struct rcv_buffer *)(buf - sizeof(*bufdata));
-
-	bufdata->alloc = alloc;
-	bufdata->data = (void *)(buf);
-
-	/* Return buffer address */
-	*rbuf = bufdata->data;
-
-	return 0;
-}
 
 static void *nicvf_rb_ptr_to_pkt(struct nicvf *nic, uintptr_t rb_ptr)
 {
-	struct rcv_buffer *bufdata;
-	void *pkt;
-
-	bufdata = (struct rcv_buffer *)(rb_ptr - sizeof(*bufdata));
-
-	pkt = bufdata->data;
-	return pkt;
-}
-
-static void *nicvf_rb_ptr_to_buf(struct nicvf *nic, uintptr_t rb_ptr)
-{
-	struct rcv_buffer *bufdata;
-	void *alloc;
-
-	bufdata = (struct rcv_buffer *)(rb_ptr - sizeof(*bufdata));
-
-	alloc = bufdata->alloc;
-	return alloc;
+	return (void *)rb_ptr;
 }
 
 static int nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,
-			   int ring_len, int buf_size)
+			    int ring_len, int buf_size)
 {
 	int idx;
-	void *rbuf;
+	uintptr_t rbuf;
 	struct rbdr_entry_t *desc;
 
 	if (nicvf_alloc_q_desc_mem(nic, &rbdr->dmem, ring_len,
@@ -144,29 +91,37 @@ static int nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,
 
 	rbdr->desc = rbdr->dmem.base;
 	/* Buffer size has to be in multiples of 128 bytes */
-	rbdr->buf_size = buf_size;
+	rbdr->dma_size = buf_size;
 	rbdr->enable = true;
 	rbdr->thresh = RBDR_THRESH;
 
-	for (idx = 0; idx < ring_len; idx++) {
-		if (nicvf_alloc_rcv_buffer(nic, rbdr->buf_size, &rbuf))
-			return -1;
+	debug("%s: %d: allocating %d bytes for rcv buffers\n",
+	      __FUNCTION__, __LINE__,
+	      ring_len * buf_size + NICVF_RCV_BUF_ALIGN_BYTES);
+	rbdr->buf_mem = (uintptr_t)malloc(ring_len * buf_size
+						+ NICVF_RCV_BUF_ALIGN_BYTES);
+
+	if (!rbdr->buf_mem) {
+		printf("Unable to allocate memory for rcv buffers\n");
+		return -1;
+	}
+
+	rbdr->buffers = NICVF_ALIGNED_ADDR(rbdr->buf_mem, NICVF_RCV_BUF_ALIGN_BYTES);
+
+	debug("%s: %d: rbdr->buf_mem: %lx, rbdr->buffers: %lx\n",
+		__FUNCTION__, __LINE__, rbdr->buf_mem, rbdr->buffers);
 
+	for (idx = 0; idx < ring_len; idx++) {
+		rbuf = rbdr->buffers + DMA_BUFFER_LEN * idx;
 		desc = GET_RBDR_DESC(rbdr, idx);
-		desc->buf_addr = (uintptr_t) rbuf >> NICVF_RCV_BUF_ALIGN;
-		flush_dcache_range((uintptr_t) desc,
-				   (uintptr_t) desc + sizeof(desc));
+		desc->buf_addr = rbuf >> NICVF_RCV_BUF_ALIGN;
+		flush_dcache_range((uintptr_t)desc, (uintptr_t)desc + sizeof(desc));
 	}
 	return 0;
 }
 
 static void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr)
 {
-	int head, tail;
-	void *pkt;
-	uint64_t buf_addr;
-	struct rbdr_entry_t *desc;
-
 	if (!rbdr)
 		return;
 
@@ -174,74 +129,14 @@ static void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr)
 	if (!rbdr->dmem.base)
 		return;
 
-	head = 0;
-	tail = rbdr->dmem.q_len - 1;
-
-	/* Free SKBs */
-	while (head != tail) {
-		desc = GET_RBDR_DESC(rbdr, head);
-
-		invalidate_dcache_range((uintptr_t) desc,
-					(uintptr_t) desc + sizeof(desc));
-		buf_addr = desc->buf_addr << NICVF_RCV_BUF_ALIGN;
-		pkt = nicvf_rb_ptr_to_buf(nic, buf_addr);
-
-		free(pkt);
-
-		head++;
-		head &= (rbdr->dmem.q_len - 1);
-	}
-	/* Free SKB of tail desc */
-	desc = GET_RBDR_DESC(rbdr, tail);
-
-	invalidate_dcache_range((uintptr_t) desc,
-				(uintptr_t) desc + sizeof(desc));
-
-	buf_addr = desc->buf_addr << NICVF_RCV_BUF_ALIGN;
-
-	pkt = nicvf_rb_ptr_to_buf(nic, buf_addr);
-
-	invalidate_dcache_range(buf_addr, buf_addr + rbdr->buf_size);
-
-	free(pkt);
+	debug("%s: %d: rbdr->buf_mem: %p\n", __FUNCTION__,
+	      __LINE__, (void *)rbdr->buf_mem);
+	free((void *)rbdr->buf_mem);
 
 	/* Free RBDR ring */
 	nicvf_free_q_desc_mem(nic, &rbdr->dmem);
 }
 
-/* Refill receive buffer descriptors with new buffers.
- * This runs in softirq context .
- */
-void nicvf_refill_rbdr(struct nicvf *nic)
-{
-	struct queue_set *qs = nic->qs;
-	int rbdr_idx = qs->rbdr_cnt;
-	int qcount;
-	struct rbdr *rbdr;
-
-refill:
-	if (!rbdr_idx)
-		return;
-	rbdr_idx--;
-	rbdr = &qs->rbdr[rbdr_idx];
-	/* Check if it's enabled */
-	if (!rbdr->enable)
-		goto next_rbdr;
-
-	/* check if valid descs reached or crossed threshold level */
-	qcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, rbdr_idx);
-	qcount &= 0x7FFFF;
-	if (qcount > rbdr->thresh)
-		goto next_rbdr;
-
-	/* Notify HW */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
-			      rbdr_idx, rbdr->thresh);
-next_rbdr:
-	if (rbdr_idx)
-		goto refill;
-}
-
 /* TBD: how to handle full packets received in CQ
  * i.e conversion of buffers into SKBs
  */
@@ -255,7 +150,10 @@ static int nicvf_init_cmp_queue(struct nicvf *nic,
 		return -1;
 	}
 	cq->desc = cq->dmem.base;
-	cq->thresh = CMP_QUEUE_CQE_THRESH;
+	if (!IS_PASS1(nic->rev_id))
+		cq->thresh = CMP_QUEUE_CQE_THRESH;
+	else
+		cq->thresh = 0;
 	cq->intr_timer_thresh = CMP_QUEUE_TIMER_THRESH;
 
 	return 0;
@@ -298,7 +196,9 @@ static void nicvf_free_snd_queue(struct nicvf *nic, struct snd_queue *sq)
 	if (!sq->dmem.base)
 		return;
 
+	debug("%s: %d\n", __FUNCTION__, __LINE__);
 	free(sq->skbuff);
+
 	nicvf_free_q_desc_mem(nic, &sq->dmem);
 }
 
@@ -317,12 +217,10 @@ static void nicvf_reclaim_snd_queue(struct nicvf *nic,
 static void nicvf_reclaim_rcv_queue(struct nicvf *nic,
 				    struct queue_set *qs, int qidx)
 {
-	struct nic_mbx mbx = { };
+	union nic_mbx mbx = {};
 
 	/* Make sure all packets in the pipeline are written back into mem */
-
-	mbx.msg = NIC_PF_VF_MSG_RQ_SW_SYNC;
-	mbx.data.rq.cfg = 0;
+	mbx.msg.msg = NIC_MBOX_MSG_RQ_SW_SYNC;
 	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
@@ -337,16 +235,27 @@ static void nicvf_reclaim_cmp_queue(struct nicvf *nic,
 	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);
 }
 
-static void nicvf_reclaim_rbdr(struct nicvf *nic, struct rbdr *rbdr, int qidx)
+static void nicvf_reclaim_rbdr(struct nicvf *nic,
+			       struct rbdr *rbdr, int qidx)
 {
-	uint64_t tmp;
+	u64 tmp, fifo_state;
 	int timeout = 10;
 
 	/* Save head and tail pointers for feeing up buffers */
 	rbdr->head = nicvf_queue_reg_read(nic,
-					  NIC_QSET_RBDR_0_1_HEAD, qidx) >> 3;
+					  NIC_QSET_RBDR_0_1_HEAD,
+					  qidx) >> 3;
 	rbdr->tail = nicvf_queue_reg_read(nic,
-					  NIC_QSET_RBDR_0_1_TAIL, qidx) >> 3;
+					  NIC_QSET_RBDR_0_1_TAIL,
+					  qidx) >> 3;
+
+	/* If RBDR FIFO is in 'FAIL' state then do a reset first
+	 * before relaiming.
+	 */
+	fifo_state = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, qidx);
+	if (((fifo_state >> 62) & 0x03) == 0x3)
+		nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,
+				      qidx, NICVF_RBDR_RESET);
 
 	/* Disable RBDR */
 	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);
@@ -358,7 +267,7 @@ static void nicvf_reclaim_rbdr(struct nicvf *nic, struct rbdr *rbdr, int qidx)
 					   qidx);
 		if ((tmp & 0xFFFFFFFF) == ((tmp >> 32) & 0xFFFFFFFF))
 			break;
-		udelay(2000);
+		mdelay(2000);
 		timeout--;
 		if (!timeout) {
 			printf("Failed polling on prefetch status\n");
@@ -375,16 +284,15 @@ static void nicvf_reclaim_rbdr(struct nicvf *nic, struct rbdr *rbdr, int qidx)
 		return;
 }
 
+
+/* Configures receive queue */
 static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
 				   int qidx, bool enable)
 {
-	struct nic_mbx mbx = { };
+	union nic_mbx mbx = {};
 	struct rcv_queue *rq;
-
-	union {
-		uintptr_t u;
-		struct rq_cfg s;
-	} rq_cfg;
+	struct cmp_queue *cq;
+	struct rq_cfg rq_cfg;
 
 	rq = &qs->rq[qidx];
 	rq->enable = enable;
@@ -403,36 +311,35 @@ static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
 	rq->start_qs_rbdr_idx = qs->rbdr_cnt - 1;
 	rq->cont_rbdr_qs = qs->vnic_id;
 	rq->cont_qs_rbdr_idx = qs->rbdr_cnt - 1;
-	rq->caching = 0;
+	/* all writes of RBDR data to be loaded into L2 Cache as well*/
+	rq->caching = 1;
 
 	/* Send a mailbox msg to PF to config RQ */
-	mbx.msg = NIC_PF_VF_MSG_RQ_CFG;
-	mbx.data.rq.qs_num = qs->vnic_id;
-	mbx.data.rq.rq_num = qidx;
-	mbx.data.rq.cfg = (rq->caching << 26) | (rq->cq_qs << 19) |
-	    (rq->cq_idx << 16) | (rq->cont_rbdr_qs << 9) |
-	    (rq->cont_qs_rbdr_idx << 8) |
-	    (rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);
+	mbx.rq.msg = NIC_MBOX_MSG_RQ_CFG;
+	mbx.rq.qs_num = qs->vnic_id;
+	mbx.rq.rq_num = qidx;
+	mbx.rq.cfg = (rq->caching << 26) | (rq->cq_qs << 19) |
+			  (rq->cq_idx << 16) | (rq->cont_rbdr_qs << 9) |
+			  (rq->cont_qs_rbdr_idx << 8) |
+			  (rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);
 	nicvf_send_msg_to_pf(nic, &mbx);
 
-	mbx.msg = NIC_PF_VF_MSG_RQ_BP_CFG;
-	mbx.data.rq.cfg = (1ULL << 63) | (1ULL << 62) | (qs->vnic_id << 0);
+	mbx.rq.msg = NIC_MBOX_MSG_RQ_BP_CFG;
+	mbx.rq.cfg = (1ULL << 63) | (1ULL << 62) | (qs->vnic_id << 0);
 	nicvf_send_msg_to_pf(nic, &mbx);
 
 	/* RQ drop config
 	 * Enable CQ drop to reserve sufficient CQEs for all tx packets
 	 */
-	mbx.msg = NIC_PF_VF_MSG_RQ_DROP_CFG;
-	mbx.data.rq.cfg = (1ULL << 62) | (RQ_CQ_DROP << 8);
+	mbx.rq.msg = NIC_MBOX_MSG_RQ_DROP_CFG;
+	mbx.rq.cfg = (1ULL << 62) | (RQ_CQ_DROP << 8);
 	nicvf_send_msg_to_pf(nic, &mbx);
-
-	nicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, qidx, 0x00);
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, 0, 0x00);
 
 	/* Enable Receive queue */
-	rq_cfg.s.ena = 1;
-	rq_cfg.s.tcp_ena = 0;
-
-	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, rq_cfg.u);
+	rq_cfg.ena = 1;
+	rq_cfg.tcp_ena = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, *(u64 *)&rq_cfg);
 }
 
 void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
@@ -460,7 +367,7 @@ void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
 
 	/* Set completion queue base address */
 	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE,
-			      qidx, (uint64_t) (cq->dmem.phys_base));
+			      qidx, (uint64_t)(cq->dmem.phys_base));
 
 	/* Enable Completion queue */
 	cq_cfg.s.ena = 1;
@@ -472,20 +379,16 @@ void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
 
 	/* Set threshold value for interrupt generation */
 	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx, cq->thresh);
-	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2,
-			      qidx, cq->intr_timer_thresh);
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2, qidx, cq->intr_timer_thresh);
 }
 
+/* Configures transmit queue */
 static void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,
 				   int qidx, bool enable)
 {
-	struct nic_mbx mbx = { };
+	union nic_mbx mbx = {};
 	struct snd_queue *sq;
-
-	union {
-		uint64_t u;
-		struct sq_cfg s;
-	} sq_cfg;
+	struct sq_cfg sq_cfg;
 
 	sq = &qs->sq[qidx];
 	sq->enable = enable;
@@ -502,37 +405,35 @@ static void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,
 	sq->cq_idx = qidx;
 
 	/* Send a mailbox msg to PF to config SQ */
-	mbx.msg = NIC_PF_VF_MSG_SQ_CFG;
-	mbx.data.sq.qs_num = qs->vnic_id;
-	mbx.data.sq.sq_num = qidx;
-	mbx.data.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;
+	mbx.sq.msg = NIC_MBOX_MSG_SQ_CFG;
+	mbx.sq.qs_num = qs->vnic_id;
+	mbx.sq.sq_num = qidx;
+	mbx.sq.sqs_mode = nic->sqs_mode;
+	mbx.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;
 	nicvf_send_msg_to_pf(nic, &mbx);
 
 	/* Set queue base address */
 	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE,
-			      qidx, (uint64_t) (sq->dmem.phys_base));
+			      qidx, (u64)(sq->dmem.phys_base));
 
 	/* Enable send queue  & set queue size */
-	sq_cfg.s.ena = 1;
-	sq_cfg.s.reset = 0;
-	sq_cfg.s.ldwb = 0;
-	sq_cfg.s.qsize = SND_QSIZE;
-	sq_cfg.s.tstmp_bgx_intf = 0;
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg.u);
+	sq_cfg.ena = 1;
+	sq_cfg.reset = 0;
+	sq_cfg.ldwb = 0;
+	sq_cfg.qsize = SND_QSIZE;
+	sq_cfg.tstmp_bgx_intf = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, *(u64 *)&sq_cfg);
 
 	/* Set threshold value for interrupt generation */
 	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx, sq->thresh);
 }
 
+/* Configures receive buffer descriptor ring */
 static void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,
 			      int qidx, bool enable)
 {
 	struct rbdr *rbdr;
-
-	union {
-		uint64_t u;
-		struct rbdr_cfg s;
-	} rbdr_cfg;
+	struct rbdr_cfg rbdr_cfg;
 
 	rbdr = &qs->rbdr[qidx];
 	nicvf_reclaim_rbdr(nic, rbdr, qidx);
@@ -541,17 +442,18 @@ static void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,
 
 	/* Set descriptor base address */
 	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE,
-			      qidx, (uint64_t) (rbdr->dmem.phys_base));
+			      qidx, (u64)(rbdr->dmem.phys_base));
 
 	/* Enable RBDR  & set queue size */
 	/* Buffer size should be in multiples of 128 bytes */
-	rbdr_cfg.s.ena = 1;
-	rbdr_cfg.s.reset = 0;
-	rbdr_cfg.s.ldwb = 0;
-	rbdr_cfg.s.qsize = RBDR_SIZE;
-	rbdr_cfg.s.avg_con = 0;
-	rbdr_cfg.s.lines = rbdr->buf_size / 128;
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, rbdr_cfg.u);
+	rbdr_cfg.ena = 1;
+	rbdr_cfg.reset = 0;
+	rbdr_cfg.ldwb = 0;
+	rbdr_cfg.qsize = RBDR_SIZE;
+	rbdr_cfg.avg_con = 0;
+	rbdr_cfg.lines = rbdr->dma_size / 128;
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,
+			      qidx, *(u64 *)&rbdr_cfg);
 
 	/* Notify HW */
 	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
@@ -562,9 +464,10 @@ static void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,
 			      qidx, rbdr->thresh - 1);
 }
 
+/* Requests PF to assign and enable Qset */
 void nicvf_qset_config(struct nicvf *nic, bool enable)
 {
-	struct nic_mbx mbx = { };
+	union nic_mbx mbx = {};
 	struct queue_set *qs = nic->qs;
 	struct qs_cfg *qs_cfg;
 
@@ -577,11 +480,14 @@ void nicvf_qset_config(struct nicvf *nic, bool enable)
 	qs->vnic_id = nic->vf_id;
 
 	/* Send a mailbox msg to PF to config Qset */
-	mbx.msg = NIC_PF_VF_MSG_QS_CFG;
-	mbx.data.qs.num = qs->vnic_id;
+	mbx.qs.msg = NIC_MBOX_MSG_QS_CFG;
+	mbx.qs.num = qs->vnic_id;
+#ifdef VNIC_MULTI_QSET_SUPPORT
+	mbx.qs.sqs_count = nic->sqs_count;
+#endif
 
-	mbx.data.qs.cfg = 0;
-	qs_cfg = (struct qs_cfg *)&mbx.data.qs.cfg;
+	mbx.qs.cfg = 0;
+	qs_cfg = (struct qs_cfg *)&mbx.qs.cfg;
 	if (qs->enable) {
 		qs_cfg->ena = 1;
 #ifdef __BIG_ENDIAN
@@ -589,7 +495,6 @@ void nicvf_qset_config(struct nicvf *nic, bool enable)
 #endif
 		qs_cfg->vnic = qs->vnic_id;
 	}
-
 	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
@@ -619,7 +524,7 @@ static int nicvf_alloc_resources(struct nicvf *nic)
 	/* Alloc receive buffer descriptor ring */
 	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {
 		if (nicvf_init_rbdr(nic, &qs->rbdr[qidx], qs->rbdr_len,
-				    RCV_BUFFER_LEN))
+				    DMA_BUFFER_LEN))
 			goto alloc_fail;
 	}
 
@@ -652,7 +557,7 @@ int nicvf_set_qset_resources(struct nicvf *nic)
 
 	/* Set count of each queue */
 	qs->rbdr_cnt = RBDR_CNT;
-	qs->rq_cnt = RCV_QUEUE_CNT;
+	qs->rq_cnt = 1;
 	qs->sq_cnt = SND_QUEUE_CNT;
 	qs->cq_cnt = CMP_QUEUE_CNT;
 
@@ -660,6 +565,10 @@ int nicvf_set_qset_resources(struct nicvf *nic)
 	qs->rbdr_len = RCV_BUF_COUNT;
 	qs->sq_len = SND_QUEUE_LEN;
 	qs->cq_len = CMP_QUEUE_LEN;
+
+	nic->rx_queues = qs->rq_cnt;
+	nic->tx_queues = qs->sq_cnt;
+
 	return 0;
 }
 
@@ -700,35 +609,34 @@ int nicvf_config_data_transfer(struct nicvf *nic, bool enable)
 	return 0;
 }
 
-/* Get a free desc from send queue
- * @qs:   Qset from which to get a SQ descriptor
- * @qnum: SQ number (0...7) in the Qset
- *
+/* Get a free desc from SQ
  * returns descriptor ponter & descriptor number
  */
-static int nicvf_get_sq_desc(struct queue_set *qs, int qnum, void **desc)
+static int nicvf_get_sq_desc(struct snd_queue *sq, int desc_cnt)
 {
 	int qentry;
-	struct snd_queue *sq = &qs->sq[qnum];
-
-	if (!sq->free_cnt)
-		return 0;
-
-	qentry = sq->tail++;
-	sq->free_cnt--;
 
+	qentry = sq->tail;
+	sq->free_cnt -= desc_cnt;
+	sq->tail += desc_cnt;
 	sq->tail &= (sq->dmem.q_len - 1);
-	*desc = GET_SQ_DESC(sq, qentry);
+
 	return qentry;
 }
 
+/* Free descriptor back to SQ for future use */
 void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt)
 {
-	while (desc_cnt--) {
-		sq->free_cnt++;
-		sq->head++;
-		sq->head &= (sq->dmem.q_len - 1);
-	}
+	sq->free_cnt += desc_cnt;
+	sq->head += desc_cnt;
+	sq->head &= (sq->dmem.q_len - 1);
+}
+
+static int nicvf_get_nxt_sqentry(struct snd_queue *sq, int qentry)
+{
+	qentry++;
+	qentry &= (sq->dmem.q_len - 1);
+	return qentry;
 }
 
 void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx)
@@ -752,10 +660,9 @@ void nicvf_sq_disable(struct nicvf *nic, int qidx)
 }
 
 void nicvf_sq_free_used_descs(struct eth_device *netdev, struct snd_queue *sq,
-			      int qidx)
+								int qidx)
 {
 	uint64_t head;
-	void *pkt;
 	struct nicvf *nic = netdev->priv;
 	struct sq_hdr_subdesc *hdr;
 
@@ -767,86 +674,97 @@ void nicvf_sq_free_used_descs(struct eth_device *netdev, struct snd_queue *sq,
 			nicvf_put_sq_desc(sq, 1);
 			continue;
 		}
-		pkt = (void *)sq->skbuff[sq->head];
-		free(pkt);
 		nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
 	}
 }
 
+/* Get the number of SQ descriptors needed to xmit this skb */
+static int nicvf_sq_subdesc_required(struct nicvf *nic)
+{
+	int subdesc_cnt = MIN_SQ_DESC_PER_PKT_XMIT;
+
+	return subdesc_cnt;
+}
+
 /* Add SQ HEADER subdescriptor.
  * First subdescriptor for every send descriptor.
  */
-struct sq_hdr_subdesc *nicvf_sq_add_hdr_subdesc(struct queue_set *qs,
-						int sq_num, int subdesc_cnt,
-						void *pkt, size_t pkt_len)
+static inline void
+nicvf_sq_add_hdr_subdesc(struct nicvf *nic, struct snd_queue *sq, int qentry,
+			 int subdesc_cnt, void *pkt, size_t pkt_len)
 {
-	int qentry;
-	void *desc;
-	struct snd_queue *sq;
 	struct sq_hdr_subdesc *hdr;
 
-	sq = &qs->sq[sq_num];
-	qentry = nicvf_get_sq_desc(qs, sq_num, &desc);
-	sq->skbuff[qentry] = (uintptr_t) pkt;
-
-	hdr = (struct sq_hdr_subdesc *)desc;
+	hdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);
+	sq->skbuff[qentry] = (uintptr_t)pkt;
 
 	memset(hdr, 0, SND_QUEUE_DESC_SIZE);
 	hdr->subdesc_type = SQ_DESC_TYPE_HEADER;
+	/* Enable notification via CQE after processing SQE */
 	hdr->post_cqe = 1;
+	/* No of subdescriptors following this */
 	hdr->subdesc_cnt = subdesc_cnt;
 	hdr->tot_len = pkt_len;
 
-	return hdr;
+	flush_dcache_range((uintptr_t)hdr,
+			   (uintptr_t)hdr + sizeof(struct sq_hdr_subdesc));
 }
 
 /* SQ GATHER subdescriptor
  * Must follow HDR descriptor
  */
-static void nicvf_sq_add_gather_subdesc(struct nicvf *nic, struct queue_set *qs,
-					int sq_num, void *pkt, size_t pkt_len)
+static inline void nicvf_sq_add_gather_subdesc(struct snd_queue *sq, int qentry,
+					       size_t size, uintptr_t data)
 {
-	void *desc;
 	struct sq_gather_subdesc *gather;
 
-	nicvf_get_sq_desc(qs, sq_num, &desc);
-	gather = (struct sq_gather_subdesc *)desc;
+	qentry &= (sq->dmem.q_len - 1);
+	gather = (struct sq_gather_subdesc *)GET_SQ_DESC(sq, qentry);
 
 	memset(gather, 0, SND_QUEUE_DESC_SIZE);
 	gather->subdesc_type = SQ_DESC_TYPE_GATHER;
 	gather->ld_type = NIC_SEND_LD_TYPE_E_LDD;
-	gather->size = pkt_len;
-	gather->addr = (uintptr_t) pkt;
+	gather->size = size;
+	gather->addr = data;
 
-	flush_dcache_range(gather->addr, gather->addr + gather->size);
+	flush_dcache_range((uintptr_t)gather,
+			   (uintptr_t)gather + sizeof(struct sq_hdr_subdesc));
 }
 
 /* Append an skb to a SQ for packet transfer. */
 int nicvf_sq_append_pkt(struct nicvf *nic, void *pkt, size_t pkt_size)
 {
 	int subdesc_cnt;
-	int sq_num;
-	struct queue_set *qs = nic->qs;
+	int sq_num = 0, qentry;
+	struct queue_set *qs;
 	struct snd_queue *sq;
 
-	sq_num = 0;
+	qs = nic->qs;
 	sq = &qs->sq[sq_num];
 
-	subdesc_cnt = 2;
+	subdesc_cnt = nicvf_sq_subdesc_required(nic);
 	if (subdesc_cnt > sq->free_cnt)
 		goto append_fail;
 
+	qentry = nicvf_get_sq_desc(sq, subdesc_cnt);
+
 	/* Add SQ header subdesc */
-	nicvf_sq_add_hdr_subdesc(qs, sq_num, subdesc_cnt - 1, pkt, pkt_size);
+	nicvf_sq_add_hdr_subdesc(nic, sq, qentry, subdesc_cnt - 1,
+				 pkt, pkt_size);
+
+	/* Add SQ gather subdescs */
+	qentry = nicvf_get_nxt_sqentry(sq, qentry);
+	nicvf_sq_add_gather_subdesc(sq, qentry, pkt_size, (uintptr_t)(pkt));
 
-	/* Add SQ gather subdesc */
-	nicvf_sq_add_gather_subdesc(nic, qs, sq_num, pkt, pkt_size);
+	flush_dcache_range((uintptr_t)pkt,
+			   (uintptr_t)pkt + pkt_size);
 
 	/* make sure all memory stores are done before ringing doorbell */
 	asm volatile ("dsb sy");
 
 	/* Inform HW to xmit new packet */
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, sq_num, subdesc_cnt);
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,
+			      sq_num, subdesc_cnt);
 	return 1;
 
 append_fail:
@@ -879,29 +797,28 @@ void *nicvf_get_rcv_pkt(struct nicvf *nic, void *cq_desc, size_t *pkt_len)
 
 	rq = &qs->rq[cqe_rx->rq_idx];
 	rbdr = &qs->rbdr[rq->start_qs_rbdr_idx];
-	rb_lens = cq_desc + (3 * sizeof(uint64_t));	/* Use offsetof */
+	rb_lens = cq_desc + (3 * sizeof(uint64_t)); /* Use offsetof */
 	rb_ptrs = cq_desc + (6 * sizeof(uint64_t));
 
-	debug("%s rb_cnt %d rb0_ptr %llx rb0_sz %d\n",
-	      __func__, cqe_rx->rb_cnt, cqe_rx->rb0_ptr, cqe_rx->rb0_sz);
-
 	for (frag = 0; frag < cqe_rx->rb_cnt; frag++) {
 		payload_len = rb_lens[frag_num(frag)];
 
-		invalidate_dcache_range((uintptr_t) (*rb_ptrs),
-					(uintptr_t) (*rb_ptrs) +
-					rbdr->buf_size);
+/*		invalidate_dcache_range((uintptr_t)(*rb_ptrs),
+					(uintptr_t)(*rb_ptrs) + rbdr->dma_size);
+*/
 
 		/* First fragment */
 		*rb_ptrs = *rb_ptrs - cqe_rx->align_pad;
+
 		pkt = nicvf_rb_ptr_to_pkt(nic, *rb_ptrs);
 
-		invalidate_dcache_range((uintptr_t) pkt,
-					(uintptr_t) pkt + payload_len);
+/*		invalidate_dcache_range((uintptr_t)pkt,
+					(uintptr_t)pkt + payload_len);
+*/
 
-		if (cqe_rx->align_pad)
+		if (cqe_rx->align_pad) {
 			pkt += cqe_rx->align_pad;
-
+		}
 		/* Next buffer pointer */
 		rb_ptrs++;
 
@@ -918,28 +835,28 @@ void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)
 	switch (int_type) {
 	case NICVF_INTR_CQ:
 		reg_val = ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);
-		break;
+	break;
 	case NICVF_INTR_SQ:
 		reg_val = ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);
-		break;
+	break;
 	case NICVF_INTR_RBDR:
 		reg_val = ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);
-		break;
+	break;
 	case NICVF_INTR_PKT_DROP:
 		reg_val = (1ULL << NICVF_INTR_PKT_DROP_SHIFT);
-		break;
+	break;
 	case NICVF_INTR_TCP_TIMER:
 		reg_val = (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);
-		break;
+	break;
 	case NICVF_INTR_MBOX:
 		reg_val = (1ULL << NICVF_INTR_MBOX_SHIFT);
-		break;
+	break;
 	case NICVF_INTR_QS_ERR:
 		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
-		break;
+	break;
 	default:
 		printf("Failed to clear interrupt: unknown type\n");
-		break;
+	break;
 	}
 
 	nicvf_reg_write(nic, NIC_VF_INT, reg_val);
@@ -987,109 +904,109 @@ int nicvf_check_cqe_rx_errs(struct nicvf *nic,
 	switch (cqe_rx->err_level) {
 	case CQ_ERRLVL_MAC:
 		stats->rx.errlvl.mac_errs++;
-		break;
+	break;
 	case CQ_ERRLVL_L2:
 		stats->rx.errlvl.l2_errs++;
-		break;
+	break;
 	case CQ_ERRLVL_L3:
 		stats->rx.errlvl.l3_errs++;
-		break;
+	break;
 	case CQ_ERRLVL_L4:
 		stats->rx.errlvl.l4_errs++;
-		break;
+	break;
 	}
 
 	switch (cqe_rx->err_opcode) {
 	case CQ_RX_ERROP_RE_PARTIAL:
 		stats->rx.errop.partial_pkts++;
-		break;
+	break;
 	case CQ_RX_ERROP_RE_JABBER:
 		stats->rx.errop.jabber_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_RE_FCS:
 		stats->rx.errop.fcs_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_RE_TERMINATE:
 		stats->rx.errop.terminate_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_RE_RX_CTL:
 		stats->rx.errop.bgx_rx_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_PREL2_ERR:
 		stats->rx.errop.prel2_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_FRAGMENT:
 		stats->rx.errop.l2_frags++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_OVERRUN:
 		stats->rx.errop.l2_overruns++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_PFCS:
 		stats->rx.errop.l2_pfcs++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_PUNY:
 		stats->rx.errop.l2_puny++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_MAL:
 		stats->rx.errop.l2_hdr_malformed++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_OVERSIZE:
 		stats->rx.errop.l2_oversize++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_UNDERSIZE:
 		stats->rx.errop.l2_undersize++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_LENMISM:
 		stats->rx.errop.l2_len_mismatch++;
-		break;
+	break;
 	case CQ_RX_ERROP_L2_PCLP:
 		stats->rx.errop.l2_pclp++;
-		break;
+	break;
 	case CQ_RX_ERROP_IP_NOT:
 		stats->rx.errop.non_ip++;
-		break;
+	break;
 	case CQ_RX_ERROP_IP_CSUM_ERR:
 		stats->rx.errop.ip_csum_err++;
-		break;
+	break;
 	case CQ_RX_ERROP_IP_MAL:
 		stats->rx.errop.ip_hdr_malformed++;
-		break;
+	break;
 	case CQ_RX_ERROP_IP_MALD:
 		stats->rx.errop.ip_payload_malformed++;
-		break;
+	break;
 	case CQ_RX_ERROP_IP_HOP:
 		stats->rx.errop.ip_hop_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_L3_ICRC:
 		stats->rx.errop.l3_icrc_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_L3_PCLP:
 		stats->rx.errop.l3_pclp++;
-		break;
+	break;
 	case CQ_RX_ERROP_L4_MAL:
 		stats->rx.errop.l4_malformed++;
-		break;
+	break;
 	case CQ_RX_ERROP_L4_CHK:
 		stats->rx.errop.l4_csum_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_UDP_LEN:
 		stats->rx.errop.udp_len_err++;
-		break;
+	break;
 	case CQ_RX_ERROP_L4_PORT:
 		stats->rx.errop.bad_l4_port++;
-		break;
+	break;
 	case CQ_RX_ERROP_TCP_FLAG:
 		stats->rx.errop.bad_tcp_flag++;
-		break;
+	break;
 	case CQ_RX_ERROP_TCP_OFFSET:
 		stats->rx.errop.tcp_offset_errs++;
-		break;
+	break;
 	case CQ_RX_ERROP_L4_PCLP:
 		stats->rx.errop.l4_pclp++;
-		break;
+	break;
 	case CQ_RX_ERROP_RBDR_TRUNC:
 		stats->rx.errop.pkt_truncated++;
-		break;
+	break;
 	}
 
 	return 1;
@@ -1107,46 +1024,46 @@ int nicvf_check_cqe_tx_errs(struct nicvf *nic,
 	case CQ_TX_ERROP_GOOD:
 		stats->tx.good++;
 		return 0;
-		break;
+	break;
 	case CQ_TX_ERROP_DESC_FAULT:
 		stats->tx.desc_fault++;
-		break;
+	break;
 	case CQ_TX_ERROP_HDR_CONS_ERR:
 		stats->tx.hdr_cons_err++;
-		break;
+	break;
 	case CQ_TX_ERROP_SUBDC_ERR:
 		stats->tx.subdesc_err++;
-		break;
+	break;
 	case CQ_TX_ERROP_IMM_SIZE_OFLOW:
 		stats->tx.imm_size_oflow++;
-		break;
+	break;
 	case CQ_TX_ERROP_DATA_SEQUENCE_ERR:
 		stats->tx.data_seq_err++;
-		break;
+	break;
 	case CQ_TX_ERROP_MEM_SEQUENCE_ERR:
 		stats->tx.mem_seq_err++;
-		break;
+	break;
 	case CQ_TX_ERROP_LOCK_VIOL:
 		stats->tx.lock_viol++;
-		break;
+	break;
 	case CQ_TX_ERROP_DATA_FAULT:
 		stats->tx.data_fault++;
-		break;
+	break;
 	case CQ_TX_ERROP_TSTMP_CONFLICT:
 		stats->tx.tstmp_conflict++;
-		break;
+	break;
 	case CQ_TX_ERROP_TSTMP_TIMEOUT:
 		stats->tx.tstmp_timeout++;
-		break;
+	break;
 	case CQ_TX_ERROP_MEM_FAULT:
 		stats->tx.mem_fault++;
-		break;
+	break;
 	case CQ_TX_ERROP_CK_OVERLAP:
 		stats->tx.csum_overlap++;
-		break;
+	break;
 	case CQ_TX_ERROP_CK_OFLOW:
 		stats->tx.csum_overflow++;
-		break;
+	break;
 	}
 
 	return 1;
diff --git a/drivers/net/cavium/nicvf_queues.h b/drivers/net/cavium/nicvf_queues.h
index 0668324641..82eb5322ea 100644
--- a/drivers/net/cavium/nicvf_queues.h
+++ b/drivers/net/cavium/nicvf_queues.h
@@ -31,35 +31,35 @@
 #define	for_each_rbdr_irq(irq)	\
 	for (irq = NICVF_INTR_ID_RBDR; irq < NICVF_INTR_ID_MISC; irq++)
 
-#define RBDR_SIZE0		0ULL	/* 8K entries */
-#define RBDR_SIZE1		1ULL	/* 16K entries */
-#define RBDR_SIZE2		2ULL	/* 32K entries */
-#define RBDR_SIZE3		3ULL	/* 64K entries */
-#define RBDR_SIZE4		4ULL	/* 126K entries */
-#define RBDR_SIZE5		5ULL	/* 256K entries */
-#define RBDR_SIZE6		6ULL	/* 512K entries */
-
-#define SND_QUEUE_SIZE0		0ULL	/* 1K entries */
-#define SND_QUEUE_SIZE1		1ULL	/* 2K entries */
-#define SND_QUEUE_SIZE2		2ULL	/* 4K entries */
-#define SND_QUEUE_SIZE3		3ULL	/* 8K entries */
-#define SND_QUEUE_SIZE4		4ULL	/* 16K entries */
-#define SND_QUEUE_SIZE5		5ULL	/* 32K entries */
-#define SND_QUEUE_SIZE6		6ULL	/* 64K entries */
-
-#define CMP_QUEUE_SIZE0		0ULL	/* 1K entries */
-#define CMP_QUEUE_SIZE1		1ULL	/* 2K entries */
-#define CMP_QUEUE_SIZE2		2ULL	/* 4K entries */
-#define CMP_QUEUE_SIZE3		3ULL	/* 8K entries */
-#define CMP_QUEUE_SIZE4		4ULL	/* 16K entries */
-#define CMP_QUEUE_SIZE5		5ULL	/* 32K entries */
-#define CMP_QUEUE_SIZE6		6ULL	/* 64K entries */
+#define RBDR_SIZE0		0ULL /* 8K entries */
+#define RBDR_SIZE1		1ULL /* 16K entries */
+#define RBDR_SIZE2		2ULL /* 32K entries */
+#define RBDR_SIZE3		3ULL /* 64K entries */
+#define RBDR_SIZE4		4ULL /* 126K entries */
+#define RBDR_SIZE5		5ULL /* 256K entries */
+#define RBDR_SIZE6		6ULL /* 512K entries */
+
+#define SND_QUEUE_SIZE0		0ULL /* 1K entries */
+#define SND_QUEUE_SIZE1		1ULL /* 2K entries */
+#define SND_QUEUE_SIZE2		2ULL /* 4K entries */
+#define SND_QUEUE_SIZE3		3ULL /* 8K entries */
+#define SND_QUEUE_SIZE4		4ULL /* 16K entries */
+#define SND_QUEUE_SIZE5		5ULL /* 32K entries */
+#define SND_QUEUE_SIZE6		6ULL /* 64K entries */
+
+#define CMP_QUEUE_SIZE0		0ULL /* 1K entries */
+#define CMP_QUEUE_SIZE1		1ULL /* 2K entries */
+#define CMP_QUEUE_SIZE2		2ULL /* 4K entries */
+#define CMP_QUEUE_SIZE3		3ULL /* 8K entries */
+#define CMP_QUEUE_SIZE4		4ULL /* 16K entries */
+#define CMP_QUEUE_SIZE5		5ULL /* 32K entries */
+#define CMP_QUEUE_SIZE6		6ULL /* 64K entries */
 
 /* Default queue count per QS, its lengths and threshold values */
 #define RBDR_CNT			1
 #define RCV_QUEUE_CNT		1
 #define SND_QUEUE_CNT		1
-#define CMP_QUEUE_CNT		1	/* Max of RCV and SND qcount */
+#define CMP_QUEUE_CNT		1 /* Max of RCV and SND qcount */
 
 #define SND_QSIZE		SND_QUEUE_SIZE0
 #define SND_QUEUE_LEN		(1ULL << (SND_QSIZE + 10))
@@ -70,26 +70,27 @@
 #define CMP_QSIZE		CMP_QUEUE_SIZE0
 #define CMP_QUEUE_LEN		(1ULL << (CMP_QSIZE + 10))
 #define CMP_QUEUE_CQE_THRESH	0
-#define CMP_QUEUE_TIMER_THRESH	1	/* 1 ms */
+#define CMP_QUEUE_TIMER_THRESH	1 /* 1 ms */
 
 #define RBDR_SIZE		RBDR_SIZE0
-#define RCV_BUF_COUNT	(1ULL << (RBDR_SIZE + 13))
-#define RBDR_THRESH		(RCV_BUF_COUNT / 1024)
-#define RCV_BUFFER_LEN	2048	/* In multiples of 128bytes */
+#define RCV_BUF_COUNT		(1ULL << (RBDR_SIZE + 13))
+#define RBDR_THRESH		(RCV_BUF_COUNT / 2)
+#define DMA_BUFFER_LEN		2048 /* In multiples of 128bytes */
+#define RCV_FRAG_LEN	 	DMA_BUFFER_LEN
 
 #define MAX_CQES_FOR_TX		((SND_QUEUE_LEN / MIN_SQ_DESC_PER_PKT_XMIT) *\
 				 MAX_CQE_PER_PKT_XMIT)
 #define RQ_CQ_DROP		((CMP_QUEUE_LEN - MAX_CQES_FOR_TX) / 256)
 
 /* Descriptor size */
-#define SND_QUEUE_DESC_SIZE	16	/* 128 bits */
+#define SND_QUEUE_DESC_SIZE	16   /* 128 bits */
 #define CMP_QUEUE_DESC_SIZE	512
 
 /* Buffer / descriptor alignments */
 #define NICVF_RCV_BUF_ALIGN		7
 #define NICVF_RCV_BUF_ALIGN_BYTES	(1ULL << NICVF_RCV_BUF_ALIGN)
-#define NICVF_CQ_BASE_ALIGN_BYTES	512	/* 9 bits */
-#define NICVF_SQ_BASE_ALIGN_BYTES	128	/* 7 bits */
+#define NICVF_CQ_BASE_ALIGN_BYTES	512  /* 9 bits */
+#define NICVF_SQ_BASE_ALIGN_BYTES	128  /* 7 bits */
 
 #define NICVF_ALIGNED_ADDR(ADDR, ALIGN_BYTES)	ALIGN(ADDR, ALIGN_BYTES)
 #define NICVF_ADDR_ALIGN_LEN(ADDR, BYTES)\
@@ -230,82 +231,85 @@ enum RQ_SQ_STATS {
 };
 
 struct rx_tx_queue_stats {
-	u64 bytes;
-	u64 pkts;
+	u64	bytes;
+	u64	pkts;
 };
 
 struct q_desc_mem {
-	uintptr_t dma;
-	uint64_t size;
-	uint16_t q_len;
-	uintptr_t phys_base;
-	void *base;
-	void *unalign_base;
+	uintptr_t	dma;
+	uint64_t	size;
+	uint16_t	q_len;
+	uintptr_t	phys_base;
+	void		*base;
+	void		*unalign_base;
+	bool		allocated;
 };
 
 struct rbdr {
-	bool enable;
-	uint32_t buf_size;
-	uint32_t thresh;	/* Threshold level for interrupt */
-	void *desc;
-	uint32_t head;
-	uint32_t tail;
-	struct q_desc_mem dmem;
+	bool		enable;
+	uint32_t	dma_size;
+	uint32_t	thresh;      /* Threshold level for interrupt */
+	void		*desc;
+	uint32_t	head;
+	uint32_t	tail;
+	struct		q_desc_mem   dmem;
+	uintptr_t	buf_mem;
+	uintptr_t	buffers;
 };
 
 struct rcv_queue {
-	bool enable;
-	struct rbdr *rbdr_start;
-	struct rbdr *rbdr_cont;
-	bool en_tcp_reassembly;
-	uint8_t cq_qs;		/* CQ's QS to which this RQ is assigned */
-	uint8_t cq_idx;		/* CQ index (0 to 7) in the QS */
-	uint8_t cont_rbdr_qs;	/* Continue buffer ptrs - QS num */
-	uint8_t cont_qs_rbdr_idx;	/* RBDR idx in the cont QS */
-	uint8_t start_rbdr_qs;	/* First buffer ptrs - QS num */
-	uint8_t start_qs_rbdr_idx;	/* RBDR idx in the above QS */
-	uint8_t caching;
-	struct rx_tx_queue_stats stats;
+	bool		enable;
+	struct	rbdr	*rbdr_start;
+	struct	rbdr	*rbdr_cont;
+	bool		en_tcp_reassembly;
+	uint8_t		cq_qs;  /* CQ's QS to which this RQ is assigned */
+	uint8_t		cq_idx; /* CQ index (0 to 7) in the QS */
+	uint8_t		cont_rbdr_qs;      /* Continue buffer ptrs - QS num */
+	uint8_t		cont_qs_rbdr_idx;  /* RBDR idx in the cont QS */
+	uint8_t		start_rbdr_qs;     /* First buffer ptrs - QS num */
+	uint8_t		start_qs_rbdr_idx; /* RBDR idx in the above QS */
+	uint8_t         caching;
+	struct		rx_tx_queue_stats stats;
 };
 
 struct cmp_queue {
-	bool enable;
-	uint16_t intr_timer_thresh;
-	uint16_t thresh;
-	void *desc;
-	struct q_desc_mem dmem;
-	struct cmp_queue_stats stats;
+	bool		enable;
+	uint16_t	intr_timer_thresh;
+	uint16_t	thresh;
+	void		*desc;
+	struct q_desc_mem   dmem;
+	struct cmp_queue_stats	stats;
 };
 
 struct snd_queue {
-	bool enable;
-	uint8_t cq_qs;		/* CQ's QS to which this SQ is pointing */
-	uint8_t cq_idx;		/* CQ index (0 to 7) in the above QS */
-	uint16_t thresh;
-	uint32_t free_cnt;
-	uint32_t head;
-	uint32_t tail;
-	uint64_t *skbuff;
-	void *desc;
-	struct q_desc_mem dmem;
+	bool		enable;
+	uint8_t		cq_qs;  /* CQ's QS to which this SQ is pointing */
+	uint8_t		cq_idx; /* CQ index (0 to 7) in the above QS */
+	uint16_t	thresh;
+	uint32_t	free_cnt;
+	uint32_t	head;
+	uint32_t	tail;
+	uint64_t	*skbuff;
+	void		*desc;
+	struct q_desc_mem   dmem;
 	struct rx_tx_queue_stats stats;
 };
 
 struct queue_set {
-	bool enable;
-	bool be_en;
-	uint8_t vnic_id;
-	uint8_t rq_cnt;
-	uint8_t cq_cnt;
-	uint64_t cq_len;
-	uint8_t sq_cnt;
-	uint64_t sq_len;
-	uint8_t rbdr_cnt;
-	uint64_t rbdr_len;
-	struct rcv_queue rq[MAX_RCV_QUEUES_PER_QS];
-	struct cmp_queue cq[MAX_CMP_QUEUES_PER_QS];
-	struct snd_queue sq[MAX_SND_QUEUES_PER_QS];
-	struct rbdr rbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];
+	bool		enable;
+	bool		be_en;
+	uint8_t		vnic_id;
+	uint8_t		rq_cnt;
+	uint8_t		cq_cnt;
+	uint64_t	cq_len;
+	uint8_t		sq_cnt;
+	uint64_t	sq_len;
+	uint8_t		rbdr_cnt;
+	uint64_t	rbdr_len;
+	struct	rcv_queue	rq[MAX_RCV_QUEUES_PER_QS];
+	struct	cmp_queue	cq[MAX_CMP_QUEUES_PER_QS];
+	struct	snd_queue	sq[MAX_SND_QUEUES_PER_QS];
+	struct	rbdr		rbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];
 };
 
 #define GET_RBDR_DESC(RING, idx)\
@@ -333,7 +337,7 @@ void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx);
 void nicvf_sq_disable(struct nicvf *nic, int qidx);
 void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt);
 void nicvf_sq_free_used_descs(struct eth_device *netdev,
-			      struct snd_queue *sq, int qidx);
+								struct snd_queue *sq, int qidx);
 int nicvf_sq_append_pkt(struct nicvf *nic, void *pkt, size_t pkt_len);
 
 void *nicvf_get_rcv_pkt(struct nicvf *nic, void *cq_desc, size_t *pkt_len);
diff --git a/drivers/net/cavium/q_struct.h b/drivers/net/cavium/q_struct.h
index 9ac49ce03c..729276bd80 100644
--- a/drivers/net/cavium/q_struct.h
+++ b/drivers/net/cavium/q_struct.h
@@ -174,180 +174,180 @@ enum cqe_rx_err_opcode {
 
 struct cqe_rx_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t cqe_type:4;	/* W0 */
-	uint64_t stdn_fault:1;
-	uint64_t rsvd0:1;
-	uint64_t rq_qs:7;
-	uint64_t rq_idx:3;
-	uint64_t rsvd1:12;
-	uint64_t rss_alg:4;
-	uint64_t rsvd2:4;
-	uint64_t rb_cnt:4;
-	uint64_t vlan_found:1;
-	uint64_t vlan_stripped:1;
-	uint64_t vlan2_found:1;
-	uint64_t vlan2_stripped:1;
-	uint64_t l4_type:4;
-	uint64_t l3_type:4;
-	uint64_t l2_present:1;
-	uint64_t err_level:3;
-	uint64_t err_opcode:8;
-
-	uint64_t pkt_len:16;	/* W1 */
-	uint64_t l2_ptr:8;
-	uint64_t l3_ptr:8;
-	uint64_t l4_ptr:8;
-	uint64_t cq_pkt_len:8;
-	uint64_t align_pad:3;
-	uint64_t rsvd3:1;
-	uint64_t chan:12;
-
-	uint64_t rss_tag:32;	/* W2 */
-	uint64_t vlan_tci:16;
-	uint64_t vlan_ptr:8;
-	uint64_t vlan2_ptr:8;
-
-	uint64_t rb3_sz:16;	/* W3 */
-	uint64_t rb2_sz:16;
-	uint64_t rb1_sz:16;
-	uint64_t rb0_sz:16;
-
-	uint64_t rb7_sz:16;	/* W4 */
-	uint64_t rb6_sz:16;
-	uint64_t rb5_sz:16;
-	uint64_t rb4_sz:16;
-
-	uint64_t rb11_sz:16;	/* W5 */
-	uint64_t rb10_sz:16;
-	uint64_t rb9_sz:16;
-	uint64_t rb8_sz:16;
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   stdn_fault:1;
+	uint64_t   rsvd0:1;
+	uint64_t   rq_qs:7;
+	uint64_t   rq_idx:3;
+	uint64_t   rsvd1:12;
+	uint64_t   rss_alg:4;
+	uint64_t   rsvd2:4;
+	uint64_t   rb_cnt:4;
+	uint64_t   vlan_found:1;
+	uint64_t   vlan_stripped:1;
+	uint64_t   vlan2_found:1;
+	uint64_t   vlan2_stripped:1;
+	uint64_t   l4_type:4;
+	uint64_t   l3_type:4;
+	uint64_t   l2_present:1;
+	uint64_t   err_level:3;
+	uint64_t   err_opcode:8;
+
+	uint64_t   pkt_len:16; /* W1 */
+	uint64_t   l2_ptr:8;
+	uint64_t   l3_ptr:8;
+	uint64_t   l4_ptr:8;
+	uint64_t   cq_pkt_len:8;
+	uint64_t   align_pad:3;
+	uint64_t   rsvd3:1;
+	uint64_t   chan:12;
+
+	uint64_t   rss_tag:32; /* W2 */
+	uint64_t   vlan_tci:16;
+	uint64_t   vlan_ptr:8;
+	uint64_t   vlan2_ptr:8;
+
+	uint64_t   rb3_sz:16; /* W3 */
+	uint64_t   rb2_sz:16;
+	uint64_t   rb1_sz:16;
+	uint64_t   rb0_sz:16;
+
+	uint64_t   rb7_sz:16; /* W4 */
+	uint64_t   rb6_sz:16;
+	uint64_t   rb5_sz:16;
+	uint64_t   rb4_sz:16;
+
+	uint64_t   rb11_sz:16; /* W5 */
+	uint64_t   rb10_sz:16;
+	uint64_t   rb9_sz:16;
+	uint64_t   rb8_sz:16;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t err_opcode:8;
-	uint64_t err_level:3;
-	uint64_t l2_present:1;
-	uint64_t l3_type:4;
-	uint64_t l4_type:4;
-	uint64_t vlan2_stripped:1;
-	uint64_t vlan2_found:1;
-	uint64_t vlan_stripped:1;
-	uint64_t vlan_found:1;
-	uint64_t rb_cnt:4;
-	uint64_t rsvd2:4;
-	uint64_t rss_alg:4;
-	uint64_t rsvd1:12;
-	uint64_t rq_idx:3;
-	uint64_t rq_qs:7;
-	uint64_t rsvd0:1;
-	uint64_t stdn_fault:1;
-	uint64_t cqe_type:4;	/* W0 */
-	uint64_t chan:12;
-	uint64_t rsvd3:1;
-	uint64_t align_pad:3;
-	uint64_t cq_pkt_len:8;
-	uint64_t l4_ptr:8;
-	uint64_t l3_ptr:8;
-	uint64_t l2_ptr:8;
-	uint64_t pkt_len:16;	/* W1 */
-	uint64_t vlan2_ptr:8;
-	uint64_t vlan_ptr:8;
-	uint64_t vlan_tci:16;
-	uint64_t rss_tag:32;	/* W2 */
-	uint64_t rb0_sz:16;
-	uint64_t rb1_sz:16;
-	uint64_t rb2_sz:16;
-	uint64_t rb3_sz:16;	/* W3 */
-	uint64_t rb4_sz:16;
-	uint64_t rb5_sz:16;
-	uint64_t rb6_sz:16;
-	uint64_t rb7_sz:16;	/* W4 */
-	uint64_t rb8_sz:16;
-	uint64_t rb9_sz:16;
-	uint64_t rb10_sz:16;
-	uint64_t rb11_sz:16;	/* W5 */
+	uint64_t   err_opcode:8;
+	uint64_t   err_level:3;
+	uint64_t   l2_present:1;
+	uint64_t   l3_type:4;
+	uint64_t   l4_type:4;
+	uint64_t   vlan2_stripped:1;
+	uint64_t   vlan2_found:1;
+	uint64_t   vlan_stripped:1;
+	uint64_t   vlan_found:1;
+	uint64_t   rb_cnt:4;
+	uint64_t   rsvd2:4;
+	uint64_t   rss_alg:4;
+	uint64_t   rsvd1:12;
+	uint64_t   rq_idx:3;
+	uint64_t   rq_qs:7;
+	uint64_t   rsvd0:1;
+	uint64_t   stdn_fault:1;
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   chan:12;
+	uint64_t   rsvd3:1;
+	uint64_t   align_pad:3;
+	uint64_t   cq_pkt_len:8;
+	uint64_t   l4_ptr:8;
+	uint64_t   l3_ptr:8;
+	uint64_t   l2_ptr:8;
+	uint64_t   pkt_len:16; /* W1 */
+	uint64_t   vlan2_ptr:8;
+	uint64_t   vlan_ptr:8;
+	uint64_t   vlan_tci:16;
+	uint64_t   rss_tag:32; /* W2 */
+	uint64_t   rb0_sz:16;
+	uint64_t   rb1_sz:16;
+	uint64_t   rb2_sz:16;
+	uint64_t   rb3_sz:16; /* W3 */
+	uint64_t   rb4_sz:16;
+	uint64_t   rb5_sz:16;
+	uint64_t   rb6_sz:16;
+	uint64_t   rb7_sz:16; /* W4 */
+	uint64_t   rb8_sz:16;
+	uint64_t   rb9_sz:16;
+	uint64_t   rb10_sz:16;
+	uint64_t   rb11_sz:16; /* W5 */
 #endif
-	uint64_t rb0_ptr:64;
-	uint64_t rb1_ptr:64;
-	uint64_t rb2_ptr:64;
-	uint64_t rb3_ptr:64;
-	uint64_t rb4_ptr:64;
-	uint64_t rb5_ptr:64;
-	uint64_t rb6_ptr:64;
-	uint64_t rb7_ptr:64;
-	uint64_t rb8_ptr:64;
-	uint64_t rb9_ptr:64;
-	uint64_t rb10_ptr:64;
-	uint64_t rb11_ptr:64;
+	uint64_t   rb0_ptr:64;
+	uint64_t   rb1_ptr:64;
+	uint64_t   rb2_ptr:64;
+	uint64_t   rb3_ptr:64;
+	uint64_t   rb4_ptr:64;
+	uint64_t   rb5_ptr:64;
+	uint64_t   rb6_ptr:64;
+	uint64_t   rb7_ptr:64;
+	uint64_t   rb8_ptr:64;
+	uint64_t   rb9_ptr:64;
+	uint64_t   rb10_ptr:64;
+	uint64_t   rb11_ptr:64;
 };
 
 struct cqe_rx_tcp_err_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t cqe_type:4;	/* W0 */
-	uint64_t rsvd0:60;
-
-	uint64_t rsvd1:4;	/* W1 */
-	uint64_t partial_first:1;
-	uint64_t rsvd2:27;
-	uint64_t rbdr_bytes:8;
-	uint64_t rsvd3:24;
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   rsvd0:60;
+
+	uint64_t   rsvd1:4; /* W1 */
+	uint64_t   partial_first:1;
+	uint64_t   rsvd2:27;
+	uint64_t   rbdr_bytes:8;
+	uint64_t   rsvd3:24;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t rsvd0:60;
-	uint64_t cqe_type:4;
-
-	uint64_t rsvd3:24;
-	uint64_t rbdr_bytes:8;
-	uint64_t rsvd2:27;
-	uint64_t partial_first:1;
-	uint64_t rsvd1:4;
+	uint64_t   rsvd0:60;
+	uint64_t   cqe_type:4;
+
+	uint64_t   rsvd3:24;
+	uint64_t   rbdr_bytes:8;
+	uint64_t   rsvd2:27;
+	uint64_t   partial_first:1;
+	uint64_t   rsvd1:4;
 #endif
 };
 
 struct cqe_rx_tcp_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t cqe_type:4;	/* W0 */
-	uint64_t rsvd0:52;
-	uint64_t cq_tcp_status:8;
-
-	uint64_t rsvd1:32;	/* W1 */
-	uint64_t tcp_cntx_bytes:8;
-	uint64_t rsvd2:8;
-	uint64_t tcp_err_bytes:16;
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   rsvd0:52;
+	uint64_t   cq_tcp_status:8;
+
+	uint64_t   rsvd1:32; /* W1 */
+	uint64_t   tcp_cntx_bytes:8;
+	uint64_t   rsvd2:8;
+	uint64_t   tcp_err_bytes:16;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t cq_tcp_status:8;
-	uint64_t rsvd0:52;
-	uint64_t cqe_type:4;	/* W0 */
-
-	uint64_t tcp_err_bytes:16;
-	uint64_t rsvd2:8;
-	uint64_t tcp_cntx_bytes:8;
-	uint64_t rsvd1:32;	/* W1 */
+	uint64_t   cq_tcp_status:8;
+	uint64_t   rsvd0:52;
+	uint64_t   cqe_type:4; /* W0 */
+
+	uint64_t   tcp_err_bytes:16;
+	uint64_t   rsvd2:8;
+	uint64_t   tcp_cntx_bytes:8;
+	uint64_t   rsvd1:32; /* W1 */
 #endif
 };
 
 struct cqe_send_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t cqe_type:4;	/* W0 */
-	uint64_t rsvd0:4;
-	uint64_t sqe_ptr:16;
-	uint64_t rsvd1:4;
-	uint64_t rsvd2:10;
-	uint64_t sq_qs:7;
-	uint64_t sq_idx:3;
-	uint64_t rsvd3:8;
-	uint64_t send_status:8;
-
-	uint64_t ptp_timestamp:64;	/* W1 */
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   rsvd0:4;
+	uint64_t   sqe_ptr:16;
+	uint64_t   rsvd1:4;
+	uint64_t   rsvd2:10;
+	uint64_t   sq_qs:7;
+	uint64_t   sq_idx:3;
+	uint64_t   rsvd3:8;
+	uint64_t   send_status:8;
+
+	uint64_t   ptp_timestamp:64; /* W1 */
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t send_status:8;
-	uint64_t rsvd3:8;
-	uint64_t sq_idx:3;
-	uint64_t sq_qs:7;
-	uint64_t rsvd2:10;
-	uint64_t rsvd1:4;
-	uint64_t sqe_ptr:16;
-	uint64_t rsvd0:4;
-	uint64_t cqe_type:4;	/* W0 */
-
-	uint64_t ptp_timestamp:64;	/* W1 */
+	uint64_t   send_status:8;
+	uint64_t   rsvd3:8;
+	uint64_t   sq_idx:3;
+	uint64_t   sq_qs:7;
+	uint64_t   rsvd2:10;
+	uint64_t   rsvd1:4;
+	uint64_t   sqe_ptr:16;
+	uint64_t   rsvd0:4;
+	uint64_t   cqe_type:4; /* W0 */
+
+	uint64_t   ptp_timestamp:64; /* W1 */
 #endif
 };
 
@@ -361,54 +361,54 @@ union cq_desc_t {
 
 struct rbdr_entry_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t rsvd0:15;
-	uint64_t buf_addr:42;
-	uint64_t cache_align:7;
+	uint64_t   rsvd0:15;
+	uint64_t   buf_addr:42;
+	uint64_t   cache_align:7;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t cache_align:7;
-	uint64_t buf_addr:42;
-	uint64_t rsvd0:15;
+	uint64_t   cache_align:7;
+	uint64_t   buf_addr:42;
+	uint64_t   rsvd0:15;
 #endif
 };
 
 /* TCP reassembly context */
 struct rbe_tcp_cnxt_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t tcp_pkt_cnt:12;
-	uint64_t rsvd1:4;
-	uint64_t align_hdr_bytes:4;
-	uint64_t align_ptr_bytes:4;
-	uint64_t ptr_bytes:16;
-	uint64_t rsvd2:24;
-	uint64_t cqe_type:4;
-	uint64_t rsvd0:54;
-	uint64_t tcp_end_reason:2;
-	uint64_t tcp_status:4;
+	uint64_t   tcp_pkt_cnt:12;
+	uint64_t   rsvd1:4;
+	uint64_t   align_hdr_bytes:4;
+	uint64_t   align_ptr_bytes:4;
+	uint64_t   ptr_bytes:16;
+	uint64_t   rsvd2:24;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd0:54;
+	uint64_t   tcp_end_reason:2;
+	uint64_t   tcp_status:4;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t tcp_status:4;
-	uint64_t tcp_end_reason:2;
-	uint64_t rsvd0:54;
-	uint64_t cqe_type:4;
-	uint64_t rsvd2:24;
-	uint64_t ptr_bytes:16;
-	uint64_t align_ptr_bytes:4;
-	uint64_t align_hdr_bytes:4;
-	uint64_t rsvd1:4;
-	uint64_t tcp_pkt_cnt:12;
+	uint64_t   tcp_status:4;
+	uint64_t   tcp_end_reason:2;
+	uint64_t   rsvd0:54;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd2:24;
+	uint64_t   ptr_bytes:16;
+	uint64_t   align_ptr_bytes:4;
+	uint64_t   align_hdr_bytes:4;
+	uint64_t   rsvd1:4;
+	uint64_t   tcp_pkt_cnt:12;
 #endif
 };
 
 /* Always Big endian */
 struct rx_hdr_t {
-	uint64_t opaque:32;
-	uint64_t rss_flow:8;
-	uint64_t skip_length:6;
-	uint64_t disable_rss:1;
-	uint64_t disable_tcp_reassembly:1;
-	uint64_t nodrop:1;
-	uint64_t dest_alg:2;
-	uint64_t rsvd0:2;
-	uint64_t dest_rq:11;
+	uint64_t   opaque:32;
+	uint64_t   rss_flow:8;
+	uint64_t   skip_length:6;
+	uint64_t   disable_rss:1;
+	uint64_t   disable_tcp_reassembly:1;
+	uint64_t   nodrop:1;
+	uint64_t   dest_alg:2;
+	uint64_t   rsvd0:2;
+	uint64_t   dest_rq:11;
 };
 
 enum send_l4_csum_type {
@@ -455,132 +455,132 @@ enum sq_subdesc_type {
 
 struct sq_crc_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t rsvd1:32;
-	uint64_t crc_ival:32;
-	uint64_t subdesc_type:4;
-	uint64_t crc_alg:2;
-	uint64_t rsvd0:10;
-	uint64_t crc_insert_pos:16;
-	uint64_t hdr_start:16;
-	uint64_t crc_len:16;
+	uint64_t    rsvd1:32;
+	uint64_t    crc_ival:32;
+	uint64_t    subdesc_type:4;
+	uint64_t    crc_alg:2;
+	uint64_t    rsvd0:10;
+	uint64_t    crc_insert_pos:16;
+	uint64_t    hdr_start:16;
+	uint64_t    crc_len:16;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t crc_len:16;
-	uint64_t hdr_start:16;
-	uint64_t crc_insert_pos:16;
-	uint64_t rsvd0:10;
-	uint64_t crc_alg:2;
-	uint64_t subdesc_type:4;
-	uint64_t crc_ival:32;
-	uint64_t rsvd1:32;
+	uint64_t    crc_len:16;
+	uint64_t    hdr_start:16;
+	uint64_t    crc_insert_pos:16;
+	uint64_t    rsvd0:10;
+	uint64_t    crc_alg:2;
+	uint64_t    subdesc_type:4;
+	uint64_t    crc_ival:32;
+	uint64_t    rsvd1:32;
 #endif
 };
 
 struct sq_gather_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t subdesc_type:4;	/* W0 */
-	uint64_t ld_type:2;
-	uint64_t rsvd0:42;
-	uint64_t size:16;
+	uint64_t    subdesc_type:4; /* W0 */
+	uint64_t    ld_type:2;
+	uint64_t    rsvd0:42;
+	uint64_t    size:16;
 
-	uint64_t rsvd1:15;	/* W1 */
-	uint64_t addr:49;
+	uint64_t    rsvd1:15; /* W1 */
+	uint64_t    addr:49;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t size:16;
-	uint64_t rsvd0:42;
-	uint64_t ld_type:2;
-	uint64_t subdesc_type:4;	/* W0 */
+	uint64_t    size:16;
+	uint64_t    rsvd0:42;
+	uint64_t    ld_type:2;
+	uint64_t    subdesc_type:4; /* W0 */
 
-	uint64_t addr:49;
-	uint64_t rsvd1:15;	/* W1 */
+	uint64_t    addr:49;
+	uint64_t    rsvd1:15; /* W1 */
 #endif
 };
 
 /* SQ immediate subdescriptor */
 struct sq_imm_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t subdesc_type:4;	/* W0 */
-	uint64_t rsvd0:46;
-	uint64_t len:14;
+	uint64_t    subdesc_type:4; /* W0 */
+	uint64_t    rsvd0:46;
+	uint64_t    len:14;
 
-	uint64_t data:64;	/* W1 */
+	uint64_t    data:64; /* W1 */
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t len:14;
-	uint64_t rsvd0:46;
-	uint64_t subdesc_type:4;	/* W0 */
+	uint64_t    len:14;
+	uint64_t    rsvd0:46;
+	uint64_t    subdesc_type:4; /* W0 */
 
-	uint64_t data:64;	/* W1 */
+	uint64_t    data:64; /* W1 */
 #endif
 };
 
 struct sq_mem_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t subdesc_type:4;	/* W0 */
-	uint64_t mem_alg:4;
-	uint64_t mem_dsz:2;
-	uint64_t wmem:1;
-	uint64_t rsvd0:21;
-	uint64_t offset:32;
-
-	uint64_t rsvd1:15;	/* W1 */
-	uint64_t addr:49;
+	uint64_t    subdesc_type:4; /* W0 */
+	uint64_t    mem_alg:4;
+	uint64_t    mem_dsz:2;
+	uint64_t    wmem:1;
+	uint64_t    rsvd0:21;
+	uint64_t    offset:32;
+
+	uint64_t    rsvd1:15; /* W1 */
+	uint64_t    addr:49;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t offset:32;
-	uint64_t rsvd0:21;
-	uint64_t wmem:1;
-	uint64_t mem_dsz:2;
-	uint64_t mem_alg:4;
-	uint64_t subdesc_type:4;	/* W0 */
-
-	uint64_t addr:49;
-	uint64_t rsvd1:15;	/* W1 */
+	uint64_t    offset:32;
+	uint64_t    rsvd0:21;
+	uint64_t    wmem:1;
+	uint64_t    mem_dsz:2;
+	uint64_t    mem_alg:4;
+	uint64_t    subdesc_type:4; /* W0 */
+
+	uint64_t    addr:49;
+	uint64_t    rsvd1:15; /* W1 */
 #endif
 };
 
 struct sq_hdr_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t subdesc_type:4;
-	uint64_t tso:1;
-	uint64_t post_cqe:1;	/* Post CQE on no error also */
-	uint64_t dont_send:1;
-	uint64_t tstmp:1;
-	uint64_t subdesc_cnt:8;
-	uint64_t csum_l4:2;
-	uint64_t csum_l3:1;
-	uint64_t rsvd0:5;
-	uint64_t l4_offset:8;
-	uint64_t l3_offset:8;
-	uint64_t rsvd1:4;
-	uint64_t tot_len:20;	/* W0 */
-
-	uint64_t tso_sdc_cont:8;
-	uint64_t tso_sdc_first:8;
-	uint64_t tso_l4_offset:8;
-	uint64_t tso_flags_last:12;
-	uint64_t tso_flags_first:12;
-	uint64_t rsvd2:2;
-	uint64_t tso_max_paysize:14;	/* W1 */
+	uint64_t    subdesc_type:4;
+	uint64_t    tso:1;
+	uint64_t    post_cqe:1; /* Post CQE on no error also */
+	uint64_t    dont_send:1;
+	uint64_t    tstmp:1;
+	uint64_t    subdesc_cnt:8;
+	uint64_t    csum_l4:2;
+	uint64_t    csum_l3:1;
+	uint64_t    rsvd0:5;
+	uint64_t    l4_offset:8;
+	uint64_t    l3_offset:8;
+	uint64_t    rsvd1:4;
+	uint64_t    tot_len:20; /* W0 */
+
+	uint64_t    tso_sdc_cont:8;
+	uint64_t    tso_sdc_first:8;
+	uint64_t    tso_l4_offset:8;
+	uint64_t    tso_flags_last:12;
+	uint64_t    tso_flags_first:12;
+	uint64_t    rsvd2:2;
+	uint64_t    tso_max_paysize:14; /* W1 */
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t tot_len:20;
-	uint64_t rsvd1:4;
-	uint64_t l3_offset:8;
-	uint64_t l4_offset:8;
-	uint64_t rsvd0:5;
-	uint64_t csum_l3:1;
-	uint64_t csum_l4:2;
-	uint64_t subdesc_cnt:8;
-	uint64_t tstmp:1;
-	uint64_t dont_send:1;
-	uint64_t post_cqe:1;	/* Post CQE on no error also */
-	uint64_t tso:1;
-	uint64_t subdesc_type:4;	/* W0 */
-
-	uint64_t tso_max_paysize:14;
-	uint64_t rsvd2:2;
-	uint64_t tso_flags_first:12;
-	uint64_t tso_flags_last:12;
-	uint64_t tso_l4_offset:8;
-	uint64_t tso_sdc_first:8;
-	uint64_t tso_sdc_cont:8;	/* W1 */
+	uint64_t    tot_len:20;
+	uint64_t    rsvd1:4;
+	uint64_t    l3_offset:8;
+	uint64_t    l4_offset:8;
+	uint64_t    rsvd0:5;
+	uint64_t    csum_l3:1;
+	uint64_t    csum_l4:2;
+	uint64_t    subdesc_cnt:8;
+	uint64_t    tstmp:1;
+	uint64_t    dont_send:1;
+	uint64_t    post_cqe:1; /* Post CQE on no error also */
+	uint64_t    tso:1;
+	uint64_t    subdesc_type:4; /* W0 */
+
+	uint64_t    tso_max_paysize:14;
+	uint64_t    rsvd2:2;
+	uint64_t    tso_flags_first:12;
+	uint64_t    tso_flags_last:12;
+	uint64_t    tso_l4_offset:8;
+	uint64_t    tso_sdc_first:8;
+	uint64_t    tso_sdc_cont:8; /* W1 */
 #endif
 };
 
@@ -666,7 +666,7 @@ struct rbdr_cfg {
 	uint64_t reserved_36_41:6;
 	uint64_t ldwb:1;
 	uint64_t reset:1;
-	uint64_t ena:1;
+	uint64_t ena: 1;
 	uint64_t reserved_45_63:19;
 #endif
 };
diff --git a/drivers/net/cavium/thunder_bgx.c b/drivers/net/cavium/thunder_bgx.c
index 361b8816a1..5318c95476 100644
--- a/drivers/net/cavium/thunder_bgx.c
+++ b/drivers/net/cavium/thunder_bgx.c
@@ -1,8 +1,11 @@
-/**
- * (C) Copyright 2014, Cavium Inc.
+/*
+ * Copyright (C) 2014 Cavium, Inc.
  *
- * SPDX-License-Identifier:	GPL-2.0+
-**/
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
 
 #include <config.h>
 #include <common.h>
@@ -11,56 +14,57 @@
 #include <malloc.h>
 #include <miiphy.h>
 #include <asm/io.h>
-
-#include <cavium/thunderx_smi.h>
-#include <cavium/thunderx_vnic.h>
+#include <asm/errno.h>
 
 #ifdef CONFIG_OF_LIBFDT
  #include <libfdt.h>
  #include <fdt_support.h>
 #endif
 
+#include <cavium/thunderx_smi.h>
+#include <cavium/thunderx_vnic.h>
+
 #include "nic_reg.h"
 #include "nic.h"
 #include "thunder_bgx.h"
 
 struct lmac {
-	struct bgx *bgx;
-	int dmac;
-	bool link_up;
-	int lmacid;		/* ID within BGX */
-	int lmacid_bd;		/* ID on board */
-	struct eth_device netdev;
-	struct mii_dev *mii_bus;
-	struct phy_device *phydev;
-	unsigned int last_duplex;
-	unsigned int last_link;
-	unsigned int last_speed;
-	bool is_sgmii;
-} lmac;
+	struct bgx		*bgx;
+	int			dmac;
+	u8			mac[6];
+	bool			link_up;
+	int			lmacid; /* ID within BGX */
+	int			lmacid_bd; /* ID on board */
+	struct eth_device	netdev;
+	struct mii_dev		*mii_bus;
+	struct phy_device       *phydev;
+	unsigned int            last_duplex;
+	unsigned int            last_link;
+	unsigned int            last_speed;
+	bool			is_sgmii;
+};
 
 struct bgx {
-	uint8_t bgx_id;
-	struct lmac lmac[MAX_LMAC_PER_BGX];
-	int lmac_count;
-	enum lmac_type lmac_type;
-	int lane_to_sds;
-	int use_training;
-	uint64_t reg_base;
-	struct pci_dev *pdev;
-
-	enum qlm_mode qlm_mode;
-} bgx;
-
-int lmac_count = 0;	/* Total no of LMACs in system */
-struct bgx *bgx_vnic[MAX_BGX_THUNDER];
+	u8			bgx_id;
+	u8			qlm_mode;
+	struct	lmac		lmac[MAX_LMAC_PER_BGX];
+	int			lmac_count;
+	int                     lmac_type;
+	int                     lane_to_sds;
+	int			use_training;
+	void __iomem		*reg_base;
+	struct pci_dev		*pdev;
+};
 
-static int bgx_lmac_xaui_link_init(struct lmac *lmac);
+
+struct bgx *bgx_vnic[MAX_BGX_THUNDER];
+static int lmac_count = 0; /* Total no of LMACs in system */
 
 /* Register read/write APIs */
 static uint64_t bgx_reg_read(struct bgx *bgx, uint8_t lmac, uint64_t offset)
 {
-	uint64_t addr = bgx->reg_base + ((uint32_t) lmac << 20) + offset;
+	uint64_t addr = (uintptr_t)bgx->reg_base +
+				((uint32_t)lmac << 20) + offset;
 
 	return readq((void *)addr);
 }
@@ -68,17 +72,19 @@ static uint64_t bgx_reg_read(struct bgx *bgx, uint8_t lmac, uint64_t offset)
 static void bgx_reg_write(struct bgx *bgx, uint8_t lmac,
 			  uint64_t offset, uint64_t val)
 {
-	uint64_t addr = bgx->reg_base + ((uint32_t) lmac << 20) + offset;
+	uint64_t addr = (uintptr_t)bgx->reg_base + 
+				((uint32_t)lmac << 20) + offset;
 
 	writeq(val, (void *)addr);
 }
 
 static void bgx_reg_modify(struct bgx *bgx, uint8_t lmac,
-			   uint64_t offset, uint64_t val)
+                           uint64_t offset, uint64_t val)
 {
-	uint64_t addr = bgx->reg_base + ((uint32_t) lmac << 20) + offset;
+	uint64_t addr = (uintptr_t)bgx->reg_base +
+				((uint32_t)lmac << 20) + offset;
 
-	writeq(val | bgx_reg_read(bgx, lmac, offset), (void *)addr);
+        writeq(val | bgx_reg_read(bgx, lmac, offset), (void *)addr);
 }
 
 static int bgx_poll_reg(struct bgx *bgx, uint8_t lmac,
@@ -99,6 +105,26 @@ static int bgx_poll_reg(struct bgx *bgx, uint8_t lmac,
 	return 1;
 }
 
+const u8 *bgx_get_lmac_mac(int node, int bgx_idx, int lmacid)
+{
+	struct bgx *bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+
+	if (bgx)
+		return bgx->lmac[lmacid].mac;
+
+	return NULL;
+}
+
+void bgx_set_lmac_mac(int node, int bgx_idx, int lmacid, const u8 *mac)
+{
+	struct bgx *bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+
+	if (!bgx)
+		return;
+
+	memcpy(bgx->lmac[lmacid].mac, mac, 6);
+}
+
 /* Return number of BGX present in HW */
 void bgx_get_count(int node, int *bgx_count)
 {
@@ -126,116 +152,20 @@ int bgx_get_lmac_count(int node, int bgx_idx)
 	return 0;
 }
 
-static void bgx_lmac_change_link_state(struct lmac *lmac)
-{
-	struct bgx *bgx = lmac->bgx;
-	uint64_t cmr_cfg;
-	uint64_t port_cfg = 0;
-	uint64_t misc_ctl = 0;
-
-	cmr_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_CMRX_CFG);
-	cmr_cfg &= ~CMR_EN;
-	bgx_reg_write(bgx, lmac->lmacid, BGX_CMRX_CFG, cmr_cfg);
-
-	port_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG);
-	misc_ctl = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_PCS_MISCX_CTL);
-
-	if (lmac->link_up) {
-		misc_ctl &= ~PCS_MISC_CTL_GMX_ENO;
-		port_cfg &= ~GMI_PORT_CFG_DUPLEX;
-		port_cfg |= (lmac->last_duplex << 2);
-	} else {
-		misc_ctl |= PCS_MISC_CTL_GMX_ENO;
-	}
-
-	switch (lmac->last_speed) {
-	case 10:
-		port_cfg &= ~GMI_PORT_CFG_SPEED;	/* speed 0 */
-		port_cfg |= GMI_PORT_CFG_SPEED_MSB;	/* speed_msb 1 */
-		port_cfg &= ~GMI_PORT_CFG_SLOT_TIME;	/* slottime 0 */
-		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
-		misc_ctl |= 50;	/* samp_pt */
-		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 64);
-		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_BURST, 0);
-		break;
-	case 100:
-		port_cfg &= ~GMI_PORT_CFG_SPEED;	/* speed 0 */
-		port_cfg &= ~GMI_PORT_CFG_SPEED_MSB;	/* speed_msb 0 */
-		port_cfg &= ~GMI_PORT_CFG_SLOT_TIME;	/* slottime 0 */
-		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
-		misc_ctl |= 5;	/* samp_pt */
-		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 64);
-		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_BURST, 0);
-		break;
-	case 1000:
-		port_cfg |= GMI_PORT_CFG_SPEED;	/* speed 1 */
-		port_cfg &= ~GMI_PORT_CFG_SPEED_MSB;	/* speed_msb 0 */
-		port_cfg |= GMI_PORT_CFG_SLOT_TIME;	/* slottime 1 */
-		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
-		misc_ctl |= 1;	/* samp_pt */
-		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 512);
-		if (lmac->last_duplex)
-			bgx_reg_write(bgx, lmac->lmacid,
-				      BGX_GMP_GMI_TXX_BURST, 0);
-		else
-			bgx_reg_write(bgx, lmac->lmacid,
-				      BGX_GMP_GMI_TXX_BURST, 8192);
-		break;
-	default:
-		break;
-	}
-	bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_PCS_MISCX_CTL, misc_ctl);
-	bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG, port_cfg);
-
-	port_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG);
-
-	/* renable lmac */
-	cmr_cfg |= CMR_EN;
-	bgx_reg_write(bgx, lmac->lmacid, BGX_CMRX_CFG, cmr_cfg);
-}
-
-static void bgx_lmac_handler(struct eth_device *netdev)
+void bgx_lmac_rx_tx_enable(int node, int bgx_idx, int lmacid, bool enable)
 {
-	struct lmac *lmac = container_of(netdev, struct lmac, netdev);
-	struct phy_device *phydev = lmac->phydev;
-	int link_changed = 0;
+	struct bgx *bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+	u64 cfg;
 
-	if (!lmac)
+	if (!bgx)
 		return;
 
-	if (!phydev->link && lmac->last_link)
-		link_changed = -1;
-
-	if (phydev->link &&
-	    (lmac->last_duplex != phydev->duplex ||
-	     lmac->last_link != phydev->link ||
-	     lmac->last_speed != phydev->speed)) {
-		link_changed = 1;
-	}
-
-	lmac->last_link = phydev->link;
-	lmac->last_speed = phydev->speed;
-	lmac->last_duplex = phydev->duplex;
-
-	if (!link_changed)
-		return;
-
-	if (link_changed > 0) {
-		printf("LMAC%d: Link is up - %d/%s\n", lmac->lmacid_bd,
-		       phydev->speed,
-		       DUPLEX_FULL == phydev->duplex ? "Full" : "Half");
-		lmac->link_up = true;
-	} else {
-		lmac->link_up = false;
-		printf("LMAC%d: Link is down\n", lmac->lmacid_bd);
-	}
-
-	if (lmac->is_sgmii) {
-		bgx_lmac_change_link_state(lmac);
-	} else {
-		if (!lmac->link_up)
-			bgx_lmac_xaui_link_init(lmac);
-	}
+	cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_CFG);
+	if (enable)
+		cfg |= CMR_PKT_RX_EN | CMR_PKT_TX_EN;
+	else
+		cfg &= ~(CMR_PKT_RX_EN | CMR_PKT_TX_EN);
+	bgx_reg_write(bgx, lmacid, BGX_CMRX_CFG, cfg);
 }
 
 static void bgx_flush_dmac_addrs(struct bgx *bgx, uint64_t lmac)
@@ -245,28 +175,60 @@ static void bgx_flush_dmac_addrs(struct bgx *bgx, uint64_t lmac)
 
 	while (bgx->lmac[lmac].dmac > 0) {
 		offset = ((bgx->lmac[lmac].dmac - 1) * sizeof(dmac)) +
-		    (lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
-		addr = bgx->reg_base + BGX_CMR_RX_DMACX_CAM + offset;
-		writeq(dmac, (void *)addr);
+			(lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
+		addr = (uintptr_t)bgx->reg_base + 
+				BGX_CMR_RX_DMACX_CAM + offset;
+		writeq_relaxed(dmac, (void *)addr);
 		bgx->lmac[lmac].dmac--;
 	}
 }
 
+/* Configure BGX LMAC in internal loopback mode */
+void bgx_lmac_internal_loopback(int node, int bgx_idx,
+				int lmac_idx, bool enable)
+{
+	struct bgx *bgx;
+	struct lmac *lmac;
+	u64    cfg;
+
+	bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+	if (!bgx)
+		return;
+
+	lmac = &bgx->lmac[lmac_idx];
+	if (lmac->is_sgmii) {
+		cfg = bgx_reg_read(bgx, lmac_idx, BGX_GMP_PCS_MRX_CTL);
+		if (enable)
+			cfg |= PCS_MRX_CTL_LOOPBACK1;
+		else
+			cfg &= ~PCS_MRX_CTL_LOOPBACK1;
+		bgx_reg_write(bgx, lmac_idx, BGX_GMP_PCS_MRX_CTL, cfg);
+	} else {
+		cfg = bgx_reg_read(bgx, lmac_idx, BGX_SPUX_CONTROL1);
+		if (enable)
+			cfg |= SPU_CTL_LOOPBACK;
+		else
+			cfg &= ~SPU_CTL_LOOPBACK;
+		bgx_reg_write(bgx, lmac_idx, BGX_SPUX_CONTROL1, cfg);
+	}
+}
+
 static int bgx_lmac_sgmii_init(struct bgx *bgx, int lmacid)
 {
-	uint64_t cfg;
+	u64 cfg;
 
 	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_THRESH, 0x30);
 	/* max packet size */
-	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_RXX_JABBER, 9216);
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_RXX_JABBER, MAX_FRAME_SIZE);
 
 	/* Disable frame alignment if using preamble */
 	cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND);
-
 	if (cfg & 1)
 		bgx_reg_write(bgx, lmacid, BGX_GMP_GMI_TXX_SGMII_CTL, 0);
+
 	/* Enable lmac */
 	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG, CMR_EN);
+
 	/* PCS reset */
 	bgx_reg_modify(bgx, lmacid, BGX_GMP_PCS_MRX_CTL, PCS_MRX_CTL_RESET);
 	if (bgx_poll_reg(bgx, lmacid, BGX_GMP_PCS_MRX_CTL,
@@ -279,8 +241,8 @@ static int bgx_lmac_sgmii_init(struct bgx *bgx, int lmacid)
 	cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_PCS_MRX_CTL);
 	cfg &= ~PCS_MRX_CTL_PWR_DN;
 	cfg |= (PCS_MRX_CTL_RST_AN | PCS_MRX_CTL_AN_EN);
-
 	bgx_reg_write(bgx, lmacid, BGX_GMP_PCS_MRX_CTL, cfg);
+
 	if (bgx_poll_reg(bgx, lmacid, BGX_GMP_PCS_MRX_STATUS,
 			 PCS_MRX_STATUS_AN_CPT, false)) {
 		printf("BGX AN_CPT not completed\n");
@@ -292,7 +254,7 @@ static int bgx_lmac_sgmii_init(struct bgx *bgx, int lmacid)
 
 static int bgx_lmac_xaui_init(struct bgx *bgx, int lmacid, int lmac_type)
 {
-	uint64_t cfg;
+	u64 cfg;
 
 	/* Reset SPU */
 	bgx_reg_modify(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET);
@@ -307,7 +269,13 @@ static int bgx_lmac_xaui_init(struct bgx *bgx, int lmacid, int lmac_type)
 	bgx_reg_write(bgx, lmacid, BGX_CMRX_CFG, cfg);
 
 	bgx_reg_modify(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_LOW_POWER);
-	bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
+	/* Set interleaved running disparity for RXAUI */
+	if (bgx->lmac_type != BGX_MODE_RXAUI)
+		bgx_reg_modify(bgx, lmacid,
+			       BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
+	else
+		bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL,
+			       SPU_MISC_CTL_RX_DIS | SPU_MISC_CTL_INTLV_RDISP);
 
 	/* clear all interrupts */
 	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_RX_INT);
@@ -349,9 +317,9 @@ static int bgx_lmac_xaui_init(struct bgx *bgx, int lmacid, int lmac_type)
 	cfg = cfg & (~((1ULL << 25) | (1ULL << 22) | (1ULL << 12)));
 	bgx_reg_write(bgx, lmacid, BGX_SPUX_AN_ADV, cfg);
 
-	cfg = bgx_reg_read(bgx, lmacid, BGX_SPU_DBG_CONTROL);
+	cfg = bgx_reg_read(bgx, 0, BGX_SPU_DBG_CONTROL);
 	cfg &= ~SPU_DBG_CTL_AN_ARB_LINK_CHK_EN;
-	bgx_reg_write(bgx, lmacid, BGX_SPU_DBG_CONTROL, cfg);
+	bgx_reg_write(bgx, 0, BGX_SPU_DBG_CONTROL, cfg);
 
 	/* Enable lmac */
 	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG, CMR_EN);
@@ -368,28 +336,17 @@ static int bgx_lmac_xaui_init(struct bgx *bgx, int lmacid, int lmac_type)
 	/* take lmac_count into account */
 	bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_THRESH, (0x100 - 1));
 	/* max packet size */
-	bgx_reg_modify(bgx, lmacid, BGX_SMUX_RX_JABBER, 9216);
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_RX_JABBER, MAX_FRAME_SIZE);
 
 	return 0;
 }
 
-static int bgx_lmac_xaui_link_init(struct lmac *lmac)
+static int bgx_xaui_check_link(struct lmac *lmac)
 {
 	struct bgx *bgx = lmac->bgx;
-	uint64_t cfg;
 	int lmacid = lmac->lmacid;
-	int tx_link_ok, rx_link_ok, rcv_link;
 	int lmac_type = bgx->lmac_type;
-
-	tx_link_ok = bgx_reg_read(bgx, lmacid,
-				  BGX_SMUX_TX_CTL) & SMU_TX_CTL_LNK_STATUS;
-	rx_link_ok = bgx_reg_read(bgx, lmacid,
-				  BGX_SMUX_RX_CTL) & SMU_RX_CTL_STATUS;
-	rcv_link = bgx_reg_read(bgx, lmacid,
-				BGX_SPUX_STATUS1) & SPU_STATUS1_RCV_LNK;
-
-	if ((tx_link_ok == 0) && (rx_link_ok == 0) && rcv_link)
-		return 0;
+	u64 cfg;
 
 	bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
 	if (bgx->use_training) {
@@ -448,20 +405,18 @@ static int bgx_lmac_xaui_link_init(struct lmac *lmac)
 	/* Wait for MAC RX to be ready */
 	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_RX_CTL,
 			 SMU_RX_CTL_STATUS, true)) {
-		printf("SMU RX link not okay\n");
+		printf( "SMU RX link not okay\n");
 		return -1;
 	}
 
 	/* Wait for BGX RX to be idle */
-	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL,
-			 SMU_CTL_RX_IDLE, false)) {
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL, SMU_CTL_RX_IDLE, false)) {
 		printf("SMU RX not idle\n");
 		return -1;
 	}
 
 	/* Wait for BGX TX to be idle */
-	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL,
-			 SMU_CTL_TX_IDLE, false)) {
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL, SMU_CTL_TX_IDLE, false)) {
 		printf("SMU TX not idle\n");
 		return -1;
 	}
@@ -485,10 +440,40 @@ static int bgx_lmac_xaui_link_init(struct lmac *lmac)
 	return 0;
 }
 
-int bgx_lmac_enable(int bgxid, int8_t lmacid)
+static void bgx_poll_for_link(struct lmac *lmac)
+{
+	u64 link;
+
+	/* Receive link is latching low. Force it high and verify it */
+	bgx_reg_modify(lmac->bgx, lmac->lmacid,
+		       BGX_SPUX_STATUS1, SPU_STATUS1_RCV_LNK);
+	bgx_poll_reg(lmac->bgx, lmac->lmacid, BGX_SPUX_STATUS1,
+		     SPU_STATUS1_RCV_LNK, false);
+
+	link = bgx_reg_read(lmac->bgx, lmac->lmacid, BGX_SPUX_STATUS1);
+	if (link & SPU_STATUS1_RCV_LNK) {
+		lmac->link_up = 1;
+		if (lmac->bgx->lmac_type == BGX_MODE_XLAUI)
+			lmac->last_speed = 40000;
+		else
+			lmac->last_speed = 10000;
+		lmac->last_duplex = 1;
+	} else {
+		lmac->link_up = 0;
+		lmac->last_speed = 0;
+		lmac->last_duplex = 0;
+	}
+
+	if (lmac->last_link != lmac->link_up) {
+		lmac->last_link = lmac->link_up;
+		if (lmac->link_up)
+			bgx_xaui_check_link(lmac);
+	}
+}
+
+static int bgx_lmac_enable(struct bgx *bgx, int8_t lmacid)
 {
 	struct lmac *lmac;
-	struct bgx *bgx = bgx_vnic[bgxid];
 	uint64_t cfg;
 	int ret;
 
@@ -509,12 +494,12 @@ int bgx_lmac_enable(int bgxid, int8_t lmacid)
 
 	if (lmac->is_sgmii) {
 		cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND);
-		cfg |= ((1ull << 2) | (1ull << 1));	/* FCS and PAD */
+		cfg |= ((1ull << 2) | (1ull << 1)); /* FCS and PAD */
 		bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND, cfg);
 		bgx_reg_write(bgx, lmacid, BGX_GMP_GMI_TXX_MIN_PKT, 60 - 1);
 	} else {
 		cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_APPEND);
-		cfg |= ((1ull << 2) | (1ull << 1));	/* FCS and PAD */
+		cfg |= ((1ull << 2) | (1ull << 1)); /* FCS and PAD */
 		bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_APPEND, cfg);
 		bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_MIN_PKT, 60 + 4);
 	}
@@ -523,40 +508,40 @@ int bgx_lmac_enable(int bgxid, int8_t lmacid)
 	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG,
 		       CMR_EN | CMR_PKT_RX_EN | CMR_PKT_TX_EN);
 
-	if (!lmac->is_sgmii) {
-		printf("%s: Only SGMII is implemented currently\n", __func__);
-		return 1;
-	}
-
-	lmac->phydev = phy_connect(lmac->mii_bus, lmac->lmacid,
-				   &lmac->netdev, PHY_INTERFACE_MODE_SGMII);
+	
+	if ((bgx->lmac_type != BGX_MODE_XFI) &&
+	    (bgx->lmac_type != BGX_MODE_XLAUI) &&
+	    (bgx->lmac_type != BGX_MODE_40G_KR) &&
+	    (bgx->lmac_type != BGX_MODE_10G_KR)) {
+		lmac->phydev = phy_connect(lmac->mii_bus, lmac->lmacid,
+					   &lmac->netdev, PHY_INTERFACE_MODE_SGMII);
 
-	if (!lmac->phydev)
-		return -1;
+		if (!lmac->phydev)
+			return -1;
 
-	ret = phy_config(lmac->phydev);
-	if (ret) {
-		printf("%s: Could not initialize PHY %s\n",
-		       lmac->netdev.name, lmac->phydev->dev->name);
-		return ret;
-	}
+		ret = phy_config(lmac->phydev);
+		if (ret) {
+			printf("%s: Could not initialize PHY %s\n",
+				lmac->netdev.name, lmac->phydev->dev->name);
+			return ret;
+		}
 
-	ret = phy_startup(lmac->phydev);
-	if (ret) {
-		printf("%s: Could not initialize PHY %s\n",
-		       lmac->netdev.name, lmac->phydev->dev->name);
-		return ret;
+		ret = phy_startup(lmac->phydev);
+		if (ret) {
+			printf("%s: Could not initialize PHY %s\n",
+				lmac->netdev.name, lmac->phydev->dev->name);
+			return ret;
+		}
+	} else {
+		bgx_poll_for_link(lmac);
 	}
 
-	bgx_lmac_handler(&lmac->netdev);
-
 	return 0;
 }
 
-void bgx_lmac_disable(int bgxid, uint8_t lmacid)
+void bgx_lmac_disable(struct bgx *bgx, uint8_t lmacid)
 {
 	struct lmac *lmac;
-	struct bgx *bgx = bgx_vnic[bgxid];
 	uint64_t cmrx_cfg;
 
 	lmac = &bgx->lmac[lmacid];
@@ -577,24 +562,21 @@ static int bgx_lmac_count(struct bgx *bgx)
 	const char *boardtype = getenv("board");
 	int lmac_count = 0;
 
-	if (!boardtype)
-		return lmac_count;
-
-	if (!strcmp(boardtype, "ebb8800")) {
+	if (!strcmp(boardtype, "ebb8800") || !strcmp(boardtype, "ebb8804")) {
 		switch (bgx->qlm_mode) {
 		case QLM_MODE_SGMII:
 		case QLM_MODE_XFI_4X1:
 		case QLM_MODE_10G_KR_4X1:
 			lmac_count = 4;
-		break;
+			break;
 		case QLM_MODE_XAUI_1X4:
 		case QLM_MODE_XLAUI_1X4:
 		case QLM_MODE_40G_KR4_1X4:
 			lmac_count = 1;
-		break;
+			break;
 		case QLM_MODE_RXAUI_2X2:
 			lmac_count = 2;
-		break;
+			break;
 		}
 	} else if (!strcmp(boardtype, "crb_1s")) {
 		switch (bgx->bgx_id) {
@@ -662,6 +644,8 @@ static void bgx_init_hw(struct bgx *bgx)
 {
 	int i;
 
+	debug("bgx->bgx_id: %u\n", bgx->bgx_id);
+
 	bgx_set_num_ports(bgx);
 
 	bgx_reg_modify(bgx, 0, BGX_CMR_GLOBAL_CFG, CMR_GLOBAL_CFG_FCS_STRIP);
@@ -680,7 +664,7 @@ static void bgx_init_hw(struct bgx *bgx)
 			continue;
 		}
 		bgx_reg_write(bgx, i, BGX_CMRX_CFG,
-			      (bgx->lmac_type << 8) | (bgx->lane_to_sds + i));
+						(bgx->lmac_type << 8) | (bgx->lane_to_sds + i));
 		bgx->lmac[i].lmacid_bd = lmac_count;
 		lmac_count++;
 	}
@@ -692,7 +676,7 @@ static void bgx_init_hw(struct bgx *bgx)
 	for (i = 0; i < bgx->lmac_count; i++)
 		bgx_reg_modify(bgx, 0, BGX_CMR_CHAN_MSK_AND,
 			       ((1ULL << MAX_BGX_CHANS_PER_LMAC) - 1) <<
-			       (i * MAX_BGX_CHANS_PER_LMAC));
+				(i * MAX_BGX_CHANS_PER_LMAC));
 
 	/* Disable all MAC filtering */
 	for (i = 0; i < RX_DMAC_COUNT; i++)
@@ -705,33 +689,32 @@ static void bgx_init_hw(struct bgx *bgx)
 
 static void bgx_get_qlm_mode(struct bgx *bgx)
 {
-	int lmac_type;
+        int lmac_type;
 	int train_en;
 
-	/* Read LMAC0 type to figure out QLM mode
+        /* Read LMAC0 type to figure out QLM mode
 	 * This is configured by low level firmware
 	 */
-
-	lmac_type = bgx_reg_read(bgx, 0, BGX_CMRX_CFG);
-	lmac_type = (lmac_type >> 8) & 0x07;
-
-	train_en = bgx_reg_read(bgx, 0, BGX_SPUX_BR_PMD_CRTL) &
-	    SPU_PMD_CRTL_TRAIN_EN;
-
-	switch (lmac_type) {
-	case BGX_MODE_SGMII:
-		bgx->qlm_mode = QLM_MODE_SGMII;
+        lmac_type = bgx_reg_read(bgx, 0, BGX_CMRX_CFG);
+        lmac_type = (lmac_type >> 8) & 0x07;
+
+	train_en = bgx_reg_read(bgx, 0,
+			 BGX_SPUX_BR_PMD_CRTL) & SPU_PMD_CRTL_TRAIN_EN;
+	printf("\n");
+        switch(lmac_type) {
+        case BGX_MODE_SGMII:
+                bgx->qlm_mode = QLM_MODE_SGMII;
 		printf("BGX%d QLM mode: SGMII\n", bgx->bgx_id);
-		break;
-	case BGX_MODE_XAUI:
-		bgx->qlm_mode = QLM_MODE_XAUI_1X4;
+                break;
+        case BGX_MODE_XAUI:
+                bgx->qlm_mode = QLM_MODE_XAUI_1X4;
 		printf("BGX%d QLM mode: XAUI\n", bgx->bgx_id);
-		break;
-	case BGX_MODE_RXAUI:
-		bgx->qlm_mode = QLM_MODE_RXAUI_2X2;
+                break;
+        case BGX_MODE_RXAUI:
+                bgx->qlm_mode = QLM_MODE_RXAUI_2X2;
 		printf("BGX%d QLM mode: RXAUI\n", bgx->bgx_id);
-		break;
-	case BGX_MODE_XFI:
+                break;
+        case BGX_MODE_XFI:
 		if (!train_en) {
 			bgx->qlm_mode = QLM_MODE_XFI_4X1;
 			printf("BGX%d QLM mode: XFI\n", bgx->bgx_id);
@@ -739,8 +722,8 @@ static void bgx_get_qlm_mode(struct bgx *bgx)
 			bgx->qlm_mode = QLM_MODE_10G_KR_4X1;
 			printf("BGX%d QLM mode: 10G_KR\n", bgx->bgx_id);
 		}
-		break;
-	case BGX_MODE_XLAUI:
+                break;
+        case BGX_MODE_XLAUI:
 		if (!train_en) {
 			bgx->qlm_mode = QLM_MODE_XLAUI_1X4;
 			printf("BGX%d QLM mode: XLAUI\n", bgx->bgx_id);
@@ -748,43 +731,66 @@ static void bgx_get_qlm_mode(struct bgx *bgx)
 			bgx->qlm_mode = QLM_MODE_40G_KR4_1X4;
 			printf("BGX%d QLM mode: 40G_KR4\n", bgx->bgx_id);
 		}
-		break;
-	}
+                break;
+        }
 }
 
-int thunderx_bgx_initialize(unsigned int bgx_idx,
-			    unsigned int smi_idx, unsigned int node)
+#define	GSERX_CFG(x) (0x000087E090000080ull + (x) * 0x1000000ull)
+#define		GSERX_CFG_BGX	(1 << 2)
+
+int thunderx_bgx_initialize(unsigned int bgx_idx, unsigned int smi_idx, unsigned int node)
 {
 	int err;
 	struct bgx *bgx = NULL;
 	uint8_t lmac = 0;
 	char mii_name[10];
+	uint64_t gser_cfg;
+
+	gser_cfg = readq(CSR_PA(node, GSERX_CFG(bgx_idx))) & GSERX_CFG_BGX;
+
+	if (!gser_cfg)
+		return -ENODEV;
 
 	bgx = malloc(sizeof(struct bgx));
 
 	/* MAP configuration registers */
-	bgx->reg_base = CSR_PA(BGXX_PF_BAR0(bgx_idx), node);
+	bgx->reg_base = (void *)CSR_PA(node, BGXX_PF_BAR0(bgx_idx));
 	bgx->bgx_id = bgx_idx + node * MAX_BGX_PER_CN88XX;
 
 	bgx_vnic[bgx->bgx_id] = bgx;
+	bgx_get_qlm_mode(bgx);
 	debug("bgx_vnic[%u]: %p\n", bgx->bgx_id, bgx);
 
 	snprintf(mii_name, sizeof(mii_name), "thunderx%d", smi_idx);
 
-	for (lmac = 0; lmac < 4; lmac++) {
-		bgx->lmac[lmac].lmacid_bd = bgx->bgx_id;
-		bgx->lmac[lmac].lmacid = lmac + bgx->bgx_id * 4;
-		bgx->lmac[lmac].mii_bus = miiphy_get_dev_by_name(mii_name);
+	debug("mii_name: %s\n", mii_name);
 
-		bgx->lmac_count++;
-	}
+	bgx_init_hw(bgx);
 
-	bgx_get_qlm_mode(bgx);
+	/* Enable all LMACs */
+	for (lmac = 0; lmac < bgx->lmac_count; lmac++) {
+		bgx->lmac[lmac].lmacid = lmac;
+		if (bgx->lmac_type == BGX_MODE_SGMII) {
+			bgx->lmac[lmac].mii_bus = miiphy_get_dev_by_name(mii_name);
+			debug("bgx->lmac[lmac].mii_bus: %p\n", bgx->lmac[lmac].mii_bus);
+			if (!bgx->lmac[lmac].mii_bus) {
+				printf("MDIO device %s not found\n", mii_name);
+				err = -ENODEV;
+				goto error;
+			}
+		}
 
-	bgx_init_hw(bgx);
+		err = bgx_lmac_enable(bgx, lmac);
+		if (err) {
+			printf("BGX%d failed to enable lmac%d\n",
+				bgx->bgx_id, lmac);
+			goto error;
+		}
+	}
 
 	return 0;
-
+error:
+	bgx_vnic[bgx->bgx_id] = NULL;
 	free(bgx);
 	return err;
 }
diff --git a/drivers/net/cavium/thunder_bgx.h b/drivers/net/cavium/thunder_bgx.h
index d4beb636c0..32c9351495 100644
--- a/drivers/net/cavium/thunder_bgx.h
+++ b/drivers/net/cavium/thunder_bgx.h
@@ -10,26 +10,27 @@
 #ifndef THUNDER_BGX_H
 #define THUNDER_BGX_H
 
-#define MAX_BGX_THUNDER		8	/* Max 4 nodes, 2 per node */
-#define MAX_BGX_PER_CN88XX		2
-#define MAX_LMAC_PER_BGX		4
-#define MAX_BGX_CHANS_PER_LMAC	16
-#define MAX_DMAC_PER_LMAC		8
+#define    MAX_BGX_THUNDER			8 /* Max 4 nodes, 2 per node */
+#define    MAX_BGX_PER_CN88XX			2
+#define    MAX_LMAC_PER_BGX			4
+#define    MAX_BGX_CHANS_PER_LMAC		16
+#define    MAX_DMAC_PER_LMAC			8
+#define    MAX_FRAME_SIZE			9216
 
-#define MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE	2
+#define    MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE	2
 
-#define MAX_LMAC	(MAX_BGX_PER_CN88XX * MAX_LMAC_PER_BGX)
+#define    MAX_LMAC	(MAX_BGX_PER_CN88XX * MAX_LMAC_PER_BGX)
 
-#define NODE_ID_MASK	0x300000000000
-#define NODE_ID(x)	((x & NODE_ID_MASK) >> 44)
+#define    NODE_ID_MASK				0x300000000000
+#define    NODE_ID(x)				((x & NODE_ID_MASK) >> 44)
 
 /* Registers */
 #define BGX_CMRX_CFG			0x00
-#define CMR_PKT_TX_EN			(1ull << 13)
-#define CMR_PKT_RX_EN			(1ull << 14)
-#define CMR_EN				(1ull << 15)
+#define CMR_PKT_TX_EN				(1ull << 13)
+#define CMR_PKT_RX_EN				(1ull << 14)
+#define CMR_EN					(1ull << 15)
 #define BGX_CMR_GLOBAL_CFG		0x08
-#define CMR_GLOBAL_CFG_FCS_STRIP	(1ull << 6)
+#define CMR_GLOBAL_CFG_FCS_STRIP		(1ull << 6)
 #define BGX_CMRX_RX_ID_MAP		0x60
 #define BGX_CMRX_RX_STAT0		0x70
 #define BGX_CMRX_RX_STAT1		0x78
@@ -45,11 +46,11 @@
 #define BGX_CMRX_RX_BP_DROP		0xC8
 #define BGX_CMRX_RX_DMAC_CTL		0x0E8
 #define BGX_CMR_RX_DMACX_CAM		0x200
-#define RX_DMACX_CAM_EN			(1ull << 48)
-#define RX_DMACX_CAM_LMACID(x)		(x << 49)
-#define RX_DMAC_COUNT			32
+#define RX_DMACX_CAM_EN				(1ull << 48)
+#define RX_DMACX_CAM_LMACID(x)			(x << 49)
+#define RX_DMAC_COUNT				32
 #define BGX_CMR_RX_STREERING		0x300
-#define RX_TRAFFIC_STEER_RULE_COUNT	8
+#define RX_TRAFFIC_STEER_RULE_COUNT		8
 #define BGX_CMR_CHAN_MSK_AND		0x450
 #define BGX_CMR_BIST_STATUS		0x460
 #define BGX_CMR_RX_LMACS		0x468
@@ -74,73 +75,76 @@
 #define BGX_CMR_TX_LMACS		0x1000
 
 #define BGX_SPUX_CONTROL1		0x10000
-#define SPU_CTL_LOW_POWER		(1ull << 11)
-#define SPU_CTL_RESET			(1ull << 15)
+#define SPU_CTL_LOW_POWER			(1ull << 11)
+#define SPU_CTL_LOOPBACK                        (1ull << 14)
+#define SPU_CTL_RESET				(1ull << 15)
 #define BGX_SPUX_STATUS1		0x10008
-#define SPU_STATUS1_RCV_LNK		(1ull << 2)
+#define SPU_STATUS1_RCV_LNK			(1ull << 2)
 #define BGX_SPUX_STATUS2		0x10020
-#define SPU_STATUS2_RCVFLT		(1ull << 10)
+#define SPU_STATUS2_RCVFLT			(1ull << 10)
 #define BGX_SPUX_BX_STATUS		0x10028
-#define SPU_BX_STATUS_RX_ALIGN		(1ull << 12)
+#define SPU_BX_STATUS_RX_ALIGN                  (1ull << 12)
 #define BGX_SPUX_BR_STATUS1		0x10030
-#define SPU_BR_STATUS_BLK_LOCK		(1ull << 0)
-#define SPU_BR_STATUS_RCV_LNK		(1ull << 12)
+#define SPU_BR_STATUS_BLK_LOCK			(1ull << 0)
+#define SPU_BR_STATUS_RCV_LNK			(1ull << 12)
 #define BGX_SPUX_BR_PMD_CRTL		0x10068
-#define SPU_PMD_CRTL_TRAIN_EN		(1ull << 1)
+#define SPU_PMD_CRTL_TRAIN_EN			(1ull << 1)
 #define BGX_SPUX_BR_PMD_LP_CUP		0x10078
 #define BGX_SPUX_BR_PMD_LD_CUP		0x10088
 #define BGX_SPUX_BR_PMD_LD_REP		0x10090
 #define BGX_SPUX_FEC_CONTROL		0x100A0
-#define SPU_FEC_CTL_FEC_EN		(1ull << 0)
-#define SPU_FEC_CTL_ERR_EN		(1ull << 1)
+#define SPU_FEC_CTL_FEC_EN			(1ull << 0)
+#define SPU_FEC_CTL_ERR_EN			(1ull << 1)
 #define BGX_SPUX_AN_CONTROL		0x100C8
-#define SPU_AN_CTL_AN_EN		(1ull << 12)
-#define SPU_AN_CTL_XNP_EN		(1ull << 13)
+#define SPU_AN_CTL_AN_EN			(1ull << 12)
+#define SPU_AN_CTL_XNP_EN			(1ull << 13)
 #define BGX_SPUX_AN_ADV			0x100D8
 #define BGX_SPUX_MISC_CONTROL		0x10218
-#define SPU_MISC_CTL_RX_DIS		(1ull << 12)
+#define SPU_MISC_CTL_INTLV_RDISP		(1ull << 10)
+#define SPU_MISC_CTL_RX_DIS			(1ull << 12)
 #define BGX_SPUX_INT			0x10220	/* +(0..3) << 20 */
 #define BGX_SPUX_INT_W1S		0x10228
 #define BGX_SPUX_INT_ENA_W1C		0x10230
 #define BGX_SPUX_INT_ENA_W1S		0x10238
 #define BGX_SPU_DBG_CONTROL		0x10300
-#define SPU_DBG_CTL_AN_ARB_LINK_CHK_EN	(1ull << 18)
-#define SPU_DBG_CTL_AN_NONCE_MCT_DIS	(1ull << 29)
+#define SPU_DBG_CTL_AN_ARB_LINK_CHK_EN		(1ull << 18)
+#define SPU_DBG_CTL_AN_NONCE_MCT_DIS		(1ull << 29)
 
 #define BGX_SMUX_RX_INT			0x20000
 #define BGX_SMUX_RX_JABBER		0x20030
 #define BGX_SMUX_RX_CTL			0x20048
-#define SMU_RX_CTL_STATUS		(3ull << 0)
+#define SMU_RX_CTL_STATUS			(3ull << 0)
 #define BGX_SMUX_TX_APPEND		0x20100
-#define SMU_TX_APPEND_FCS_D		(1ull << 2)
+#define SMU_TX_APPEND_FCS_D			(1ull << 2)
 #define BGX_SMUX_TX_MIN_PKT		0x20118
 #define BGX_SMUX_TX_INT			0x20140
 #define BGX_SMUX_TX_CTL			0x20178
-#define SMU_TX_CTL_DIC_EN		(1ull << 0)
-#define SMU_TX_CTL_UNI_EN		(1ull << 1)
-#define SMU_TX_CTL_LNK_STATUS		(3ull << 4)
+#define SMU_TX_CTL_DIC_EN			(1ull << 0)
+#define SMU_TX_CTL_UNI_EN			(1ull << 1)
+#define SMU_TX_CTL_LNK_STATUS			(3ull << 4)
 #define BGX_SMUX_TX_THRESH		0x20180
 #define BGX_SMUX_CTL			0x20200
-#define SMU_CTL_RX_IDLE			(1ull << 0)
-#define SMU_CTL_TX_IDLE			(1ull << 1)
+#define SMU_CTL_RX_IDLE				(1ull << 0)
+#define SMU_CTL_TX_IDLE				(1ull << 1)
 
 #define BGX_GMP_PCS_MRX_CTL		0x30000
-#define	PCS_MRX_CTL_RST_AN		(1ull << 9)
-#define	PCS_MRX_CTL_PWR_DN		(1ull << 11)
-#define	PCS_MRX_CTL_AN_EN		(1ull << 12)
-#define	PCS_MRX_CTL_RESET		(1ull << 15)
+#define	PCS_MRX_CTL_RST_AN			(1ull << 9)
+#define	PCS_MRX_CTL_PWR_DN			(1ull << 11)
+#define	PCS_MRX_CTL_AN_EN			(1ull << 12)
+#define PCS_MRX_CTL_LOOPBACK1                   (1ull << 14)
+#define	PCS_MRX_CTL_RESET			(1ull << 15)
 #define BGX_GMP_PCS_MRX_STATUS		0x30008
-#define	PCS_MRX_STATUS_AN_CPT		(1ull << 5)
+#define	PCS_MRX_STATUS_AN_CPT			(1ull << 5)
 #define BGX_GMP_PCS_ANX_AN_RESULTS	0x30020
 #define BGX_GMP_PCS_SGM_AN_ADV		0x30068
 #define BGX_GMP_PCS_MISCX_CTL		0x30078
-#define PCS_MISC_CTL_GMX_ENO		(1ull << 11)
-#define PCS_MISC_CTL_SAMP_PT_MASK	0x7Full
+#define PCS_MISC_CTL_GMX_ENO			(1ull << 11)
+#define PCS_MISC_CTL_SAMP_PT_MASK		0x7Full
 #define BGX_GMP_GMI_PRTX_CFG		0x38020
-#define GMI_PORT_CFG_SPEED		(1ull << 1)
-#define GMI_PORT_CFG_DUPLEX		(1ull << 2)
-#define GMI_PORT_CFG_SLOT_TIME		(1ull << 3)
-#define GMI_PORT_CFG_SPEED_MSB		(1ull << 8)
+#define GMI_PORT_CFG_SPEED			(1ull << 1)
+#define GMI_PORT_CFG_DUPLEX			(1ull << 2)
+#define GMI_PORT_CFG_SLOT_TIME			(1ull << 3)
+#define GMI_PORT_CFG_SPEED_MSB			(1ull << 8)
 #define BGX_GMP_GMI_RXX_JABBER		0x38038
 #define BGX_GMP_GMI_TXX_THRESH		0x38210
 #define BGX_GMP_GMI_TXX_APPEND		0x38218
@@ -149,7 +153,7 @@
 #define BGX_GMP_GMI_TXX_MIN_PKT		0x38240
 #define BGX_GMP_GMI_TXX_SGMII_CTL	0x38300
 
-#define BGX_MSIX_VEC_0_29_ADDR		0x400000	/* +(0..29) << 4 */
+#define BGX_MSIX_VEC_0_29_ADDR		0x400000 /* +(0..29) << 4 */
 #define BGX_MSIX_VEC_0_29_CTL		0x400008
 #define BGX_MSIX_PBA_0			0x4F0000
 
@@ -173,42 +177,44 @@
 
 /*  RX_DMAC_CTL configuration*/
 enum MCAST_MODE {
-	MCAST_MODE_REJECT,
-	MCAST_MODE_ACCEPT,
-	MCAST_MODE_CAM_FILTER,
-	RSVD
+		MCAST_MODE_REJECT,
+		MCAST_MODE_ACCEPT,
+		MCAST_MODE_CAM_FILTER,
+		RSVD
 };
 
 #define BCAST_ACCEPT	1
 #define CAM_ACCEPT	1
 
+int thunderx_bgx_initialize(unsigned int bgx_idx, 
+			    unsigned int smi_idx, unsigned int node);
+
+void bgx_add_dmac_addr(uint64_t dmac, int node, int bgx_idx, int lmac);
 void bgx_get_count(int node, int *bgx_count);
 int bgx_get_lmac_count(int node, int bgx);
-
-int bgx_lmac_enable(int bgxid, int8_t lmacid);
-void bgx_lmac_disable(int bgxid, uint8_t lmacid);
+void bgx_print_stats(int bgx_idx, int lmac);
 
 #undef LINK_INTR_ENABLE
 
-enum lmac_type {
-	BGX_MODE_SGMII = 0,	/* 1 lane, 1.250 Gbaud */
-	BGX_MODE_XAUI = 1,	/* 4 lanes, 3.125 Gbaud */
-	BGX_MODE_DXAUI = 1,	/* 4 lanes, 6.250 Gbaud */
-	BGX_MODE_RXAUI = 2,	/* 2 lanes, 6.250 Gbaud */
-	BGX_MODE_XFI = 3,	/* 1 lane, 10.3125 Gbaud */
-	BGX_MODE_XLAUI = 4,	/* 4 lanes, 10.3125 Gbaud */
-	BGX_MODE_10G_KR = 3,	/* 1 lane, 10.3125 Gbaud */
-	BGX_MODE_40G_KR = 4,	/* 4 lanes, 10.3125 Gbaud */
+enum LMAC_TYPE {
+	BGX_MODE_SGMII = 0, /* 1 lane, 1.250 Gbaud */
+	BGX_MODE_XAUI = 1,  /* 4 lanes, 3.125 Gbaud */
+	BGX_MODE_DXAUI = 1, /* 4 lanes, 6.250 Gbaud */
+	BGX_MODE_RXAUI = 2, /* 2 lanes, 6.250 Gbaud */
+	BGX_MODE_XFI = 3,   /* 1 lane, 10.3125 Gbaud */
+	BGX_MODE_XLAUI = 4, /* 4 lanes, 10.3125 Gbaud */
+	BGX_MODE_10G_KR = 3,/* 1 lane, 10.3125 Gbaud */
+	BGX_MODE_40G_KR = 4,/* 4 lanes, 10.3125 Gbaud */
 };
 
 enum qlm_mode {
-	QLM_MODE_SGMII,		/* SGMII, each lane independent */
-	QLM_MODE_XAUI_1X4,	/* 1 XAUI or DXAUI, 4 lanes */
-	QLM_MODE_RXAUI_2X2,	/* 2 RXAUI, 2 lanes each */
-	QLM_MODE_XFI_4X1,	/* 4 XFI, 1 lane each */
-	QLM_MODE_XLAUI_1X4,	/* 1 XLAUI, 4 lanes each */
-	QLM_MODE_10G_KR_4X1,	/* 4 10GBASE-KR, 1 lane each */
-	QLM_MODE_40G_KR4_1X4,	/* 1 40GBASE-KR4, 4 lanes each */
+	QLM_MODE_SGMII,         /* SGMII, each lane independent */
+	QLM_MODE_XAUI_1X4,      /* 1 XAUI or DXAUI, 4 lanes */
+	QLM_MODE_RXAUI_2X2,     /* 2 RXAUI, 2 lanes each */
+	QLM_MODE_XFI_4X1,       /* 4 XFI, 1 lane each */
+	QLM_MODE_XLAUI_1X4,     /* 1 XLAUI, 4 lanes each */
+	QLM_MODE_10G_KR_4X1,    /* 4 10GBASE-KR, 1 lane each */
+	QLM_MODE_40G_KR4_1X4,   /* 1 40GBASE-KR4, 4 lanes each */
 };
 
 #endif /* THUNDER_BGX_H */
diff --git a/drivers/net/cavium/thunderx_smi.c b/drivers/net/cavium/thunderx_smi.c
index 6083e807f5..9fcd332571 100644
--- a/drivers/net/cavium/thunderx_smi.c
+++ b/drivers/net/cavium/thunderx_smi.c
@@ -251,6 +251,8 @@ int thunderx_smi_initialize(bd_t *bis, unsigned int index)
 	struct mii_dev *bus = mdio_alloc();
 	struct thunderx_priv *priv = malloc(sizeof(*priv));
 
+	debug("%s: %d\n", __FUNCTION__, __LINE__);
+
 	if (!bus || !priv) {
 		printf("Failed to allocate ThunderX MDIO bus # %u\n", index);
 		return -1;
@@ -268,5 +270,7 @@ int thunderx_smi_initialize(bd_t *bis, unsigned int index)
 	/* use given name or generate its own unique name */
 	snprintf(bus->name, MDIO_NAME_LEN, "thunderx%d", priv->bus_num);
 
+	debug("%s: %d\n", __FUNCTION__, __LINE__);
+
 	return mdio_register(bus);
 }
-- 
2.29.0

