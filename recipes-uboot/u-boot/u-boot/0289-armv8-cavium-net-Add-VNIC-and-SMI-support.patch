From 5f9a55a743aca382b24cde8257b958c23c7f7760 Mon Sep 17 00:00:00 2001
From: Sergey Temerkhanov <sergey.temerkhanov@auriga.com>
Date: Fri, 13 Mar 2015 18:08:18 +0300
Subject: [PATCH 0289/1239] armv8: cavium: net: Add VNIC and SMI support

---
 drivers/net/Makefile              |    1 +
 drivers/net/cavium/Makefile       |    7 +
 drivers/net/cavium/nic.h          |  417 +++++++++++
 drivers/net/cavium/nic_main.c     |  528 +++++++++++++
 drivers/net/cavium/nic_reg.h      |  247 ++++++
 drivers/net/cavium/nicvf_main.c   |  556 ++++++++++++++
 drivers/net/cavium/nicvf_queues.c | 1153 +++++++++++++++++++++++++++++
 drivers/net/cavium/nicvf_queues.h |  364 +++++++++
 drivers/net/cavium/q_struct.h     |  702 ++++++++++++++++++
 drivers/net/cavium/thunder_bgx.c  |  787 ++++++++++++++++++++
 drivers/net/cavium/thunder_bgx.h  |  222 ++++++
 drivers/net/cavium/thunderx_smi.c |  272 +++++++
 include/cavium/thunderx_smi.h     |   12 +
 include/cavium/thunderx_vnic.h    |   23 +
 14 files changed, 5291 insertions(+)
 create mode 100644 drivers/net/cavium/Makefile
 create mode 100644 drivers/net/cavium/nic.h
 create mode 100644 drivers/net/cavium/nic_main.c
 create mode 100644 drivers/net/cavium/nic_reg.h
 create mode 100644 drivers/net/cavium/nicvf_main.c
 create mode 100644 drivers/net/cavium/nicvf_queues.c
 create mode 100644 drivers/net/cavium/nicvf_queues.h
 create mode 100644 drivers/net/cavium/q_struct.h
 create mode 100644 drivers/net/cavium/thunder_bgx.c
 create mode 100644 drivers/net/cavium/thunder_bgx.h
 create mode 100644 drivers/net/cavium/thunderx_smi.c
 create mode 100644 include/cavium/thunderx_smi.h
 create mode 100644 include/cavium/thunderx_vnic.h

diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index 30991834ec..0abfd5609b 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -62,6 +62,7 @@ obj-$(CONFIG_RENESAS_RAVB) += ravb.o
 obj-$(CONFIG_SMC91111) += smc91111.o
 obj-$(CONFIG_SMC911X) += smc911x.o
 obj-$(CONFIG_TSEC_ENET) += tsec.o fsl_mdio.o
+obj-$(CONFIG_THUNDERX_VNIC) += cavium/
 obj-$(CONFIG_FMAN_ENET) += fsl_mdio.o
 obj-$(CONFIG_ULI526X) += uli526x.o
 obj-$(CONFIG_VSC7385_ENET) += vsc7385.o
diff --git a/drivers/net/cavium/Makefile b/drivers/net/cavium/Makefile
new file mode 100644
index 0000000000..0a2c9a2ff3
--- /dev/null
+++ b/drivers/net/cavium/Makefile
@@ -0,0 +1,7 @@
+#
+# (C) Copyright 2014, Cavium Inc.
+#
+# SPDX-License-Identifier:	GPL-2.0+
+#
+obj-y = thunder_bgx.o nic_main.o nicvf_queues.o \
+		nicvf_main.o thunderx_smi.o
diff --git a/drivers/net/cavium/nic.h b/drivers/net/cavium/nic.h
new file mode 100644
index 0000000000..32ed44148b
--- /dev/null
+++ b/drivers/net/cavium/nic.h
@@ -0,0 +1,417 @@
+/*
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#ifndef NIC_H
+#define	NIC_H
+
+#include <linux/netdevice.h>
+#include "thunder_bgx.h"
+#include <asm/io.h>
+
+/**
+ * Macro to get the physical address of a CSR on a node
+ */
+#define CSR_PA(csr, node) ((csr) | ((uint64_t)(node) << 44))
+
+/* PCI device IDs */
+#define	PCI_DEVICE_ID_THUNDER_NIC_PF	0xA01E
+#define	PCI_DEVICE_ID_THUNDER_NIC_VF	0x0011
+#define	PCI_DEVICE_ID_THUNDER_BGX	0xA026
+
+/* PCI BAR nos */
+#define	PCI_CFG_REG_BAR_NUM		0
+#define	PCI_MSIX_REG_BAR_NUM		4
+
+/* NIC SRIOV VF count */
+#define	MAX_NUM_VFS_SUPPORTED		128
+#define	DEFAULT_NUM_VF_ENABLED		8
+
+#define	NIC_TNS_BYPASS_MODE		0
+#define	NIC_TNS_MODE			1
+
+/* NIC priv flags */
+#define	NIC_SRIOV_ENABLED		(1 << 0)
+#define	NIC_TNS_ENABLED			(1 << 1)
+
+/* VNIC HW optimiation features */
+#define	VNIC_RX_CSUM_OFFLOAD_SUPPORT
+#undef	VNIC_TX_CSUM_OFFLOAD_SUPPORT
+#undef	VNIC_SG_SUPPORT
+#undef	VNIC_TSO_SUPPORT
+#undef	VNIC_LRO_SUPPORT
+#undef  VNIC_RSS_SUPPORT
+
+/* TSO not supported in Thunder pass1 */
+#ifdef	VNIC_TSO_SUPPORT
+#define	VNIC_SW_TSO_SUPPORT
+#undef	VNIC_HW_TSO_SUPPORT
+#endif
+
+/* ETHTOOL enable or disable, undef this to disable */
+#define	NICVF_ETHTOOL_ENABLE
+
+/* Min/Max packet size */
+#define	NIC_HW_MIN_FRS			64
+#define	NIC_HW_MAX_FRS			9200
+/* 9216 max packet including FCS */
+
+/* Max pkinds */
+#define	NIC_MAX_PKIND			16
+
+/* Rx Channels */
+/* Receive channel configuration in TNS bypass mode
+ * Below is configuration in TNS bypass mode
+ * BGX0-LMAC0-CHAN0 - VNIC CHAN0
+ * BGX0-LMAC1-CHAN0 - VNIC CHAN16
+ * ...
+ * BGX1-LMAC0-CHAN0 - VNIC CHAN128
+ * ...
+ * BGX1-LMAC3-CHAN0 - VNIC CHAN174
+ */
+#define	NIC_INF_COUNT			2	/* No of interfaces */
+#define	NIC_CHANS_PER_INF		128
+#define	NIC_MAX_CHANS			(NIC_INF_COUNT * NIC_CHANS_PER_INF)
+/* No of channel parse indices */
+#define	NIC_CPI_COUNT			2048
+
+/* TNS bypass mode: 1-1 mapping between VNIC and BGX:LMAC */
+#define NIC_MAX_BGX				MAX_BGX_PER_CN88XX
+#define	NIC_CPI_PER_BGX			(NIC_CPI_COUNT / NIC_MAX_BGX)
+#define	NIC_MAX_CPI_PER_LMAC	64	/* Max when CPI_ALG is IP diffserv */
+#define	NIC_RSSI_PER_BGX		(NIC_RSSI_COUNT / NIC_MAX_BGX)
+
+/* Tx scheduling */
+#define	NIC_MAX_TL4				1024
+#define	NIC_MAX_TL4_SHAPERS		256	/* 1 shaper for 4 TL4s */
+#define	NIC_MAX_TL3				256
+#define	NIC_MAX_TL3_SHAPERS		64	/* 1 shaper for 4 TL3s */
+#define	NIC_MAX_TL2				64
+#define	NIC_MAX_TL2_SHAPERS		2	/* 1 shaper for 32 TL2s */
+#define	NIC_MAX_TL1				2
+
+/* TNS bypass mode */
+#define	NIC_TL4_PER_BGX			(NIC_MAX_TL4 / NIC_MAX_BGX)
+#define	NIC_TL4_PER_LMAC		(NIC_MAX_TL4 / NIC_CHANS_PER_INF)
+
+/* NIC VF Interrupts */
+#define	NICVF_INTR_CQ			0
+#define	NICVF_INTR_SQ			1
+#define	NICVF_INTR_RBDR			2
+#define	NICVF_INTR_PKT_DROP		3
+#define	NICVF_INTR_TCP_TIMER	4
+#define	NICVF_INTR_MBOX			5
+#define	NICVF_INTR_QS_ERR		6
+
+#define	NICVF_INTR_CQ_SHIFT			0
+#define	NICVF_INTR_SQ_SHIFT			8
+#define	NICVF_INTR_RBDR_SHIFT		16
+#define	NICVF_INTR_PKT_DROP_SHIFT	20
+#define	NICVF_INTR_TCP_TIMER_SHIFT	21
+#define	NICVF_INTR_MBOX_SHIFT		22
+#define	NICVF_INTR_QS_ERR_SHIFT		23
+
+#define	NICVF_INTR_CQ_MASK		(0xFF << NICVF_INTR_CQ_SHIFT)
+#define	NICVF_INTR_SQ_MASK		(0xFF << NICVF_INTR_SQ_SHIFT)
+#define	NICVF_INTR_RBDR_MASK		(0x03 << NICVF_INTR_RBDR_SHIFT)
+#define	NICVF_INTR_PKT_DROP_MASK	(1 << NICVF_INTR_PKT_DROP_SHIFT)
+#define	NICVF_INTR_TCP_TIMER_MASK	(1 << NICVF_INTR_TCP_TIMER_SHIFT)
+#define	NICVF_INTR_MBOX_MASK		(1 << NICVF_INTR_MBOX_SHIFT)
+#define	NICVF_INTR_QS_ERR_MASK		(1 << NICVF_INTR_QS_ERR_SHIFT)
+
+/* MSI-X interrupts */
+#define	NIC_PF_MSIX_VECTORS		10
+#define	NIC_VF_MSIX_VECTORS		20
+
+#define NIC_PF_INTR_ID_ECC0_SBE		0
+#define NIC_PF_INTR_ID_ECC0_DBE		1
+#define NIC_PF_INTR_ID_ECC1_SBE		2
+#define NIC_PF_INTR_ID_ECC1_DBE		3
+#define NIC_PF_INTR_ID_ECC2_SBE		4
+#define NIC_PF_INTR_ID_ECC2_DBE		5
+#define NIC_PF_INTR_ID_ECC3_SBE		6
+#define NIC_PF_INTR_ID_ECC3_DBE		7
+#define NIC_PF_INTR_ID_MBOX0		8
+#define NIC_PF_INTR_ID_MBOX1		9
+
+/* Global timer for CQ timer thresh interrupts
+ * Calculated for SCLK of 700Mhz
+ * value written should be a 1/16thof what is expected
+ *
+ * 1 tick per ms
+ */
+#define NICPF_CLK_PER_INT_TICK		43750
+
+struct nicvf_cq_poll {
+	uint8_t cq_idx;		/* Completion queue index */
+};
+
+#define	NIC_RSSI_COUNT			4096	/* Total no of RSS indices */
+#define NIC_MAX_RSS_HASH_BITS		8
+#define NIC_MAX_RSS_IDR_TBL_SIZE	(1 << NIC_MAX_RSS_HASH_BITS)
+#define RSS_HASH_KEY_SIZE		5	/* 320 bit key */
+
+#ifdef VNIC_RSS_SUPPORT
+struct nicvf_rss_info {
+	bool enable;
+#define	RSS_L2_EXTENDED_HASH_ENA	(1 << 0)
+#define	RSS_IP_HASH_ENA			(1 << 1)
+#define	RSS_TCP_HASH_ENA		(1 << 2)
+#define	RSS_TCP_SYN_DIS			(1 << 3)
+#define	RSS_UDP_HASH_ENA		(1 << 4)
+#define RSS_L4_EXTENDED_HASH_ENA	(1 << 5)
+#define	RSS_ROCE_ENA			(1 << 6)
+#define	RSS_L3_BI_DIRECTION_ENA		(1 << 7)
+#define	RSS_L4_BI_DIRECTION_ENA		(1 << 8)
+	uint64_t cfg;
+	uint8_t hash_bits;
+	uint16_t rss_size;
+	uint8_t ind_tbl[NIC_MAX_RSS_IDR_TBL_SIZE];
+	uint64_t key[RSS_HASH_KEY_SIZE];
+};
+#endif
+
+enum rx_stats_reg_offset {
+	RX_OCTS = 0x0,
+	RX_UCAST = 0x1,
+	RX_BCAST = 0x2,
+	RX_MCAST = 0x3,
+	RX_RED = 0x4,
+	RX_RED_OCTS = 0x5,
+	RX_ORUN = 0x6,
+	RX_ORUN_OCTS = 0x7,
+	RX_FCS = 0x8,
+	RX_L2ERR = 0x9,
+	RX_DRP_BCAST = 0xa,
+	RX_DRP_MCAST = 0xb,
+	RX_DRP_L3BCAST = 0xc,
+	RX_DRP_L3MCAST = 0xd,
+	RX_STATS_ENUM_LAST,
+};
+
+enum tx_stats_reg_offset {
+	TX_OCTS = 0x0,
+	TX_UCAST = 0x1,
+	TX_BCAST = 0x2,
+	TX_MCAST = 0x3,
+	TX_DROP = 0x4,
+	TX_STATS_ENUM_LAST,
+};
+
+struct nicvf_hw_stats {
+	u64 rx_bytes_ok;
+	u64 rx_ucast_frames_ok;
+	u64 rx_bcast_frames_ok;
+	u64 rx_mcast_frames_ok;
+	u64 rx_fcs_errors;
+	u64 rx_l2_errors;
+	u64 rx_drop_red;
+	u64 rx_drop_red_bytes;
+	u64 rx_drop_overrun;
+	u64 rx_drop_overrun_bytes;
+	u64 rx_drop_bcast;
+	u64 rx_drop_mcast;
+	u64 rx_drop_l3_bcast;
+	u64 rx_drop_l3_mcast;
+	u64 tx_bytes_ok;
+	u64 tx_ucast_frames_ok;
+	u64 tx_bcast_frames_ok;
+	u64 tx_mcast_frames_ok;
+	u64 tx_drops;
+};
+
+struct nicvf_drv_stats {
+	/* Rx */
+	u64 rx_frames_ok;
+	u64 rx_frames_64;
+	u64 rx_frames_127;
+	u64 rx_frames_255;
+	u64 rx_frames_511;
+	u64 rx_frames_1023;
+	u64 rx_frames_1518;
+	u64 rx_frames_jumbo;
+	u64 rx_drops;
+	/* Tx */
+	u64 tx_frames_ok;
+	u64 tx_drops;
+	u64 tx_busy;
+	u64 tx_tso;
+};
+
+struct nicpf {
+	struct eth_device *netdev;
+#define NIC_NODE_ID_MASK	0x300000000000
+#define NIC_NODE_ID(x)		((x & NODE_ID_MASK) >> 44)
+	uint8_t node;
+	unsigned int flags;
+	uint16_t total_vf_cnt;	/* Total num of VF supported */
+	uint16_t num_vf_en;	/* No of VF enabled */
+	uint64_t reg_base;	/* Register start address */
+	struct pkind_cfg pkind;
+	uint8_t bgx_cnt;
+#define	NIC_SET_VF_LMAC_MAP(bgx, lmac)	(((bgx & 0xF) << 4) | (lmac & 0xF))
+#define	NIC_GET_BGX_FROM_VF_LMAC_MAP(map)	((map >> 4) & 0xF)
+#define	NIC_GET_LMAC_FROM_VF_LMAC_MAP(map)	(map & 0xF)
+	uint8_t vf_lmac_map[MAX_LMAC];
+	uint16_t cpi_base[MAX_NUM_VFS_SUPPORTED];
+	uint16_t rss_ind_tbl_size;
+	uint64_t mac[MAX_NUM_VFS_SUPPORTED];
+};
+
+struct nicvf {
+	struct eth_device *netdev;
+	uint8_t vf_id;
+	uint8_t tns_mode;
+	uint8_t node;
+	uint16_t mtu;
+	struct queue_set *qs;
+	uint8_t num_qs;
+	void *addnl_qs;
+	uint16_t vf_mtu;
+	uint64_t reg_base;
+#ifdef VNIC_RSS_SUPPORT
+	struct nicvf_rss_info rss_info;
+#endif
+	uint8_t cpi_alg;
+
+	struct nicvf_hw_stats stats;
+	struct nicvf_drv_stats drv_stats;
+
+	struct nicpf *nicpf;
+};
+
+/* PF <--> VF Mailbox communication
+ * Eight 64bit registers are shared between PF and VF.
+ * Separate set for each VF.
+ * Writing '1' into last register mbx7 means end of message.
+ */
+
+/* PF <--> VF mailbox communication */
+#define	NIC_PF_VF_MAILBOX_SIZE		8
+#define	NIC_PF_VF_MBX_TIMEOUT		2000	/* ms */
+
+/* Mailbox message types */
+/* Is PF ready to rcv msgs */
+#define	NIC_PF_VF_MSG_READY		0x01	/* Is PF ready to rcv msgs */
+#define	NIC_PF_VF_MSG_ACK		0x02	/* ACK the message received */
+#define	NIC_PF_VF_MSG_NACK		0x03	/* NACK the message received */
+#define	NIC_PF_VF_MSG_QS_CFG		0x04	/* Configure Qset */
+#define	NIC_PF_VF_MSG_RQ_CFG		0x05	/* Configure receive queue */
+#define	NIC_PF_VF_MSG_SQ_CFG		0x06	/* Configure Send queue */
+#define	NIC_PF_VF_MSG_RQ_DROP_CFG	0x07	/* Configure receive queue */
+#define	NIC_PF_VF_MSG_SET_MAC		0x08	/* Add MAC ID to DMAC filter */
+#define	NIC_PF_VF_MSG_SET_MAX_FRS	0x09	/* Set max frame size */
+#define	NIC_PF_VF_MSG_CPI_CFG		0x0A	/* Config CPI, RSSI */
+#define	NIC_PF_VF_MSG_RSS_SIZE		0x0B	/* Get RSS indir_tbl size */
+#define	NIC_PF_VF_MSG_RSS_CFG		0x0C	/* Config RSS table */
+#define	NIC_PF_VF_MSG_RSS_CFG_CONT	0x0D	/* RSS config continuation */
+#define	NIC_PF_VF_MSG_RQ_BP_CFG		0x0E
+#define	NIC_PF_VF_MSG_RQ_SW_SYNC	0x0F
+
+struct nic_cfg_msg {
+	uint64_t vf_id;
+	uint64_t tns_mode;
+	uint64_t mac_addr;
+	uint64_t node_id;
+};
+
+/* Qset configuration */
+struct qs_cfg_msg {
+	uint64_t num;
+	uint64_t cfg;
+};
+
+/* Receive queue configuration */
+struct rq_cfg_msg {
+	uint64_t qs_num;
+	uint64_t rq_num;
+	uint64_t cfg;
+};
+
+/* Send queue configuration */
+struct sq_cfg_msg {
+	uint64_t qs_num;
+	uint64_t sq_num;
+	uint64_t cfg;
+};
+
+/* Set VF's MAC address */
+struct set_mac_msg {
+	uint64_t vf_id;
+	uint64_t addr;
+};
+
+/* Set Maximum frame size */
+struct set_frs_msg {
+	uint64_t vf_id;
+	uint64_t max_frs;
+};
+
+/* Set CPI algorithm type */
+struct cpi_cfg_msg {
+	uint64_t vf_id;
+	uint64_t rq_cnt;
+	uint64_t cpi_alg;
+};
+
+#ifdef VNIC_RSS_SUPPORT
+/* Get RSS table size */
+struct rss_sz_msg {
+	uint64_t vf_id;
+	uint64_t ind_tbl_size;
+};
+
+/* Set RSS configuration */
+struct rss_cfg_msg {
+	uint8_t vf_id;
+	uint8_t hash_bits;
+	uint16_t tbl_len;
+	uint16_t tbl_offset;
+#define RSS_IND_TBL_LEN_PER_MBX_MSG	42
+	uint8_t ind_tbl[RSS_IND_TBL_LEN_PER_MBX_MSG];
+};
+#endif
+
+/* Maximum 8 64bit locations */
+struct nic_mbx {
+#define	NIC_PF_VF_MBX_MSG_MASK		0xFFFF
+	uint16_t msg;
+#define	NIC_PF_VF_MBX_LOCK_OFFSET	0
+#define	NIC_PF_VF_MBX_LOCK_VAL(x)	((x >> 16) & 0xFFFF)
+#define	NIC_PF_VF_MBX_LOCK_CLEAR(x)	(x & ~(0xFFFF0000))
+#define	NIC_PF_VF_MBX_LOCK_SET(x)\
+	(NIC_PF_VF_MBX_LOCK_CLEAR(x) | (1 << 16))
+	uint16_t mbx_lock;
+	uint32_t unused;
+	union {
+		struct nic_cfg_msg nic_cfg;
+		struct qs_cfg_msg qs;
+		struct rq_cfg_msg rq;
+		struct sq_cfg_msg sq;
+		struct set_mac_msg mac;
+		struct set_frs_msg frs;
+		struct cpi_cfg_msg cpi_cfg;
+#ifdef VNIC_RSS_SUPPORT
+		struct rss_sz_msg rss_size;
+		struct rss_cfg_msg rss_cfg;
+#endif
+		uint64_t rsvd[6];
+	} data;
+	uint64_t mbx_trigger_intr;
+};
+
+int nicvf_set_real_num_queues(struct eth_device *netdev,
+			      int tx_queues, int rx_queues);
+int nicvf_send_msg_to_pf(struct nicvf *vf, struct nic_mbx *mbx);
+void nicvf_free_pkt(struct nicvf *nic, void *pkt);
+void nicvf_update_stats(struct nicvf *nic);
+
+void nic_handle_mbx_intr(struct nicpf *nic, int vf);
+
+
+#endif /* NIC_H */
diff --git a/drivers/net/cavium/nic_main.c b/drivers/net/cavium/nic_main.c
new file mode 100644
index 0000000000..a7167d0c0e
--- /dev/null
+++ b/drivers/net/cavium/nic_main.c
@@ -0,0 +1,528 @@
+/*
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#include <config.h>
+#include <common.h>
+#include <net.h>
+#include <netdev.h>
+#include <malloc.h>
+#include <miiphy.h>
+
+#include <asm/io.h>
+
+#include <cavium/thunderx_vnic.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "q_struct.h"
+#include "thunder_bgx.h"
+
+#define ETH_HLEN 6
+
+unsigned long rounddown_pow_of_two(unsigned long n)
+{
+	n |= n >> 1;
+	n |= n >> 2;
+	n |= n >> 4;
+	n |= n >> 8;
+	n |= n >> 16;
+	n |= n >> 32;
+
+	return n + 1;
+}
+
+static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg);
+static void nic_tx_channel_cfg(struct nicpf *nic, int vnic, int sq_idx);
+static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf);
+static int nic_rcv_queue_sw_sync(struct nicpf *nic);
+
+/* Register read/write APIs */
+static void nic_reg_write(struct nicpf *nic, uint64_t offset, uint64_t val)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	writeq(val, (void *)addr);
+}
+
+static uint64_t nic_reg_read(struct nicpf *nic, uint64_t offset)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	return readq((void *)addr);
+}
+
+static uint64_t nic_get_mbx_addr(int vf)
+{
+	return NIC_PF_VF_0_127_MAILBOX_0_7 + (vf << NIC_VF_NUM_SHIFT);
+}
+
+static int nic_lock_mbox(struct nicpf *nic, int vf)
+{
+	int timeout = NIC_PF_VF_MBX_TIMEOUT;
+	int sleep = 10;
+	uint64_t lock, mbx_addr;
+
+	mbx_addr = nic_get_mbx_addr(vf) + NIC_PF_VF_MBX_LOCK_OFFSET;
+	lock = NIC_PF_VF_MBX_LOCK_VAL(nic_reg_read(nic, mbx_addr));
+	while (lock) {
+		mdelay(sleep);
+		lock = NIC_PF_VF_MBX_LOCK_VAL(nic_reg_read(nic, mbx_addr));
+		timeout -= sleep;
+		if (!timeout) {
+			printf("PF couldn't lock mailbox\n");
+			return 0;
+		}
+	}
+	lock = nic_reg_read(nic, mbx_addr);
+	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MBX_LOCK_SET(lock));
+	return 1;
+}
+
+void nic_release_mbx(struct nicpf *nic, int vf)
+{
+	uint64_t mbx_addr, lock;
+
+	mbx_addr = nic_get_mbx_addr(vf) + NIC_PF_VF_MBX_LOCK_OFFSET;
+	lock = nic_reg_read(nic, mbx_addr);
+	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MBX_LOCK_CLEAR(lock));
+}
+
+static int nic_send_msg_to_vf(struct nicpf *nic, int vf,
+			      struct nic_mbx *mbx, bool lock_needed)
+{
+	int i;
+	uint64_t *msg;
+	uint64_t mbx_addr;
+
+	if (lock_needed && (!nic_lock_mbox(nic, vf)))
+		return -1;
+
+	mbx->mbx_trigger_intr = 1;
+	msg = (uint64_t *)mbx;
+	mbx_addr = nic->reg_base + nic_get_mbx_addr(vf);
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++)
+		writeq(*(msg + i), (void *)(mbx_addr + (i * 8)));
+
+	if (lock_needed)
+		nic_release_mbx(nic, vf);
+	return 0;
+}
+
+static void nic_mbx_send_ready(struct nicpf *nic, int vf)
+{
+	struct nic_mbx mbx = { };
+
+	/* Respond with VNIC ID */
+	mbx.msg = NIC_PF_VF_MSG_READY;
+	mbx.data.nic_cfg.vf_id = vf;
+
+	if (nic->flags & NIC_TNS_ENABLED)
+		mbx.data.nic_cfg.tns_mode = NIC_TNS_MODE;
+	else
+		mbx.data.nic_cfg.tns_mode = NIC_TNS_BYPASS_MODE;
+
+/*
+	memcpy(&mbx.data.nic_cfg.mac_addr,
+	       &nic->mac[vf], 6);
+*/
+	mbx.data.nic_cfg.node_id = nic->node;
+
+	nic_send_msg_to_vf(nic, vf, &mbx, false);
+}
+
+static void nic_mbx_send_ack(struct nicpf *nic, int vf)
+{
+	struct nic_mbx mbx = { };
+
+	mbx.msg = NIC_PF_VF_MSG_ACK;
+	nic_send_msg_to_vf(nic, vf, &mbx, false);
+}
+
+static void nic_mbx_send_nack(struct nicpf *nic, int vf)
+{
+	struct nic_mbx mbx = { };
+
+	mbx.msg = NIC_PF_VF_MSG_NACK;
+	nic_send_msg_to_vf(nic, vf, &mbx, false);
+}
+
+/* Handle Mailbox messages from VF and ack the message. */
+void nic_handle_mbx_intr(struct nicpf *nic, int vf)
+{
+	struct nic_mbx mbx = { };
+	uint64_t *mbx_data;
+	uint64_t mbx_addr;
+	uint64_t reg_addr;
+	int lmac;
+	int i;
+	int ret = 0;
+
+	mbx_addr = nic_get_mbx_addr(vf);
+	mbx_data = (uint64_t *)&mbx;
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
+		*mbx_data = nic_reg_read(nic, mbx_addr);
+		mbx_data++;
+		mbx_addr += NIC_PF_VF_MAILBOX_SIZE;
+	}
+
+	mbx.msg &= NIC_PF_VF_MBX_MSG_MASK;
+
+	debug("%s: Mailbox msg %d from VF%d\n", __func__, mbx.msg, vf);
+
+	switch (mbx.msg) {
+	case NIC_PF_VF_MSG_READY:
+		nic_mbx_send_ready(nic, vf);
+		ret = 1;
+		break;
+	case NIC_PF_VF_MSG_QS_CFG:
+		reg_addr = NIC_PF_QSET_0_127_CFG |
+		    (mbx.data.qs.num << NIC_QS_ID_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.qs.cfg);
+		break;
+	case NIC_PF_VF_MSG_RQ_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_CFG |
+		    (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
+		    (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+		break;
+	case NIC_PF_VF_MSG_RQ_BP_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_BP_CFG |
+		    (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
+		    (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+		break;
+	case NIC_PF_VF_MSG_RQ_SW_SYNC:
+		ret = nic_rcv_queue_sw_sync(nic);
+		break;
+	case NIC_PF_VF_MSG_RQ_DROP_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG |
+		    (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
+		    (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+		break;
+	case NIC_PF_VF_MSG_SQ_CFG:
+		reg_addr = NIC_PF_QSET_0_127_SQ_0_7_CFG |
+		    (mbx.data.sq.qs_num << NIC_QS_ID_SHIFT) |
+		    (mbx.data.sq.sq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.sq.cfg);
+		nic_tx_channel_cfg(nic, mbx.data.qs.num, mbx.data.sq.sq_num);
+		break;
+	case NIC_PF_VF_MSG_SET_MAC:
+		lmac = mbx.data.mac.vf_id;
+		debug("LMAC: %d, map: %x\n", lmac, nic->vf_lmac_map[lmac]);
+		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lmac]);
+
+		break;
+	case NIC_PF_VF_MSG_SET_MAX_FRS:
+		ret = nic_update_hw_frs(nic, mbx.data.frs.max_frs,
+					mbx.data.frs.vf_id);
+		break;
+	case NIC_PF_VF_MSG_CPI_CFG:
+		nic_config_cpi(nic, &mbx.data.cpi_cfg);
+		break;
+	default:
+		printf("Invalid msg from VF%d, msg 0x%x\n", vf, mbx.msg);
+		break;
+	}
+
+	if (!ret)
+		nic_mbx_send_ack(nic, vf);
+	else if (mbx.msg != NIC_PF_VF_MSG_READY)
+		nic_mbx_send_nack(nic, vf);
+}
+
+static int nic_rcv_queue_sw_sync(struct nicpf *nic)
+{
+	int timeout = 20;
+
+	nic_reg_write(nic, NIC_PF_SW_SYNC_RX, 0x01);
+	while (timeout) {
+		if (nic_reg_read(nic, NIC_PF_SW_SYNC_RX_DONE) & 0x1)
+			break;
+		udelay(2000);
+		timeout--;
+	}
+	nic_reg_write(nic, NIC_PF_SW_SYNC_RX, 0x00);
+	if (!timeout) {
+		printf("Recevie queue software sync failed");
+		return 1;
+	}
+	return 0;
+}
+
+static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf)
+{
+	uint64_t *pkind = (uint64_t *)&nic->pkind;
+	if ((new_frs > NIC_HW_MAX_FRS) || (new_frs < NIC_HW_MIN_FRS)) {
+		printf("MTU from VF%d rejected, out of range %d..%d\n",
+		       vf, NIC_HW_MIN_FRS, NIC_HW_MAX_FRS);
+		return 1;
+	}
+	new_frs += ETH_HLEN;
+	if (new_frs <= nic->pkind.maxlen)
+		return 0;
+
+	nic->pkind.maxlen = new_frs;
+
+	nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG, *pkind);
+	return 0;
+}
+
+/* Set minimum transmit packet size */
+static void nic_set_tx_pkt_pad(struct nicpf *nic, int size)
+{
+	int lmac;
+	uint64_t lmac_cfg;
+
+	/* Max value that can be set is 60 */
+	if (size > 60)
+		size = 60;
+
+	for (lmac = 0; lmac < (MAX_BGX_PER_CN88XX * MAX_LMAC_PER_BGX); lmac++) {
+		lmac_cfg = nic_reg_read(nic, NIC_PF_LMAC_0_7_CFG | (lmac << 3));
+		lmac_cfg &= ~(0xF << 2);
+		lmac_cfg |= ((size / 4) << 2);
+		nic_reg_write(nic, NIC_PF_LMAC_0_7_CFG | (lmac << 3), lmac_cfg);
+	}
+}
+
+/* Function to check number of LMACs present and set VF to LMAC mapping.
+ * Mapping will be used while initializing channels.
+ */
+static void nic_set_lmac_vf_mapping(struct nicpf *nic)
+{
+	int bgx, bgx_count, next_bgx_lmac = 0;
+	int lmac, lmac_cnt = 0;
+	uint64_t lmac_credit;
+
+	nic->num_vf_en = 0;
+	if (nic->flags & NIC_TNS_ENABLED) {
+		nic->num_vf_en = DEFAULT_NUM_VF_ENABLED;
+		return;
+	}
+
+	bgx_get_count(nic->node, &bgx_count);
+	debug("bgx_count: %d\n", bgx_count);
+
+	for (bgx = 0; bgx < NIC_MAX_BGX; bgx++) {
+		if (!(bgx_count & (1 << bgx)))
+			continue;
+		nic->bgx_cnt++;
+		lmac_cnt = bgx_get_lmac_count(nic->node, bgx);
+		debug("lmac_cnt: %d\n", lmac_cnt);
+		for (lmac = 0; lmac < lmac_cnt; lmac++)
+			nic->vf_lmac_map[next_bgx_lmac++] =
+			    NIC_SET_VF_LMAC_MAP(bgx, lmac);
+		nic->num_vf_en += lmac_cnt;
+
+		/* Program LMAC credits */
+		lmac_credit = (1ull << 1);	/* chennel credit enable */
+		lmac_credit |= (0x1ff << 2);
+		lmac_credit |= (((((48 * 1024) / lmac_cnt) -
+				  NIC_HW_MAX_FRS) / 16) << 12);
+		nic_reg_write(nic,
+			      NIC_PF_LMAC_0_7_CREDIT + (lmac * 8), lmac_credit);
+	}
+}
+
+static void nic_init_hw(struct nicpf *nic)
+{
+	int i;
+	uint64_t reg;
+	uint64_t *pkind = (uint64_t *)&nic->pkind;
+
+	/* Reset NIC, incase if driver is repeatedly inserted and removed */
+	nic_reg_write(nic, NIC_PF_SOFT_RESET, 1);
+
+	/* Enable NIC HW block */
+	nic_reg_write(nic, NIC_PF_CFG, 0x3);
+
+	/* Enable backpressure */
+	nic_reg_write(nic, NIC_PF_BP_CFG, (1ULL << 6) | 0x03);
+	nic_reg_write(nic, NIC_PF_INTF_0_1_BP_CFG, (1ULL << 63) | 0x08);
+	nic_reg_write(nic, NIC_PF_INTF_0_1_BP_CFG + (1 << 8),
+		      (1ULL << 63) | 0x09);
+
+	for (i = 0; i < NIC_MAX_CHANS; i++)
+		nic_reg_write(nic, NIC_PF_CHAN_0_255_TX_CFG | (i << 3), 1);
+
+	if (nic->flags & NIC_TNS_ENABLED) {
+		reg = NIC_TNS_MODE << 7;
+		reg |= 0x06;
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, reg);
+		reg &= ~0xFull;
+		reg |= 0x07;
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG | (1 << 8), reg);
+	} else {
+		/* Disable TNS mode on both interfaces */
+		reg = NIC_TNS_BYPASS_MODE << 7;
+		reg |= 0x08;	/* Block identifier */
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, reg);
+		reg &= ~0xFull;
+		reg |= 0x09;
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG | (1 << 8), reg);
+	}
+
+	/* PKIND configuration */
+	nic->pkind.minlen = 0;
+	nic->pkind.maxlen = NIC_HW_MAX_FRS + ETH_HLEN;
+	nic->pkind.lenerr_en = 1;
+	nic->pkind.rx_hdr = 0;
+	nic->pkind.hdr_sl = 0;
+
+	for (i = 0; i < NIC_MAX_PKIND; i++)
+		nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG | (i << 3), *pkind);
+
+	nic_set_tx_pkt_pad(nic, NIC_HW_MIN_FRS);
+
+	/* Timer config */
+	nic_reg_write(nic, NIC_PF_INTR_TIMER_CFG, NICPF_CLK_PER_INT_TICK);
+}
+
+static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
+{
+	uint32_t vnic, bgx, lmac, chan;
+	uint32_t padd, cpi_count = 0;
+	uint64_t cpi_base, cpi, rssi_base, rssi;
+	uint8_t qset, rq_idx = 0;
+
+	vnic = cfg->vf_id;
+	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+
+	debug("%s: vnic: %d, bgx: %d, lmac: %d\n", __func__, vnic, bgx, lmac);
+
+	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_INF);
+	cpi_base = (lmac * NIC_MAX_CPI_PER_LMAC) + (bgx * NIC_CPI_PER_BGX);
+	rssi_base = (lmac * nic->rss_ind_tbl_size) + (bgx * NIC_RSSI_PER_BGX);
+
+	/* Rx channel configuration */
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_CFG | (chan << 3),
+		      (cfg->cpi_alg << 62) | (cpi_base << 48));
+
+	if (cfg->cpi_alg == CPI_ALG_NONE)
+		cpi_count = 1;
+	else if (cfg->cpi_alg == CPI_ALG_VLAN)	/* 3 bits of PCP */
+		cpi_count = 8;
+	else if (cfg->cpi_alg == CPI_ALG_VLAN16)	/* 3 bits PCP + DEI */
+		cpi_count = 16;
+	else if (cfg->cpi_alg == CPI_ALG_DIFF)	/* 6bits DSCP */
+		cpi_count = NIC_MAX_CPI_PER_LMAC;
+
+	/* RSS Qset, Qidx mapping */
+	qset = cfg->vf_id;
+	rssi = rssi_base;
+	for (; rssi < (rssi_base + cfg->rq_cnt); rssi++) {
+		nic_reg_write(nic, NIC_PF_RSSI_0_4097_RQ | (rssi << 3),
+			      (qset << 3) | rq_idx);
+	}
+
+	rssi = 0;
+	cpi = cpi_base;
+	for (; cpi < (cpi_base + cpi_count); cpi++) {
+		/* Determine port to channel adder */
+		if (cfg->cpi_alg != CPI_ALG_DIFF)
+			padd = cpi % cpi_count;
+		else
+			padd = cpi % 8;	/* 3 bits CS out of 6bits DSCP */
+
+		/* Leave RSS_SIZE as '0' to disable RSS */
+		nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi << 3),
+			      (vnic << 24) | (padd << 16) | (rssi_base + rssi));
+
+		if ((rssi + 1) >= cfg->rq_cnt)
+			continue;
+
+		if (cfg->cpi_alg == CPI_ALG_VLAN)
+			rssi++;
+		else if (cfg->cpi_alg == CPI_ALG_VLAN16)
+			rssi = ((cpi - cpi_base) & 0xe) >> 1;
+		else if (cfg->cpi_alg == CPI_ALG_DIFF)
+			rssi = ((cpi - cpi_base) & 0x38) >> 3;
+	}
+	nic->cpi_base[cfg->vf_id] = cpi_base;
+}
+
+/* Transmit channel configuration (TL4 -> TL3 -> Chan)
+ * VNIC0-SQ0 -> TL4(0)  -> TL4A(0) -> TL3[0] -> BGX0/LMAC0/Chan0
+ * VNIC1-SQ0 -> TL4(8)  -> TL4A(2) -> TL3[2] -> BGX0/LMAC1/Chan0
+ * VNIC2-SQ0 -> TL4(16) -> TL4A(4) -> TL3[4] -> BGX0/LMAC2/Chan0
+ * VNIC3-SQ0 -> TL4(32) -> TL4A(6) -> TL3[6] -> BGX0/LMAC3/Chan0
+ * VNIC4-SQ0 -> TL4(512)  -> TL4A(128) -> TL3[128] -> BGX1/LMAC0/Chan0
+ * VNIC5-SQ0 -> TL4(520)  -> TL4A(130) -> TL3[130] -> BGX1/LMAC1/Chan0
+ * VNIC6-SQ0 -> TL4(528)  -> TL4A(132) -> TL3[132] -> BGX1/LMAC2/Chan0
+ * VNIC7-SQ0 -> TL4(536)  -> TL4A(134) -> TL3[134] -> BGX1/LMAC3/Chan0
+ */
+static void nic_tx_channel_cfg(struct nicpf *nic, int vnic, int sq_idx)
+{
+	uint32_t bgx, lmac;
+	uint32_t tl2, tl3, tl4;
+
+	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+
+	debug("%s: bgx: %u, lmac: %u\n", __func__, bgx, lmac);
+
+	tl4 = (lmac * NIC_TL4_PER_LMAC) + (bgx * NIC_TL4_PER_BGX);
+	tl4 += sq_idx;
+	tl3 = tl4 / (NIC_MAX_TL4 / NIC_MAX_TL3);
+	nic_reg_write(nic, NIC_PF_QSET_0_127_SQ_0_7_CFG2 |
+		      (vnic << NIC_QS_ID_SHIFT) |
+		      (sq_idx << NIC_Q_NUM_SHIFT), tl4);
+	nic_reg_write(nic, NIC_PF_TL4_0_1023_CFG | (tl4 << 3),
+		      (vnic << 27) | (sq_idx << 24) | (NIC_HW_MAX_FRS / 4));
+
+	nic_reg_write(nic, NIC_PF_TL3_0_255_CFG | (tl3 << 3),
+		      NIC_HW_MAX_FRS / 4);
+	nic_reg_write(nic, NIC_PF_TL3_0_255_CHAN | (tl3 << 3), lmac << 4);
+
+	tl2 = tl3 >> 2;
+	nic_reg_write(nic, NIC_PF_TL3A_0_63_CFG | (tl2 << 3), tl2);
+	nic_reg_write(nic, NIC_PF_TL2_0_63_CFG | (tl2 << 3),
+		      NIC_HW_MAX_FRS / 4);
+	nic_reg_write(nic, NIC_PF_TL2_0_63_PRI | (tl2 << 3), 0x00);
+}
+
+struct nicpf *nic_initialize(unsigned int node)
+{
+	struct nicpf *nic;
+
+	nic = malloc(sizeof(struct nicpf));
+	if (!nic)
+		return nic;
+
+	/* MAP PF's configuration registers */
+	nic->reg_base = CSR_PA(NIC_PF_BAR0, node);
+	if (!nic->reg_base) {
+		printf("Cannot map config register space, aborting\n");
+		goto exit;
+	}
+
+	nic->node = node;
+
+	/* By default set NIC in TNS bypass mode */
+	nic->flags &= ~NIC_TNS_ENABLED;
+
+	nic_set_lmac_vf_mapping(nic);
+
+	/* Initialize hardware */
+	nic_init_hw(nic);
+
+	/* Set RSS TBL size for each VF */
+	nic->rss_ind_tbl_size = NIC_MAX_RSS_IDR_TBL_SIZE;
+
+	nic->rss_ind_tbl_size = rounddown_pow_of_two(nic->rss_ind_tbl_size);
+
+	return nic;
+exit:
+	free(nic);
+	return NULL;
+}
diff --git a/drivers/net/cavium/nic_reg.h b/drivers/net/cavium/nic_reg.h
new file mode 100644
index 0000000000..8324a14c71
--- /dev/null
+++ b/drivers/net/cavium/nic_reg.h
@@ -0,0 +1,247 @@
+/*
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#ifndef NIC_REG_H
+#define NIC_REG_H
+
+#define   NIC_PF_REG_COUNT			29573
+#define   NIC_VF_REG_COUNT			249
+
+/* Physical function register offsets */
+#define   NIC_PF_CFG				(0x0000)
+#define   NIC_PF_STATUS				(0x0010)
+
+#define   NIC_PF_INTR_TIMER_CFG		(0x0030)
+#define   NIC_PF_BIST_STATUS		(0x0040)
+#define   NIC_PF_SOFT_RESET			(0x0050)
+
+#define   NIC_PF_TCP_TIMER			(0x0060)
+#define   NIC_PF_BP_CFG				(0x0080)
+#define   NIC_PF_RRM_CFG			(0x0088)
+#define   NIC_PF_CQM_CF				(0x00A0)
+#define   NIC_PF_CNM_CF				(0x00A8)
+#define   NIC_PF_CNM_STATUS			(0x00B0)
+#define   NIC_PF_CQ_AVG_CFG			(0x00C0)
+#define   NIC_PF_RRM_AVG_CFG			(0x00C8)
+
+#define   NIC_PF_INTF_0_1_SEND_CFG		(0x0200)
+#define   NIC_PF_INTF_0_1_BP_CFG		(0x0208)
+#define   NIC_PF_INTF_0_1_BP_DIS_0_1	(0x0210)
+#define   NIC_PF_INTF_0_1_BP_SW_0_1		(0x0220)
+#define   NIC_PF_RBDR_BP_STATE_0_3		(0x0240)
+
+#define   NIC_PF_MAILBOX_INT			(0x0410)
+#define   NIC_PF_MAILBOX_INT_W1S		(0x0430)
+#define   NIC_PF_MAILBOX_ENA_W1C		(0x0450)
+#define   NIC_PF_MAILBOX_ENA_W1S		(0x0470)
+
+#define   NIC_PF_RX_ETYPE_0_7			(0x0500)
+#define   NIC_PF_PKIND_0_15_CFG			(0x0600)
+
+#define   NIC_PF_ECC0_FLIP0			(0x1000)
+#define   NIC_PF_ECC1_FLIP0			(0x1008)
+#define   NIC_PF_ECC2_FLIP0			(0x1010)
+#define   NIC_PF_ECC3_FLIP0			(0x1018)
+#define   NIC_PF_ECC0_FLIP1			(0x1080)
+#define   NIC_PF_ECC1_FLIP1			(0x1088)
+#define   NIC_PF_ECC2_FLIP1			(0x1090)
+#define   NIC_PF_ECC3_FLIP1			(0x1098)
+#define   NIC_PF_ECC0_CDIS			(0x1100)
+#define   NIC_PF_ECC1_CDIS			(0x1108)
+#define   NIC_PF_ECC2_CDIS			(0x1110)
+#define   NIC_PF_ECC3_CDIS			(0x1118)
+#define   NIC_PF_BIST0_STATUS			(0x1280)
+#define   NIC_PF_BIST1_STATUS			(0x1288)
+#define   NIC_PF_BIST2_STATUS			(0x1290)
+#define   NIC_PF_BIST3_STATUS			(0x1298)
+
+#define   NIC_PF_ECC0_SBE_INT			(0x2000)
+#define   NIC_PF_ECC0_SBE_INT_W1S		(0x2008)
+#define   NIC_PF_ECC0_SBE_ENA_W1C		(0x2010)
+#define   NIC_PF_ECC0_SBE_ENA_W1S		(0x2018)
+#define   NIC_PF_ECC0_DBE_INT			(0x2100)
+#define   NIC_PF_ECC0_DBE_INT_W1S		(0x2108)
+#define   NIC_PF_ECC0_DBE_ENA_W1C		(0x2110)
+#define   NIC_PF_ECC0_DBE_ENA_W1S		(0x2118)
+
+#define   NIC_PF_ECC1_SBE_INT			(0x2200)
+#define   NIC_PF_ECC1_SBE_INT_W1S		(0x2208)
+#define   NIC_PF_ECC1_SBE_ENA_W1C		(0x2210)
+#define   NIC_PF_ECC1_SBE_ENA_W1S		(0x2218)
+#define   NIC_PF_ECC1_DBE_INT			(0x2300)
+#define   NIC_PF_ECC1_DBE_INT_W1S		(0x2308)
+#define   NIC_PF_ECC1_DBE_ENA_W1C		(0x2310)
+#define   NIC_PF_ECC1_DBE_ENA_W1S		(0x2318)
+
+#define   NIC_PF_ECC2_SBE_INT			(0x2400)
+#define   NIC_PF_ECC2_SBE_INT_W1S		(0x2408)
+#define   NIC_PF_ECC2_SBE_ENA_W1C		(0x2410)
+#define   NIC_PF_ECC2_SBE_ENA_W1S		(0x2418)
+#define   NIC_PF_ECC2_DBE_INT			(0x2500)
+#define   NIC_PF_ECC2_DBE_INT_W1S		(0x2508)
+#define   NIC_PF_ECC2_DBE_ENA_W1C		(0x2510)
+#define   NIC_PF_ECC2_DBE_ENA_W1S		(0x2518)
+
+#define   NIC_PF_ECC3_SBE_INT			(0x2600)
+#define   NIC_PF_ECC3_SBE_INT_W1S		(0x2608)
+#define   NIC_PF_ECC3_SBE_ENA_W1C		(0x2610)
+#define   NIC_PF_ECC3_SBE_ENA_W1S		(0x2618)
+#define   NIC_PF_ECC3_DBE_INT			(0x2700)
+#define   NIC_PF_ECC3_DBE_INT_W1S		(0x2708)
+#define   NIC_PF_ECC3_DBE_ENA_W1C		(0x2710)
+#define   NIC_PF_ECC3_DBE_ENA_W1S		(0x2718)
+
+#define   NIC_PF_CPI_0_2047_CFG			(0x200000)
+#define   NIC_PF_RSSI_0_4097_RQ			(0x220000)
+#define   NIC_PF_LMAC_0_7_CFG			(0x240000)
+#define   NIC_PF_LMAC_0_7_SW_XOFF		(0x242000)
+#define   NIC_PF_LMAC_0_7_CREDIT		(0x244000)
+#define   NIC_PF_CHAN_0_255_TX_CFG		(0x400000)
+#define   NIC_PF_CHAN_0_255_RX_CFG		(0x420000)
+#define   NIC_PF_CHAN_0_255_SW_XOFF		(0x440000)
+#define   NIC_PF_CHAN_0_255_CREDIT		(0x460000)
+#define   NIC_PF_CHAN_0_255_RX_BP_CFG	(0x480000)
+
+#define   NIC_PF_SW_SYNC_RX				(0x490000)
+
+#define   NIC_PF_SW_SYNC_RX_DONE		(0x490008)
+#define   NIC_PF_TL2_0_63_CFG			(0x500000)
+#define   NIC_PF_TL2_0_63_PRI			(0x520000)
+#define   NIC_PF_TL2_0_63_SH_STATUS		(0x580000)
+#define   NIC_PF_TL3A_0_63_CFG			(0x5F0000)
+#define   NIC_PF_TL3_0_255_CFG			(0x600000)
+#define   NIC_PF_TL3_0_255_CHAN			(0x620000)
+#define   NIC_PF_TL3_0_255_PIR			(0x640000)
+#define   NIC_PF_TL3_0_255_SW_XOFF		(0x660000)
+#define   NIC_PF_TL3_0_255_CNM_RATE		(0x680000)
+#define   NIC_PF_TL3_0_255_SH_STATUS	(0x6A0000)
+#define   NIC_PF_TL4A_0_255_CFG			(0x6F0000)
+#define   NIC_PF_TL4_0_1023_CFG			(0x800000)
+#define   NIC_PF_TL4_0_1023_SW_XOFF		(0x820000)
+#define   NIC_PF_TL4_0_1023_SH_STATUS		(0x840000)
+#define   NIC_PF_TL4A_0_1023_CNM_RATE		(0x880000)
+#define   NIC_PF_TL4A_0_1023_CNM_STATUS		(0x8A0000)
+#define   NIC_PF_VF_0_127_MAILBOX_0_7		(0x20002000)
+#define   NIC_PF_VNIC_0_127_TX_STAT_0_4		(0x20004000)
+#define   NIC_PF_VNIC_0_127_RX_STAT_0_13	(0x20004100)
+#define   NIC_PF_QSET_0_127_LOCK_0_15		(0x20006000)
+#define   NIC_PF_QSET_0_127_CFG				(0x20010000)
+#define   NIC_PF_QSET_0_127_RQ_0_7_CFG		(0x20010400)
+#define   NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG	(0x20010420)
+#define   NIC_PF_QSET_0_127_RQ_0_7_BP_CFG	(0x20010500)
+#define   NIC_PF_QSET_0_127_RQ_0_7_STAT_0_1	(0x20010600)
+#define   NIC_PF_QSET_0_127_SQ_0_7_CFG		(0x20010C00)
+#define   NIC_PF_QSET_0_127_SQ_0_7_CFG2		(0x20010C08)
+#define   NIC_PF_QSET_0_127_SQ_0_7_STAT_0_1	(0x20010D00)
+
+#define   NIC_PF_MSIX_VEC_0_18_ADDR		(0x000000)
+#define   NIC_PF_MSIX_VEC_0_CTL			(0x000008)
+#define   NIC_PF_MSIX_PBA_0				(0x0F0000)
+
+/* Virtual function register offsets */
+#define   NIC_VNIC_CFG				(0x000020)
+#define   NIC_VF_PF_MAILBOX_0_7		(0x000100)
+#define   NIC_VF_INT				(0x000200)
+#define   NIC_VF_INT_W1S			(0x000220)
+#define   NIC_VF_ENA_W1C			(0x000240)
+#define   NIC_VF_ENA_W1S			(0x000260)
+
+#define   NIC_VNIC_RSS_CFG			(0x0020E0)
+#define   NIC_VNIC_RSS_KEY_0_4		(0x002200)
+#define   NIC_VNIC_TX_STAT_0_4		(0x004000)
+#define   NIC_VNIC_RX_STAT_0_13		(0x004100)
+#define   NIC_QSET_RQ_GEN_CFG		(0x010010)
+
+#define   NIC_QSET_CQ_0_7_CFG		(0x010400)
+#define   NIC_QSET_CQ_0_7_CFG2		(0x010408)
+#define   NIC_QSET_CQ_0_7_THRESH	(0x010410)
+#define   NIC_QSET_CQ_0_7_BASE		(0x010420)
+#define   NIC_QSET_CQ_0_7_HEAD		(0x010428)
+#define   NIC_QSET_CQ_0_7_TAIL		(0x010430)
+#define   NIC_QSET_CQ_0_7_DOOR		(0x010438)
+#define   NIC_QSET_CQ_0_7_STATUS	(0x010440)
+#define   NIC_QSET_CQ_0_7_STATUS2	(0x010448)
+#define   NIC_QSET_CQ_0_7_DEBUG		(0x010450)
+
+#define   NIC_QSET_RQ_0_7_CFG		(0x010600)
+#define   NIC_QSET_RQ_0_7_STAT_0_1	(0x010700)
+
+#define   NIC_QSET_SQ_0_7_CFG			(0x010800)
+#define   NIC_QSET_SQ_0_7_THRESH		(0x010810)
+#define   NIC_QSET_SQ_0_7_BASE			(0x010820)
+#define   NIC_QSET_SQ_0_7_HEAD			(0x010828)
+#define   NIC_QSET_SQ_0_7_TAIL			(0x010830)
+#define   NIC_QSET_SQ_0_7_DOOR			(0x010838)
+#define   NIC_QSET_SQ_0_7_STATUS		(0x010840)
+#define   NIC_QSET_SQ_0_7_DEBUG			(0x010848)
+#define   NIC_QSET_SQ_0_7_CNM_CHG		(0x010860)
+#define   NIC_QSET_SQ_0_7_STAT_0_1		(0x010900)
+
+#define   NIC_QSET_RBDR_0_1_CFG			(0x010C00)
+#define   NIC_QSET_RBDR_0_1_THRESH		(0x010C10)
+#define   NIC_QSET_RBDR_0_1_BASE		(0x010C20)
+#define   NIC_QSET_RBDR_0_1_HEAD		(0x010C28)
+#define   NIC_QSET_RBDR_0_1_TAIL		(0x010C30)
+#define   NIC_QSET_RBDR_0_1_DOOR		(0x010C38)
+#define   NIC_QSET_RBDR_0_1_STATUS0		(0x010C40)
+#define   NIC_QSET_RBDR_0_1_STATUS1		(0x010C48)
+#define   NIC_QSET_RBDR_0_1_PREFETCH_STATUS	(0x010C50)
+
+#define   NIC_VF_MSIX_VECTOR_0_19_ADDR		(0x000000)
+#define   NIC_VF_MSIX_VECTOR_0_19_CTL		(0x000008)
+#define   NIC_VF_MSIX_PBA			(0x0F0000)
+
+/* Offsets within registers */
+#define   NIC_MSIX_VEC_SHIFT			4
+#define   NIC_Q_NUM_SHIFT			18
+#define   NIC_QS_ID_SHIFT			21
+#define   NIC_VF_NUM_SHIFT			21
+
+/* Port kind configuration register */
+struct pkind_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_42_63:22;
+	uint64_t hdr_sl:5;	/* Header skip length */
+	uint64_t rx_hdr:3;	/* TNS Receive header present */
+	uint64_t lenerr_en:1;	/* L2 length error check enable */
+	uint64_t reserved_32_32:1;
+	uint64_t maxlen:16;	/* Max frame size */
+	uint64_t minlen:16;	/* Min frame size */
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t minlen:16;
+	uint64_t maxlen:16;
+	uint64_t reserved_32_32:1;
+	uint64_t lenerr_en:1;
+	uint64_t rx_hdr:3;
+	uint64_t hdr_sl:5;
+	uint64_t reserved_42_63:22;
+#endif
+};
+
+static inline uint64_t BGXX_PF_BAR0(unsigned long param1)
+{
+	assert(param1 <= 1);
+	return 0x87E0E0000000 + (param1 << 24);
+}
+
+#define BGXX_PF_BAR0_SIZE 0x400000
+
+#define NIC_PF_BAR0 0x843000000000
+#define NIC_PF_BAR0_SIZE 0x40000000
+
+static inline uint64_t NIC_VFX_BAR0(unsigned long param1)
+{
+	assert(param1 <= 127);
+
+	return 0x8430A0000000 + (param1 << 21);
+}
+
+#define NIC_VFX_BAR0_SIZE 0x200000
+
+#endif /* NIC_REG_H */
diff --git a/drivers/net/cavium/nicvf_main.c b/drivers/net/cavium/nicvf_main.c
new file mode 100644
index 0000000000..24d10ff495
--- /dev/null
+++ b/drivers/net/cavium/nicvf_main.c
@@ -0,0 +1,556 @@
+/*
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#include <config.h>
+#include <common.h>
+#include <net.h>
+#include <netdev.h>
+#include <malloc.h>
+#include <asm/io.h>
+
+#include <cavium/thunderx_vnic.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "nicvf_queues.h"
+#include "thunder_bgx.h"
+
+#define ETH_ALEN 6
+
+/* Register read/write APIs */
+void nicvf_reg_write(struct nicvf *nicvf, uint64_t offset, uint64_t val)
+{
+	uint64_t addr = nicvf->reg_base + offset;
+
+	writeq(val, (void *)addr);
+}
+
+uint64_t nicvf_reg_read(struct nicvf *nicvf, uint64_t offset)
+{
+	uint64_t addr = nicvf->reg_base + offset;
+
+	return readq((void *)addr);
+}
+
+void nicvf_queue_reg_write(struct nicvf *nicvf, uint64_t offset,
+			   uint64_t qidx, uint64_t val)
+{
+	uint64_t addr = nicvf->reg_base + offset;
+
+	writeq(val, (void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
+}
+
+uint64_t nicvf_queue_reg_read(struct nicvf *nicvf, uint64_t offset, uint64_t qidx)
+{
+	uint64_t addr = nicvf->reg_base + offset;
+
+	return readq((void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
+}
+
+/* VF -> PF mailbox communication */
+static bool pf_ready_to_rcv_msg;
+static bool pf_acked;
+static bool pf_nacked;
+
+int nicvf_lock_mbox(struct nicvf *nicvf)
+{
+	int timeout = NIC_PF_VF_MBX_TIMEOUT;
+	int sleep = 10;
+	uint64_t lock, mbx_addr;
+
+	mbx_addr = NIC_VF_PF_MAILBOX_0_7 + NIC_PF_VF_MBX_LOCK_OFFSET;
+	lock = NIC_PF_VF_MBX_LOCK_VAL(nicvf_reg_read(nicvf, mbx_addr));
+	while (lock) {
+		mdelay(sleep);
+		lock = NIC_PF_VF_MBX_LOCK_VAL(nicvf_reg_read(nicvf, mbx_addr));
+		timeout -= sleep;
+		if (!timeout) {
+			printf("VF%d Couldn't lock mailbox\n", nicvf->vf_id);
+			return 0;
+		}
+	}
+	nicvf_reg_write(nicvf, mbx_addr, NIC_PF_VF_MBX_LOCK_SET(lock));
+	return 1;
+}
+
+void nicvf_release_mbx(struct nicvf *nicvf)
+{
+	uint64_t mbx_addr, lock;
+
+	mbx_addr = NIC_VF_PF_MAILBOX_0_7 + NIC_PF_VF_MBX_LOCK_OFFSET;
+	lock = nicvf_reg_read(nicvf, mbx_addr);
+	nicvf_reg_write(nicvf, mbx_addr, NIC_PF_VF_MBX_LOCK_CLEAR(lock));
+}
+
+static void  nicvf_handle_mbx_intr(struct nicvf *nicvf);
+
+int nicvf_send_msg_to_pf(struct nicvf *nicvf, struct nic_mbx *mbx)
+{
+	int i, timeout = NIC_PF_VF_MBX_TIMEOUT;
+	int sleep = 10;
+	uint64_t *msg;
+	uint64_t mbx_addr;
+
+	if (!nicvf_lock_mbox(nicvf))
+		return -1;
+
+	pf_acked = false;
+	pf_nacked = false;
+	mbx->mbx_trigger_intr = 1;
+	msg = (uint64_t *)mbx;
+
+	mbx_addr = nicvf->reg_base + NIC_VF_PF_MAILBOX_0_7;
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++)
+		writeq(*(msg + i), (void *)(mbx_addr + (i * 8)));
+
+	nicvf_release_mbx(nicvf);
+
+	nic_handle_mbx_intr(nicvf->nicpf, nicvf->vf_id);
+
+	/* Wait for previous message to be acked, timeout 2sec */
+	while (!pf_acked) {
+		if (pf_nacked)
+			return -1;
+		mdelay(sleep);
+		nicvf_handle_mbx_intr(nicvf);
+
+		if (pf_acked)
+			break;
+		timeout -= sleep;
+		if (!timeout) {
+			printf("PF didn't ack to mbox msg %d from VF%d\n",
+			       (mbx->msg & 0xFF), nicvf->vf_id);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+/* Checks if VF is able to comminicate with PF
+* and also gets the VNIC number this VF is associated to.
+*/
+static int nicvf_check_pf_ready(struct nicvf *nicvf)
+{
+	int timeout = 5000, sleep = 20;
+	uint64_t mbx_addr = NIC_VF_PF_MAILBOX_0_7;
+
+	pf_ready_to_rcv_msg = false;
+
+	nicvf_reg_write(nicvf, mbx_addr, NIC_PF_VF_MSG_READY);
+
+	mbx_addr += (NIC_PF_VF_MAILBOX_SIZE - 1) * 8;
+	nicvf_reg_write(nicvf, mbx_addr, 1ULL);
+
+	nic_handle_mbx_intr(nicvf->nicpf, nicvf->vf_id);
+
+	while (!pf_ready_to_rcv_msg) {
+		mdelay(sleep);
+		nicvf_handle_mbx_intr(nicvf);
+
+		if (pf_ready_to_rcv_msg)
+			break;
+		timeout -= sleep;
+		if (!timeout) {
+			printf("PF didn't respond to READY msg\n");
+			return 0;
+		}
+	}
+	return 1;
+}
+
+static void nicvf_handle_mbx_intr(struct nicvf *nicvf)
+{
+	struct nic_mbx mbx = { };
+	uint64_t *mbx_data;
+	uint64_t mbx_addr;
+	int i;
+
+	mbx_addr = NIC_VF_PF_MAILBOX_0_7;
+	mbx_data = (uint64_t *)&mbx;
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
+		*mbx_data = nicvf_reg_read(nicvf, mbx_addr);
+		asm volatile("dsb sy");
+
+		mbx_data++;
+		mbx_addr += NIC_PF_VF_MAILBOX_SIZE;
+	}
+
+	debug("Mbox message from PF, msg 0x%x\n", mbx.msg);
+
+	switch (mbx.msg & NIC_PF_VF_MBX_MSG_MASK) {
+	case NIC_PF_VF_MSG_READY:
+		pf_ready_to_rcv_msg = true;
+		nicvf->vf_id = mbx.data.nic_cfg.vf_id & 0x7F;
+		nicvf->tns_mode = mbx.data.nic_cfg.tns_mode & 0x7F;
+		nicvf->node = mbx.data.nic_cfg.node_id;
+
+		debug("MAC: %pM\n", nicvf->netdev->enetaddr);
+		break;
+	case NIC_PF_VF_MSG_ACK:
+		pf_acked = true;
+		break;
+	case NIC_PF_VF_MSG_NACK:
+		pf_nacked = true;
+		break;
+	default:
+		printf("Invalid message from PF, msg 0x%x\n", mbx.msg);
+		break;
+	}
+	nicvf_clear_intr(nicvf, NICVF_INTR_MBOX, 0);
+}
+
+static int nicvf_hw_set_mac_addr(struct nicvf *nicvf, struct eth_device *netdev)
+{
+	struct nic_mbx mbx = { };
+	int i;
+
+	mbx.msg = NIC_PF_VF_MSG_SET_MAC;
+	mbx.data.mac.vf_id = nicvf->vf_id;
+	for (i = 0; i < ETH_ALEN; i++)
+		mbx.data.mac.addr =
+		    (mbx.data.mac.addr << 8) | netdev->enetaddr[i];
+
+	return nicvf_send_msg_to_pf(nicvf, &mbx);
+}
+
+void nicvf_config_cpi(struct nicvf *nicvf)
+{
+	struct nic_mbx mbx = { };
+
+	mbx.msg = NIC_PF_VF_MSG_CPI_CFG;
+	mbx.data.cpi_cfg.vf_id = nicvf->vf_id;
+	mbx.data.cpi_cfg.cpi_alg = nicvf->cpi_alg;
+	mbx.data.cpi_cfg.rq_cnt = nicvf->qs->rq_cnt;
+
+	nicvf_send_msg_to_pf(nicvf, &mbx);
+}
+
+static int nicvf_init_resources(struct nicvf *nicvf)
+{
+	int err;
+
+	nicvf->num_qs = 1;
+
+	/* Enable Qset */
+	nicvf_qset_config(nicvf, true);
+
+	/* Initialize queues and HW for data transfer */
+	err = nicvf_config_data_transfer(nicvf, true);
+	if (err) {
+		printf("Failed to alloc/config VF's QSet resources\n");
+		return err;
+	}
+	return 0;
+}
+
+void nicvf_free_pkt(struct nicvf *nicvf, void *pkt)
+{
+	free(pkt);
+}
+
+static void nicvf_snd_pkt_handler(struct nicvf *nicvf,
+				  struct cmp_queue *cq,
+				  void *cq_desc, int cqe_type)
+{
+	struct cqe_send_t *cqe_tx;
+	struct snd_queue *sq;
+	struct sq_hdr_subdesc *hdr;
+
+	cqe_tx = (struct cqe_send_t *)cq_desc;
+	sq = &nicvf->qs->sq[cqe_tx->sq_idx];
+
+	hdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, cqe_tx->sqe_ptr);
+	if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER)
+		return;
+
+	debug("%s Qset #%d SQ #%d SQ ptr #%d subdesc count %d\n",
+	      __func__, cqe_tx->sq_qs, cqe_tx->sq_idx,
+	      cqe_tx->sqe_ptr, hdr->subdesc_cnt);
+
+	nicvf_check_cqe_tx_errs(nicvf, cq, cq_desc);
+	nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
+}
+
+static int nicvf_rcv_pkt_handler(struct nicvf *nicvf,
+				 struct cmp_queue *cq, void *cq_desc,
+				 void **ppkt, int cqe_type)
+{
+	void *pkt;
+
+	size_t pkt_len;
+	struct cqe_rx_t *cqe_rx = (struct cqe_rx_t *)cq_desc;
+	int err = 0;
+
+	/* Check for errors */
+	err = nicvf_check_cqe_rx_errs(nicvf, cq, cq_desc);
+	if (err && !cqe_rx->rb_cnt)
+		return -1;
+
+	pkt = nicvf_get_rcv_pkt(nicvf, cq_desc, &pkt_len);
+	if (!pkt) {
+		debug("Packet not received\n");
+		return -1;
+	}
+
+	if (pkt)
+		*ppkt = pkt;
+
+	return pkt_len;
+}
+
+int nicvf_cq_handler(struct nicvf *nicvf, void **ppkt, int *pkt_len)
+{
+	int cq_qnum = 0;
+	int processed_sq_cqe = 0;
+	int processed_rq_cqe = 0;
+	int processed_cqe = 0;
+
+	unsigned long cqe_count, cqe_head;
+	struct queue_set *qs = nicvf->qs;
+	struct cmp_queue *cq = &qs->cq[cq_qnum];
+	struct cqe_rx_t *cq_desc;
+
+	/* Get num of valid CQ entries expect next one to be SQ completion */
+	cqe_count = nicvf_queue_reg_read(nicvf, NIC_QSET_CQ_0_7_STATUS, cq_qnum);
+	cqe_count &= 0xFFFF;
+	if (!cqe_count)
+		return 0;
+
+	/* Get head of the valid CQ entries */
+	cqe_head =
+	    nicvf_queue_reg_read(nicvf, NIC_QSET_CQ_0_7_HEAD, cq_qnum) >> 9;
+	cqe_head &= 0xFFFF;
+
+	if (cqe_count) {
+		/* Get the CQ descriptor */
+		cq_desc = (struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head);
+
+		switch (cq_desc->cqe_type) {
+		case CQE_TYPE_RX:
+			debug("%s: Got Rx CQE\n", nicvf->netdev->name);
+			*pkt_len =
+			    nicvf_rcv_pkt_handler(nicvf, cq, cq_desc, ppkt,
+						  CQE_TYPE_RX);
+			processed_rq_cqe++;
+			break;
+		case CQE_TYPE_SEND:
+			debug("%s: Got Tx CQE\n", nicvf->netdev->name);
+			nicvf_snd_pkt_handler(nicvf, cq, cq_desc, CQE_TYPE_SEND);
+			processed_sq_cqe++;
+			break;
+		default:
+			debug("%s: Got CQ type %u\n", nicvf->netdev->name,
+			      cq_desc->cqe_type);
+			break;
+		}
+		processed_cqe++;
+		cqe_head++;
+		cqe_head &= (cq->dmem.q_len - 1);
+	}
+
+	/* Dequeue CQE */
+	nicvf_queue_reg_write(nicvf, NIC_QSET_CQ_0_7_DOOR,
+			      cq_qnum, processed_cqe);
+
+	asm volatile ("dsb sy");
+
+	return processed_sq_cqe;
+}
+
+/* Qset error interrupt handler
+ *
+ * As of now only CQ errors are handled
+ */
+void nicvf_handle_qs_err(struct nicvf *nicvf)
+{
+	struct queue_set *qs = nicvf->qs;
+	int qidx;
+	uint64_t status;
+
+	/* Check if it is CQ err */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		status = nicvf_queue_reg_read(nicvf, NIC_QSET_CQ_0_7_STATUS,
+					      qidx);
+		if (!(status & CQ_ERR_MASK))
+			continue;
+		/* Process already queued CQEs and reconfig CQ */
+		nicvf_sq_disable(nicvf, qidx);
+		nicvf_cmp_queue_config(nicvf, qs, qidx, true);
+		nicvf_sq_free_used_descs(nicvf->netdev, &qs->sq[qidx], qidx);
+		nicvf_sq_enable(nicvf, &qs->sq[qidx], qidx);
+
+		nicvf_enable_intr(nicvf, NICVF_INTR_CQ, qidx);
+	}
+}
+
+static int nicvf_xmit(struct eth_device *netdev, void *pkt, int pkt_len)
+{
+	struct nicvf *nicvf = netdev->priv;
+	int ret = 0;
+	int rcv_len = 0;
+	unsigned int timeout = 5000;
+	void *rpkt = NULL;
+
+	if (!nicvf_sq_append_pkt(nicvf, pkt, pkt_len)) {
+		printf("VF%d: TX ring full\n", nicvf->vf_id);
+		return -1;
+	}
+
+	/* check and update CQ for pkt sent */
+	while (!ret && timeout--) {
+		ret = nicvf_cq_handler(nicvf, &rpkt, &rcv_len);
+		if (!ret)
+			debug("%s: %d, Not sent\n", __func__, __LINE__);
+		udelay(1);
+	}
+
+	return 0;
+}
+
+static int nicvf_recv(struct eth_device *netdev)
+{
+	struct nicvf *nicvf = netdev->priv;
+	void *pkt;
+	int pkt_len = 0;
+#ifdef DEBUG
+	u8 *dpkt;
+	int i, j;
+#endif
+
+	nicvf_cq_handler(nicvf, &pkt, &pkt_len);
+
+	if (pkt_len) {
+#ifdef DEBUG
+		dpkt = pkt;
+		printf("RX packet contents:\n");
+		for (i = 0; i < 8; i++) {
+			puts("\t");
+			for (j = 0; j < 10; j++)
+				printf("%02x ", dpkt[i * 10 + j]);
+			puts("\n");
+		}
+#endif
+		net_process_received_packet(pkt, pkt_len);
+		nicvf_refill_rbdr(nicvf);
+	}
+
+	return pkt_len;
+}
+
+void nicvf_stop(struct eth_device *netdev)
+{
+	struct nicvf *nicvf = netdev->priv;
+
+	/* Free resources */
+	nicvf_config_data_transfer(nicvf, false);
+
+	/* Disable HW Qset */
+	nicvf_qset_config(nicvf, false);
+}
+
+int nicvf_open(struct eth_device *netdev, bd_t *bis)
+{
+	int err;
+	struct nicvf *nicvf = netdev->priv;
+	struct nicpf *nicpf = nicvf->nicpf;
+	int vnic = nicvf->vf_id;
+
+	int bgx, lmac;
+
+	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nicpf->vf_lmac_map[vnic]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nicpf->vf_lmac_map[vnic]);
+
+	bgx_lmac_enable(bgx, lmac);
+	nicvf_hw_set_mac_addr(nicvf, netdev);
+
+	/* Configure CPI alorithm */
+	nicvf->cpi_alg = CPI_ALG_NONE;
+	nicvf_config_cpi(nicvf);
+
+	/* Initialize the queues */
+	err = nicvf_init_resources(nicvf);
+	if (err)
+		return -1;
+
+	if (!nicvf_check_pf_ready(nicvf))
+		return -1;
+
+	/* Make sure queue initialization is written */
+	asm volatile ("dsb sy");
+
+	return 0;
+}
+
+int nicvf_initialize(struct nicpf *nicpf, int vf_num, unsigned int node)
+{
+	struct eth_device *netdev = NULL;
+	struct nicvf *nicvf = NULL;
+	int    err;
+
+	netdev = malloc(sizeof(struct eth_device));
+
+	if (!netdev) {
+		err = -1;
+		goto fail;
+	}
+
+	nicvf = malloc(sizeof(struct nicvf));
+
+	if (!nicvf) {
+		err = -1;
+		goto fail;
+	}
+
+	netdev->priv = nicvf;
+	nicvf->netdev = netdev;
+	nicvf->nicpf = nicpf;
+	nicvf->vf_id = vf_num;
+
+	/* MAP VF's configuration registers */
+	nicvf->reg_base = CSR_PA(NIC_VFX_BAR0(vf_num), node);
+	if (!nicvf->reg_base) {
+		printf("Cannot map config register space, aborting\n");
+		err = -1;
+		goto fail;
+	}
+
+	err = nicvf_set_qset_resources(nicvf);
+	if (err)
+		return -1;
+
+	snprintf(netdev->name, sizeof(netdev->name), "vnic%u", vf_num);
+
+	netdev->halt = nicvf_stop;
+	netdev->init = nicvf_open;
+	netdev->send = nicvf_xmit;
+	netdev->recv = nicvf_recv;
+
+	if (!eth_getenv_enetaddr_by_index("eth", vf_num, netdev->enetaddr)) {
+		eth_getenv_enetaddr("ethaddr", netdev->enetaddr);
+		netdev->enetaddr[5] += vf_num;
+	}
+
+	err = eth_register(netdev);
+
+	if (err) {
+		printf("Failed to register netdevice\n");
+		return -1;
+	}
+
+	return 0;
+fail:
+	if (nicvf)
+		free(nicvf);
+	if (netdev)
+		free(netdev);
+	return err;
+}
diff --git a/drivers/net/cavium/nicvf_queues.c b/drivers/net/cavium/nicvf_queues.c
new file mode 100644
index 0000000000..5eab33dbb6
--- /dev/null
+++ b/drivers/net/cavium/nicvf_queues.c
@@ -0,0 +1,1153 @@
+/*
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#include <config.h>
+#include <common.h>
+#include <net.h>
+#include <netdev.h>
+#include <malloc.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "q_struct.h"
+#include "nicvf_queues.h"
+
+struct rcv_buffer {
+	void *alloc;
+	void *data;
+};
+
+static int nicvf_poll_reg(struct nicvf *nic, int qidx,
+			  uint64_t reg, int bit_pos, int bits, int val)
+{
+	uint64_t bit_mask;
+	uint64_t reg_val;
+	int timeout = 10;
+
+	bit_mask = (1ULL << bits) - 1;
+	bit_mask = (bit_mask << bit_pos);
+
+	while (timeout) {
+		reg_val = nicvf_queue_reg_read(nic, reg, qidx);
+		if (((reg_val & bit_mask) >> bit_pos) == val)
+			return 0;
+		udelay(2000);
+		timeout--;
+	}
+	printf("Poll on reg 0x%llx failed\n", reg);
+	return 1;
+}
+
+static int nicvf_alloc_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem,
+				  int q_len, int desc_size, int align_bytes)
+{
+	dmem->q_len = q_len;
+	dmem->size = (desc_size * q_len) + align_bytes;
+	dmem->unalign_base = malloc(dmem->size);
+	if (!dmem->unalign_base)
+		return -1;
+
+	dmem->dma = (uintptr_t) dmem->unalign_base;
+	dmem->phys_base = NICVF_ALIGNED_ADDR((uint64_t) dmem->dma, align_bytes);
+	dmem->base = (void *)((u8 *)dmem->unalign_base +
+			      (dmem->phys_base - dmem->dma));
+	return 0;
+}
+
+static void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem)
+{
+	if (!dmem)
+		return;
+
+	free(dmem->unalign_base);
+	dmem->unalign_base = NULL;
+	dmem->base = NULL;
+}
+
+static int nicvf_alloc_rcv_buffer(struct nicvf *nic,
+				  uint64_t buf_len, void **rbuf)
+{
+	uintptr_t buf = 0;
+	void *alloc;
+	struct rcv_buffer *bufdata;
+
+	buf_len += NICVF_RCV_BUF_ALIGN_BYTES + sizeof(void *);
+
+	alloc = malloc(buf_len);
+
+	buf = (uintptr_t) alloc;
+
+	if (!buf) {
+		printf("Failed to allocate new rcv buffer\n");
+		return -1;
+	}
+
+	/* Reserve bytes for storing skb address */
+	buf += sizeof(void *);
+
+	/* Align buffer addr to cache line i.e 128 bytes */
+	buf += NICVF_RCV_BUF_ALIGN_LEN(buf);
+
+	/* Store skb address */
+	bufdata = (struct rcv_buffer *)(buf - sizeof(*bufdata));
+
+	bufdata->alloc = alloc;
+	bufdata->data = (void *)(buf);
+
+	/* Return buffer address */
+	*rbuf = bufdata->data;
+
+	return 0;
+}
+
+static void *nicvf_rb_ptr_to_pkt(struct nicvf *nic, uintptr_t rb_ptr)
+{
+	struct rcv_buffer *bufdata;
+	void *pkt;
+
+	bufdata = (struct rcv_buffer *)(rb_ptr - sizeof(*bufdata));
+
+	pkt = bufdata->data;
+	return pkt;
+}
+
+static void *nicvf_rb_ptr_to_buf(struct nicvf *nic, uintptr_t rb_ptr)
+{
+	struct rcv_buffer *bufdata;
+	void *alloc;
+
+	bufdata = (struct rcv_buffer *)(rb_ptr - sizeof(*bufdata));
+
+	alloc = bufdata->alloc;
+	return alloc;
+}
+
+static int nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,
+			   int ring_len, int buf_size)
+{
+	int idx;
+	void *rbuf;
+	struct rbdr_entry_t *desc;
+
+	if (nicvf_alloc_q_desc_mem(nic, &rbdr->dmem, ring_len,
+				   sizeof(struct rbdr_entry_t),
+				   NICVF_RCV_BUF_ALIGN_BYTES)) {
+		printf("Unable to allocate memory for rcv buffer ring\n");
+		return -1;
+	}
+
+	rbdr->desc = rbdr->dmem.base;
+	/* Buffer size has to be in multiples of 128 bytes */
+	rbdr->buf_size = buf_size;
+	rbdr->enable = true;
+	rbdr->thresh = RBDR_THRESH;
+
+	for (idx = 0; idx < ring_len; idx++) {
+		if (nicvf_alloc_rcv_buffer(nic, rbdr->buf_size, &rbuf))
+			return -1;
+
+		desc = GET_RBDR_DESC(rbdr, idx);
+		desc->buf_addr = (uintptr_t) rbuf >> NICVF_RCV_BUF_ALIGN;
+		flush_dcache_range((uintptr_t) desc,
+				   (uintptr_t) desc + sizeof(desc));
+	}
+	return 0;
+}
+
+static void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr)
+{
+	int head, tail;
+	void *pkt;
+	uint64_t buf_addr;
+	struct rbdr_entry_t *desc;
+
+	if (!rbdr)
+		return;
+
+	rbdr->enable = false;
+	if (!rbdr->dmem.base)
+		return;
+
+	head = 0;
+	tail = rbdr->dmem.q_len - 1;
+
+	/* Free SKBs */
+	while (head != tail) {
+		desc = GET_RBDR_DESC(rbdr, head);
+
+		invalidate_dcache_range((uintptr_t) desc,
+					(uintptr_t) desc + sizeof(desc));
+		buf_addr = desc->buf_addr << NICVF_RCV_BUF_ALIGN;
+		pkt = nicvf_rb_ptr_to_buf(nic, buf_addr);
+
+		free(pkt);
+
+		head++;
+		head &= (rbdr->dmem.q_len - 1);
+	}
+	/* Free SKB of tail desc */
+	desc = GET_RBDR_DESC(rbdr, tail);
+
+	invalidate_dcache_range((uintptr_t) desc,
+				(uintptr_t) desc + sizeof(desc));
+
+	buf_addr = desc->buf_addr << NICVF_RCV_BUF_ALIGN;
+
+	pkt = nicvf_rb_ptr_to_buf(nic, buf_addr);
+
+	invalidate_dcache_range(buf_addr, buf_addr + rbdr->buf_size);
+
+	free(pkt);
+
+	/* Free RBDR ring */
+	nicvf_free_q_desc_mem(nic, &rbdr->dmem);
+}
+
+/* Refill receive buffer descriptors with new buffers.
+ * This runs in softirq context .
+ */
+void nicvf_refill_rbdr(struct nicvf *nic)
+{
+	struct queue_set *qs = nic->qs;
+	int rbdr_idx = qs->rbdr_cnt;
+	int qcount;
+	struct rbdr *rbdr;
+
+refill:
+	if (!rbdr_idx)
+		return;
+	rbdr_idx--;
+	rbdr = &qs->rbdr[rbdr_idx];
+	/* Check if it's enabled */
+	if (!rbdr->enable)
+		goto next_rbdr;
+
+	/* check if valid descs reached or crossed threshold level */
+	qcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, rbdr_idx);
+	qcount &= 0x7FFFF;
+	if (qcount > rbdr->thresh)
+		goto next_rbdr;
+
+	/* Notify HW */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
+			      rbdr_idx, rbdr->thresh);
+next_rbdr:
+	if (rbdr_idx)
+		goto refill;
+}
+
+/* TBD: how to handle full packets received in CQ
+ * i.e conversion of buffers into SKBs
+ */
+static int nicvf_init_cmp_queue(struct nicvf *nic,
+				struct cmp_queue *cq, int q_len)
+{
+	if (nicvf_alloc_q_desc_mem(nic, &cq->dmem, q_len,
+				   CMP_QUEUE_DESC_SIZE,
+				   NICVF_CQ_BASE_ALIGN_BYTES)) {
+		printf("Unable to allocate memory for completion queue\n");
+		return -1;
+	}
+	cq->desc = cq->dmem.base;
+	cq->thresh = CMP_QUEUE_CQE_THRESH;
+	cq->intr_timer_thresh = CMP_QUEUE_TIMER_THRESH;
+
+	return 0;
+}
+
+static void nicvf_free_cmp_queue(struct nicvf *nic, struct cmp_queue *cq)
+{
+	if (!cq)
+		return;
+	if (!cq->dmem.base)
+		return;
+
+	nicvf_free_q_desc_mem(nic, &cq->dmem);
+}
+
+static int nicvf_init_snd_queue(struct nicvf *nic,
+				struct snd_queue *sq, int q_len)
+{
+	if (nicvf_alloc_q_desc_mem(nic, &sq->dmem, q_len,
+				   SND_QUEUE_DESC_SIZE,
+				   NICVF_SQ_BASE_ALIGN_BYTES)) {
+		printf("Unable to allocate memory for send queue\n");
+		return -1;
+	}
+
+	sq->desc = sq->dmem.base;
+	sq->skbuff = calloc(q_len, sizeof(uint64_t));
+	sq->head = 0;
+	sq->tail = 0;
+	sq->free_cnt = q_len - 1;
+	sq->thresh = SND_QUEUE_THRESH;
+
+	return 0;
+}
+
+static void nicvf_free_snd_queue(struct nicvf *nic, struct snd_queue *sq)
+{
+	if (!sq)
+		return;
+	if (!sq->dmem.base)
+		return;
+
+	free(sq->skbuff);
+	nicvf_free_q_desc_mem(nic, &sq->dmem);
+}
+
+static void nicvf_reclaim_snd_queue(struct nicvf *nic,
+				    struct queue_set *qs, int qidx)
+{
+	/* Disable send queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, 0);
+	/* Check if SQ is stopped */
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_SQ_0_7_STATUS, 21, 1, 0x01))
+		return;
+	/* Reset send queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);
+}
+
+static void nicvf_reclaim_rcv_queue(struct nicvf *nic,
+				    struct queue_set *qs, int qidx)
+{
+	struct nic_mbx mbx = { };
+
+	/* Make sure all packets in the pipeline are written back into mem */
+
+	mbx.msg = NIC_PF_VF_MSG_RQ_SW_SYNC;
+	mbx.data.rq.cfg = 0;
+	nicvf_send_msg_to_pf(nic, &mbx);
+}
+
+static void nicvf_reclaim_cmp_queue(struct nicvf *nic,
+				    struct queue_set *qs, int qidx)
+{
+	/* Disable timer threshold (doesn't get reset upon CQ reset */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2, qidx, 0);
+	/* Disable completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, 0);
+	/* Reset completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);
+}
+
+static void nicvf_reclaim_rbdr(struct nicvf *nic, struct rbdr *rbdr, int qidx)
+{
+	uint64_t tmp;
+	int timeout = 10;
+
+	/* Save head and tail pointers for feeing up buffers */
+	rbdr->head = nicvf_queue_reg_read(nic,
+					  NIC_QSET_RBDR_0_1_HEAD, qidx) >> 3;
+	rbdr->tail = nicvf_queue_reg_read(nic,
+					  NIC_QSET_RBDR_0_1_TAIL, qidx) >> 3;
+
+	/* Disable RBDR */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))
+		return;
+	while (1) {
+		tmp = nicvf_queue_reg_read(nic,
+					   NIC_QSET_RBDR_0_1_PREFETCH_STATUS,
+					   qidx);
+		if ((tmp & 0xFFFFFFFF) == ((tmp >> 32) & 0xFFFFFFFF))
+			break;
+		udelay(2000);
+		timeout--;
+		if (!timeout) {
+			printf("Failed polling on prefetch status\n");
+			return;
+		}
+	}
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,
+			      qidx, NICVF_RBDR_RESET);
+
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x02))
+		return;
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0x00);
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))
+		return;
+}
+
+static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
+				   int qidx, bool enable)
+{
+	struct nic_mbx mbx = { };
+	struct rcv_queue *rq;
+
+	union {
+		uintptr_t u;
+		struct rq_cfg s;
+	} rq_cfg;
+
+	rq = &qs->rq[qidx];
+	rq->enable = enable;
+
+	/* Disable receive queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, 0);
+
+	if (!rq->enable) {
+		nicvf_reclaim_rcv_queue(nic, qs, qidx);
+		return;
+	}
+
+	rq->cq_qs = qs->vnic_id;
+	rq->cq_idx = qidx;
+	rq->start_rbdr_qs = qs->vnic_id;
+	rq->start_qs_rbdr_idx = qs->rbdr_cnt - 1;
+	rq->cont_rbdr_qs = qs->vnic_id;
+	rq->cont_qs_rbdr_idx = qs->rbdr_cnt - 1;
+	rq->caching = 0;
+
+	/* Send a mailbox msg to PF to config RQ */
+	mbx.msg = NIC_PF_VF_MSG_RQ_CFG;
+	mbx.data.rq.qs_num = qs->vnic_id;
+	mbx.data.rq.rq_num = qidx;
+	mbx.data.rq.cfg = (rq->caching << 26) | (rq->cq_qs << 19) |
+	    (rq->cq_idx << 16) | (rq->cont_rbdr_qs << 9) |
+	    (rq->cont_qs_rbdr_idx << 8) |
+	    (rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	mbx.msg = NIC_PF_VF_MSG_RQ_BP_CFG;
+	mbx.data.rq.cfg = (1ULL << 63) | (1ULL << 62) | (qs->vnic_id << 0);
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	/* RQ drop config
+	 * Enable CQ drop to reserve sufficient CQEs for all tx packets
+	 */
+	mbx.msg = NIC_PF_VF_MSG_RQ_DROP_CFG;
+	mbx.data.rq.cfg = (1ULL << 62) | (RQ_CQ_DROP << 8);
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, qidx, 0x00);
+
+	/* Enable Receive queue */
+	rq_cfg.s.ena = 1;
+	rq_cfg.s.tcp_ena = 0;
+
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, rq_cfg.u);
+}
+
+void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
+			    int qidx, bool enable)
+{
+	struct cmp_queue *cq;
+	union {
+		uint64_t u;
+		struct cq_cfg s;
+	} cq_cfg;
+
+	cq = &qs->cq[qidx];
+	cq->enable = enable;
+
+	if (!cq->enable) {
+		nicvf_reclaim_cmp_queue(nic, qs, qidx);
+		return;
+	}
+
+	/* Reset completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);
+
+	if (!cq->enable)
+		return;
+
+	/* Set completion queue base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE,
+			      qidx, (uint64_t) (cq->dmem.phys_base));
+
+	/* Enable Completion queue */
+	cq_cfg.s.ena = 1;
+	cq_cfg.s.reset = 0;
+	cq_cfg.s.caching = 0;
+	cq_cfg.s.qsize = CMP_QSIZE;
+	cq_cfg.s.avg_con = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, cq_cfg.u);
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx, cq->thresh);
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2,
+			      qidx, cq->intr_timer_thresh);
+}
+
+static void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,
+				   int qidx, bool enable)
+{
+	struct nic_mbx mbx = { };
+	struct snd_queue *sq;
+
+	union {
+		uint64_t u;
+		struct sq_cfg s;
+	} sq_cfg;
+
+	sq = &qs->sq[qidx];
+	sq->enable = enable;
+
+	if (!sq->enable) {
+		nicvf_reclaim_snd_queue(nic, qs, qidx);
+		return;
+	}
+
+	/* Reset send queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);
+
+	sq->cq_qs = qs->vnic_id;
+	sq->cq_idx = qidx;
+
+	/* Send a mailbox msg to PF to config SQ */
+	mbx.msg = NIC_PF_VF_MSG_SQ_CFG;
+	mbx.data.sq.qs_num = qs->vnic_id;
+	mbx.data.sq.sq_num = qidx;
+	mbx.data.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	/* Set queue base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE,
+			      qidx, (uint64_t) (sq->dmem.phys_base));
+
+	/* Enable send queue  & set queue size */
+	sq_cfg.s.ena = 1;
+	sq_cfg.s.reset = 0;
+	sq_cfg.s.ldwb = 0;
+	sq_cfg.s.qsize = SND_QSIZE;
+	sq_cfg.s.tstmp_bgx_intf = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg.u);
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx, sq->thresh);
+}
+
+static void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,
+			      int qidx, bool enable)
+{
+	struct rbdr *rbdr;
+
+	union {
+		uint64_t u;
+		struct rbdr_cfg s;
+	} rbdr_cfg;
+
+	rbdr = &qs->rbdr[qidx];
+	nicvf_reclaim_rbdr(nic, rbdr, qidx);
+	if (!enable)
+		return;
+
+	/* Set descriptor base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE,
+			      qidx, (uint64_t) (rbdr->dmem.phys_base));
+
+	/* Enable RBDR  & set queue size */
+	/* Buffer size should be in multiples of 128 bytes */
+	rbdr_cfg.s.ena = 1;
+	rbdr_cfg.s.reset = 0;
+	rbdr_cfg.s.ldwb = 0;
+	rbdr_cfg.s.qsize = RBDR_SIZE;
+	rbdr_cfg.s.avg_con = 0;
+	rbdr_cfg.s.lines = rbdr->buf_size / 128;
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, rbdr_cfg.u);
+
+	/* Notify HW */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
+			      qidx, qs->rbdr_len - 1);
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_THRESH,
+			      qidx, rbdr->thresh - 1);
+}
+
+void nicvf_qset_config(struct nicvf *nic, bool enable)
+{
+	struct nic_mbx mbx = { };
+	struct queue_set *qs = nic->qs;
+	struct qs_cfg *qs_cfg;
+
+	if (!qs) {
+		printf("Qset is still not allocated, don't init queues\n");
+		return;
+	}
+
+	qs->enable = enable;
+	qs->vnic_id = nic->vf_id;
+
+	/* Send a mailbox msg to PF to config Qset */
+	mbx.msg = NIC_PF_VF_MSG_QS_CFG;
+	mbx.data.qs.num = qs->vnic_id;
+
+	mbx.data.qs.cfg = 0;
+	qs_cfg = (struct qs_cfg *)&mbx.data.qs.cfg;
+	if (qs->enable) {
+		qs_cfg->ena = 1;
+#ifdef __BIG_ENDIAN
+		qs_cfg->be = 1;
+#endif
+		qs_cfg->vnic = qs->vnic_id;
+	}
+
+	nicvf_send_msg_to_pf(nic, &mbx);
+}
+
+static void nicvf_free_resources(struct nicvf *nic)
+{
+	int qidx;
+	struct queue_set *qs = nic->qs;
+
+	/* Free receive buffer descriptor ring */
+	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+		nicvf_free_rbdr(nic, &qs->rbdr[qidx]);
+
+	/* Free completion queue */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+		nicvf_free_cmp_queue(nic, &qs->cq[qidx]);
+
+	/* Free send queue */
+	for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+		nicvf_free_snd_queue(nic, &qs->sq[qidx]);
+}
+
+static int nicvf_alloc_resources(struct nicvf *nic)
+{
+	int qidx;
+	struct queue_set *qs = nic->qs;
+
+	/* Alloc receive buffer descriptor ring */
+	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {
+		if (nicvf_init_rbdr(nic, &qs->rbdr[qidx], qs->rbdr_len,
+				    RCV_BUFFER_LEN))
+			goto alloc_fail;
+	}
+
+	/* Alloc send queue */
+	for (qidx = 0; qidx < qs->sq_cnt; qidx++) {
+		if (nicvf_init_snd_queue(nic, &qs->sq[qidx], qs->sq_len))
+			goto alloc_fail;
+	}
+
+	/* Alloc completion queue */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		if (nicvf_init_cmp_queue(nic, &qs->cq[qidx], qs->cq_len))
+			goto alloc_fail;
+	}
+
+	return 0;
+alloc_fail:
+	nicvf_free_resources(nic);
+	return -1;
+}
+
+int nicvf_set_qset_resources(struct nicvf *nic)
+{
+	struct queue_set *qs;
+
+	qs = malloc(sizeof(struct queue_set));
+	if (!qs)
+		return -1;
+	nic->qs = qs;
+
+	/* Set count of each queue */
+	qs->rbdr_cnt = RBDR_CNT;
+	qs->rq_cnt = RCV_QUEUE_CNT;
+	qs->sq_cnt = SND_QUEUE_CNT;
+	qs->cq_cnt = CMP_QUEUE_CNT;
+
+	/* Set queue lengths */
+	qs->rbdr_len = RCV_BUF_COUNT;
+	qs->sq_len = SND_QUEUE_LEN;
+	qs->cq_len = CMP_QUEUE_LEN;
+	return 0;
+}
+
+int nicvf_config_data_transfer(struct nicvf *nic, bool enable)
+{
+	bool disable = false;
+	struct queue_set *qs = nic->qs;
+	int qidx;
+
+	if (!qs)
+		return 0;
+
+	if (enable) {
+		if (nicvf_alloc_resources(nic))
+			return -1;
+
+		for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+			nicvf_snd_queue_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+			nicvf_cmp_queue_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+			nicvf_rbdr_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+			nicvf_rcv_queue_config(nic, qs, qidx, enable);
+	} else {
+		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+			nicvf_rcv_queue_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+			nicvf_rbdr_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+			nicvf_snd_queue_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+			nicvf_cmp_queue_config(nic, qs, qidx, disable);
+
+		nicvf_free_resources(nic);
+	}
+
+	return 0;
+}
+
+/* Get a free desc from send queue
+ * @qs:   Qset from which to get a SQ descriptor
+ * @qnum: SQ number (0...7) in the Qset
+ *
+ * returns descriptor ponter & descriptor number
+ */
+static int nicvf_get_sq_desc(struct queue_set *qs, int qnum, void **desc)
+{
+	int qentry;
+	struct snd_queue *sq = &qs->sq[qnum];
+
+	if (!sq->free_cnt)
+		return 0;
+
+	qentry = sq->tail++;
+	sq->free_cnt--;
+
+	sq->tail &= (sq->dmem.q_len - 1);
+	*desc = GET_SQ_DESC(sq, qentry);
+	return qentry;
+}
+
+void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt)
+{
+	while (desc_cnt--) {
+		sq->free_cnt++;
+		sq->head++;
+		sq->head &= (sq->dmem.q_len - 1);
+	}
+}
+
+void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx)
+{
+	uint64_t sq_cfg;
+
+	sq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
+	sq_cfg |= NICVF_SQ_EN;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);
+	/* Ring doorbell so that H/W restarts processing SQEs */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, qidx, 0);
+}
+
+void nicvf_sq_disable(struct nicvf *nic, int qidx)
+{
+	uint64_t sq_cfg;
+
+	sq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
+	sq_cfg &= ~NICVF_SQ_EN;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);
+}
+
+void nicvf_sq_free_used_descs(struct eth_device *netdev, struct snd_queue *sq,
+			      int qidx)
+{
+	uint64_t head;
+	void *pkt;
+	struct nicvf *nic = netdev->priv;
+	struct sq_hdr_subdesc *hdr;
+
+	head = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_HEAD, qidx) >> 4;
+
+	while (sq->head != head) {
+		hdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, sq->head);
+		if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER) {
+			nicvf_put_sq_desc(sq, 1);
+			continue;
+		}
+		pkt = (void *)sq->skbuff[sq->head];
+		free(pkt);
+		nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
+	}
+}
+
+/* Add SQ HEADER subdescriptor.
+ * First subdescriptor for every send descriptor.
+ */
+struct sq_hdr_subdesc *nicvf_sq_add_hdr_subdesc(struct queue_set *qs,
+						int sq_num, int subdesc_cnt,
+						void *pkt, size_t pkt_len)
+{
+	int qentry;
+	void *desc;
+	struct snd_queue *sq;
+	struct sq_hdr_subdesc *hdr;
+
+	sq = &qs->sq[sq_num];
+	qentry = nicvf_get_sq_desc(qs, sq_num, &desc);
+	sq->skbuff[qentry] = (uintptr_t) pkt;
+
+	hdr = (struct sq_hdr_subdesc *)desc;
+
+	memset(hdr, 0, SND_QUEUE_DESC_SIZE);
+	hdr->subdesc_type = SQ_DESC_TYPE_HEADER;
+	hdr->post_cqe = 1;
+	hdr->subdesc_cnt = subdesc_cnt;
+	hdr->tot_len = pkt_len;
+
+	return hdr;
+}
+
+/* SQ GATHER subdescriptor
+ * Must follow HDR descriptor
+ */
+static void nicvf_sq_add_gather_subdesc(struct nicvf *nic, struct queue_set *qs,
+					int sq_num, void *pkt, size_t pkt_len)
+{
+	void *desc;
+	struct sq_gather_subdesc *gather;
+
+	nicvf_get_sq_desc(qs, sq_num, &desc);
+	gather = (struct sq_gather_subdesc *)desc;
+
+	memset(gather, 0, SND_QUEUE_DESC_SIZE);
+	gather->subdesc_type = SQ_DESC_TYPE_GATHER;
+	gather->ld_type = NIC_SEND_LD_TYPE_E_LDD;
+	gather->size = pkt_len;
+	gather->addr = (uintptr_t) pkt;
+
+	flush_dcache_range(gather->addr, gather->addr + gather->size);
+}
+
+/* Append an skb to a SQ for packet transfer. */
+int nicvf_sq_append_pkt(struct nicvf *nic, void *pkt, size_t pkt_size)
+{
+	int subdesc_cnt;
+	int sq_num;
+	struct queue_set *qs = nic->qs;
+	struct snd_queue *sq;
+
+	sq_num = 0;
+	sq = &qs->sq[sq_num];
+
+	subdesc_cnt = 2;
+	if (subdesc_cnt > sq->free_cnt)
+		goto append_fail;
+
+	/* Add SQ header subdesc */
+	nicvf_sq_add_hdr_subdesc(qs, sq_num, subdesc_cnt - 1, pkt, pkt_size);
+
+	/* Add SQ gather subdesc */
+	nicvf_sq_add_gather_subdesc(nic, qs, sq_num, pkt, pkt_size);
+
+	/* make sure all memory stores are done before ringing doorbell */
+	asm volatile ("dsb sy");
+
+	/* Inform HW to xmit new packet */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, sq_num, subdesc_cnt);
+	return 1;
+
+append_fail:
+	printf("Not enough SQ descriptors to xmit pkt\n");
+	return 0;
+}
+
+static unsigned frag_num(unsigned i)
+{
+#ifdef __BIG_ENDIAN
+	return (i & ~3) + 3 - (i & 3);
+#else
+	return i;
+#endif
+}
+
+void *nicvf_get_rcv_pkt(struct nicvf *nic, void *cq_desc, size_t *pkt_len)
+{
+	int frag;
+	int payload_len = 0;
+	void *pkt = NULL;
+	struct cqe_rx_t *cqe_rx;
+	struct rbdr *rbdr;
+	struct rcv_queue *rq;
+	struct queue_set *qs = nic->qs;
+	uint16_t *rb_lens = NULL;
+	uint64_t *rb_ptrs = NULL;
+
+	cqe_rx = (struct cqe_rx_t *)cq_desc;
+
+	rq = &qs->rq[cqe_rx->rq_idx];
+	rbdr = &qs->rbdr[rq->start_qs_rbdr_idx];
+	rb_lens = cq_desc + (3 * sizeof(uint64_t));	/* Use offsetof */
+	rb_ptrs = cq_desc + (6 * sizeof(uint64_t));
+
+	debug("%s rb_cnt %d rb0_ptr %llx rb0_sz %d\n",
+	      __func__, cqe_rx->rb_cnt, cqe_rx->rb0_ptr, cqe_rx->rb0_sz);
+
+	for (frag = 0; frag < cqe_rx->rb_cnt; frag++) {
+		payload_len = rb_lens[frag_num(frag)];
+
+		invalidate_dcache_range((uintptr_t) (*rb_ptrs),
+					(uintptr_t) (*rb_ptrs) +
+					rbdr->buf_size);
+
+		/* First fragment */
+		*rb_ptrs = *rb_ptrs - cqe_rx->align_pad;
+		pkt = nicvf_rb_ptr_to_pkt(nic, *rb_ptrs);
+
+		invalidate_dcache_range((uintptr_t) pkt,
+					(uintptr_t) pkt + payload_len);
+
+		if (cqe_rx->align_pad)
+			pkt += cqe_rx->align_pad;
+
+		/* Next buffer pointer */
+		rb_ptrs++;
+
+		*pkt_len = payload_len;
+	}
+	return pkt;
+}
+
+/* Clear interrupt */
+void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)
+{
+	uint64_t reg_val = 0;
+
+	switch (int_type) {
+	case NICVF_INTR_CQ:
+		reg_val = ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);
+		break;
+	case NICVF_INTR_SQ:
+		reg_val = ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);
+		break;
+	case NICVF_INTR_RBDR:
+		reg_val = ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);
+		break;
+	case NICVF_INTR_PKT_DROP:
+		reg_val = (1ULL << NICVF_INTR_PKT_DROP_SHIFT);
+		break;
+	case NICVF_INTR_TCP_TIMER:
+		reg_val = (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);
+		break;
+	case NICVF_INTR_MBOX:
+		reg_val = (1ULL << NICVF_INTR_MBOX_SHIFT);
+		break;
+	case NICVF_INTR_QS_ERR:
+		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
+		break;
+	default:
+		printf("Failed to clear interrupt: unknown type\n");
+		break;
+	}
+
+	nicvf_reg_write(nic, NIC_VF_INT, reg_val);
+}
+
+void nicvf_update_rq_stats(struct nicvf *nic, int rq_idx)
+{
+	struct rcv_queue *rq;
+
+#define GET_RQ_STATS(reg) \
+	nicvf_reg_read(nic, NIC_QSET_RQ_0_7_STAT_0_1 |\
+			    (rq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))
+
+	rq = &nic->qs->rq[rq_idx];
+	rq->stats.bytes = GET_RQ_STATS(RQ_SQ_STATS_OCTS);
+	rq->stats.pkts = GET_RQ_STATS(RQ_SQ_STATS_PKTS);
+}
+
+void nicvf_update_sq_stats(struct nicvf *nic, int sq_idx)
+{
+	struct snd_queue *sq;
+
+#define GET_SQ_STATS(reg) \
+	nicvf_reg_read(nic, NIC_QSET_SQ_0_7_STAT_0_1 |\
+			    (sq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))
+
+	sq = &nic->qs->sq[sq_idx];
+	sq->stats.bytes = GET_SQ_STATS(RQ_SQ_STATS_OCTS);
+	sq->stats.pkts = GET_SQ_STATS(RQ_SQ_STATS_PKTS);
+}
+
+/* Check for errors in the receive cmp.queue entry */
+int nicvf_check_cqe_rx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, void *cq_desc)
+{
+	struct cqe_rx_t *cqe_rx;
+	struct cmp_queue_stats *stats = &cq->stats;
+
+	cqe_rx = (struct cqe_rx_t *)cq_desc;
+	if (!cqe_rx->err_level && !cqe_rx->err_opcode) {
+		stats->rx.errop.good++;
+		return 0;
+	}
+
+	switch (cqe_rx->err_level) {
+	case CQ_ERRLVL_MAC:
+		stats->rx.errlvl.mac_errs++;
+		break;
+	case CQ_ERRLVL_L2:
+		stats->rx.errlvl.l2_errs++;
+		break;
+	case CQ_ERRLVL_L3:
+		stats->rx.errlvl.l3_errs++;
+		break;
+	case CQ_ERRLVL_L4:
+		stats->rx.errlvl.l4_errs++;
+		break;
+	}
+
+	switch (cqe_rx->err_opcode) {
+	case CQ_RX_ERROP_RE_PARTIAL:
+		stats->rx.errop.partial_pkts++;
+		break;
+	case CQ_RX_ERROP_RE_JABBER:
+		stats->rx.errop.jabber_errs++;
+		break;
+	case CQ_RX_ERROP_RE_FCS:
+		stats->rx.errop.fcs_errs++;
+		break;
+	case CQ_RX_ERROP_RE_TERMINATE:
+		stats->rx.errop.terminate_errs++;
+		break;
+	case CQ_RX_ERROP_RE_RX_CTL:
+		stats->rx.errop.bgx_rx_errs++;
+		break;
+	case CQ_RX_ERROP_PREL2_ERR:
+		stats->rx.errop.prel2_errs++;
+		break;
+	case CQ_RX_ERROP_L2_FRAGMENT:
+		stats->rx.errop.l2_frags++;
+		break;
+	case CQ_RX_ERROP_L2_OVERRUN:
+		stats->rx.errop.l2_overruns++;
+		break;
+	case CQ_RX_ERROP_L2_PFCS:
+		stats->rx.errop.l2_pfcs++;
+		break;
+	case CQ_RX_ERROP_L2_PUNY:
+		stats->rx.errop.l2_puny++;
+		break;
+	case CQ_RX_ERROP_L2_MAL:
+		stats->rx.errop.l2_hdr_malformed++;
+		break;
+	case CQ_RX_ERROP_L2_OVERSIZE:
+		stats->rx.errop.l2_oversize++;
+		break;
+	case CQ_RX_ERROP_L2_UNDERSIZE:
+		stats->rx.errop.l2_undersize++;
+		break;
+	case CQ_RX_ERROP_L2_LENMISM:
+		stats->rx.errop.l2_len_mismatch++;
+		break;
+	case CQ_RX_ERROP_L2_PCLP:
+		stats->rx.errop.l2_pclp++;
+		break;
+	case CQ_RX_ERROP_IP_NOT:
+		stats->rx.errop.non_ip++;
+		break;
+	case CQ_RX_ERROP_IP_CSUM_ERR:
+		stats->rx.errop.ip_csum_err++;
+		break;
+	case CQ_RX_ERROP_IP_MAL:
+		stats->rx.errop.ip_hdr_malformed++;
+		break;
+	case CQ_RX_ERROP_IP_MALD:
+		stats->rx.errop.ip_payload_malformed++;
+		break;
+	case CQ_RX_ERROP_IP_HOP:
+		stats->rx.errop.ip_hop_errs++;
+		break;
+	case CQ_RX_ERROP_L3_ICRC:
+		stats->rx.errop.l3_icrc_errs++;
+		break;
+	case CQ_RX_ERROP_L3_PCLP:
+		stats->rx.errop.l3_pclp++;
+		break;
+	case CQ_RX_ERROP_L4_MAL:
+		stats->rx.errop.l4_malformed++;
+		break;
+	case CQ_RX_ERROP_L4_CHK:
+		stats->rx.errop.l4_csum_errs++;
+		break;
+	case CQ_RX_ERROP_UDP_LEN:
+		stats->rx.errop.udp_len_err++;
+		break;
+	case CQ_RX_ERROP_L4_PORT:
+		stats->rx.errop.bad_l4_port++;
+		break;
+	case CQ_RX_ERROP_TCP_FLAG:
+		stats->rx.errop.bad_tcp_flag++;
+		break;
+	case CQ_RX_ERROP_TCP_OFFSET:
+		stats->rx.errop.tcp_offset_errs++;
+		break;
+	case CQ_RX_ERROP_L4_PCLP:
+		stats->rx.errop.l4_pclp++;
+		break;
+	case CQ_RX_ERROP_RBDR_TRUNC:
+		stats->rx.errop.pkt_truncated++;
+		break;
+	}
+
+	return 1;
+}
+
+/* Check for errors in the send cmp.queue entry */
+int nicvf_check_cqe_tx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, void *cq_desc)
+{
+	struct cqe_send_t *cqe_tx;
+	struct cmp_queue_stats *stats = &cq->stats;
+
+	cqe_tx = (struct cqe_send_t *)cq_desc;
+	switch (cqe_tx->send_status) {
+	case CQ_TX_ERROP_GOOD:
+		stats->tx.good++;
+		return 0;
+		break;
+	case CQ_TX_ERROP_DESC_FAULT:
+		stats->tx.desc_fault++;
+		break;
+	case CQ_TX_ERROP_HDR_CONS_ERR:
+		stats->tx.hdr_cons_err++;
+		break;
+	case CQ_TX_ERROP_SUBDC_ERR:
+		stats->tx.subdesc_err++;
+		break;
+	case CQ_TX_ERROP_IMM_SIZE_OFLOW:
+		stats->tx.imm_size_oflow++;
+		break;
+	case CQ_TX_ERROP_DATA_SEQUENCE_ERR:
+		stats->tx.data_seq_err++;
+		break;
+	case CQ_TX_ERROP_MEM_SEQUENCE_ERR:
+		stats->tx.mem_seq_err++;
+		break;
+	case CQ_TX_ERROP_LOCK_VIOL:
+		stats->tx.lock_viol++;
+		break;
+	case CQ_TX_ERROP_DATA_FAULT:
+		stats->tx.data_fault++;
+		break;
+	case CQ_TX_ERROP_TSTMP_CONFLICT:
+		stats->tx.tstmp_conflict++;
+		break;
+	case CQ_TX_ERROP_TSTMP_TIMEOUT:
+		stats->tx.tstmp_timeout++;
+		break;
+	case CQ_TX_ERROP_MEM_FAULT:
+		stats->tx.mem_fault++;
+		break;
+	case CQ_TX_ERROP_CK_OVERLAP:
+		stats->tx.csum_overlap++;
+		break;
+	case CQ_TX_ERROP_CK_OFLOW:
+		stats->tx.csum_overflow++;
+		break;
+	}
+
+	return 1;
+}
diff --git a/drivers/net/cavium/nicvf_queues.h b/drivers/net/cavium/nicvf_queues.h
new file mode 100644
index 0000000000..0668324641
--- /dev/null
+++ b/drivers/net/cavium/nicvf_queues.h
@@ -0,0 +1,364 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#ifndef NICVF_QUEUES_H
+#define NICVF_QUEUES_H
+
+#include "q_struct.h"
+
+#define MAX_QUEUE_SET			128
+#define MAX_RCV_QUEUES_PER_QS		8
+#define MAX_RCV_BUF_DESC_RINGS_PER_QS	2
+#define MAX_SND_QUEUES_PER_QS		8
+#define MAX_CMP_QUEUES_PER_QS		8
+
+/* VF's queue interrupt ranges */
+#define	NICVF_INTR_ID_CQ		0
+#define	NICVF_INTR_ID_SQ		8
+#define	NICVF_INTR_ID_RBDR		16
+#define	NICVF_INTR_ID_MISC		18
+#define	NICVF_INTR_ID_QS_ERR		19
+
+#define	for_each_cq_irq(irq)	\
+	for (irq = NICVF_INTR_ID_CQ; irq < NICVF_INTR_ID_SQ; irq++)
+#define	for_each_sq_irq(irq)	\
+	for (irq = NICVF_INTR_ID_SQ; irq < NICVF_INTR_ID_RBDR; irq++)
+#define	for_each_rbdr_irq(irq)	\
+	for (irq = NICVF_INTR_ID_RBDR; irq < NICVF_INTR_ID_MISC; irq++)
+
+#define RBDR_SIZE0		0ULL	/* 8K entries */
+#define RBDR_SIZE1		1ULL	/* 16K entries */
+#define RBDR_SIZE2		2ULL	/* 32K entries */
+#define RBDR_SIZE3		3ULL	/* 64K entries */
+#define RBDR_SIZE4		4ULL	/* 126K entries */
+#define RBDR_SIZE5		5ULL	/* 256K entries */
+#define RBDR_SIZE6		6ULL	/* 512K entries */
+
+#define SND_QUEUE_SIZE0		0ULL	/* 1K entries */
+#define SND_QUEUE_SIZE1		1ULL	/* 2K entries */
+#define SND_QUEUE_SIZE2		2ULL	/* 4K entries */
+#define SND_QUEUE_SIZE3		3ULL	/* 8K entries */
+#define SND_QUEUE_SIZE4		4ULL	/* 16K entries */
+#define SND_QUEUE_SIZE5		5ULL	/* 32K entries */
+#define SND_QUEUE_SIZE6		6ULL	/* 64K entries */
+
+#define CMP_QUEUE_SIZE0		0ULL	/* 1K entries */
+#define CMP_QUEUE_SIZE1		1ULL	/* 2K entries */
+#define CMP_QUEUE_SIZE2		2ULL	/* 4K entries */
+#define CMP_QUEUE_SIZE3		3ULL	/* 8K entries */
+#define CMP_QUEUE_SIZE4		4ULL	/* 16K entries */
+#define CMP_QUEUE_SIZE5		5ULL	/* 32K entries */
+#define CMP_QUEUE_SIZE6		6ULL	/* 64K entries */
+
+/* Default queue count per QS, its lengths and threshold values */
+#define RBDR_CNT			1
+#define RCV_QUEUE_CNT		1
+#define SND_QUEUE_CNT		1
+#define CMP_QUEUE_CNT		1	/* Max of RCV and SND qcount */
+
+#define SND_QSIZE		SND_QUEUE_SIZE0
+#define SND_QUEUE_LEN		(1ULL << (SND_QSIZE + 10))
+#define SND_QUEUE_THRESH	2ULL
+#define MIN_SQ_DESC_PER_PKT_XMIT	2
+#define MAX_CQE_PER_PKT_XMIT		2
+
+#define CMP_QSIZE		CMP_QUEUE_SIZE0
+#define CMP_QUEUE_LEN		(1ULL << (CMP_QSIZE + 10))
+#define CMP_QUEUE_CQE_THRESH	0
+#define CMP_QUEUE_TIMER_THRESH	1	/* 1 ms */
+
+#define RBDR_SIZE		RBDR_SIZE0
+#define RCV_BUF_COUNT	(1ULL << (RBDR_SIZE + 13))
+#define RBDR_THRESH		(RCV_BUF_COUNT / 1024)
+#define RCV_BUFFER_LEN	2048	/* In multiples of 128bytes */
+
+#define MAX_CQES_FOR_TX		((SND_QUEUE_LEN / MIN_SQ_DESC_PER_PKT_XMIT) *\
+				 MAX_CQE_PER_PKT_XMIT)
+#define RQ_CQ_DROP		((CMP_QUEUE_LEN - MAX_CQES_FOR_TX) / 256)
+
+/* Descriptor size */
+#define SND_QUEUE_DESC_SIZE	16	/* 128 bits */
+#define CMP_QUEUE_DESC_SIZE	512
+
+/* Buffer / descriptor alignments */
+#define NICVF_RCV_BUF_ALIGN		7
+#define NICVF_RCV_BUF_ALIGN_BYTES	(1ULL << NICVF_RCV_BUF_ALIGN)
+#define NICVF_CQ_BASE_ALIGN_BYTES	512	/* 9 bits */
+#define NICVF_SQ_BASE_ALIGN_BYTES	128	/* 7 bits */
+
+#define NICVF_ALIGNED_ADDR(ADDR, ALIGN_BYTES)	ALIGN(ADDR, ALIGN_BYTES)
+#define NICVF_ADDR_ALIGN_LEN(ADDR, BYTES)\
+	(NICVF_ALIGNED_ADDR(ADDR, BYTES) - BYTES)
+#define NICVF_RCV_BUF_ALIGN_LEN(X)\
+	(NICVF_ALIGNED_ADDR(X, NICVF_RCV_BUF_ALIGN_BYTES) - X)
+
+/* Queue enable/disable */
+#define NICVF_SQ_EN            (1ULL << 19)
+
+/* Queue reset */
+#define NICVF_CQ_RESET		(1ULL << 41)
+#define NICVF_SQ_RESET		(1ULL << 17)
+#define NICVF_RBDR_RESET	(1ULL << 43)
+
+enum CQ_RX_ERRLVL_E {
+	CQ_ERRLVL_MAC,
+	CQ_ERRLVL_L2,
+	CQ_ERRLVL_L3,
+	CQ_ERRLVL_L4,
+};
+
+enum CQ_RX_ERROP_E {
+	CQ_RX_ERROP_RE_NONE = 0x0,
+	CQ_RX_ERROP_RE_PARTIAL = 0x1,
+	CQ_RX_ERROP_RE_JABBER = 0x2,
+	CQ_RX_ERROP_RE_FCS = 0x7,
+	CQ_RX_ERROP_RE_TERMINATE = 0x9,
+	CQ_RX_ERROP_RE_RX_CTL = 0xb,
+	CQ_RX_ERROP_PREL2_ERR = 0x1f,
+	CQ_RX_ERROP_L2_FRAGMENT = 0x20,
+	CQ_RX_ERROP_L2_OVERRUN = 0x21,
+	CQ_RX_ERROP_L2_PFCS = 0x22,
+	CQ_RX_ERROP_L2_PUNY = 0x23,
+	CQ_RX_ERROP_L2_MAL = 0x24,
+	CQ_RX_ERROP_L2_OVERSIZE = 0x25,
+	CQ_RX_ERROP_L2_UNDERSIZE = 0x26,
+	CQ_RX_ERROP_L2_LENMISM = 0x27,
+	CQ_RX_ERROP_L2_PCLP = 0x28,
+	CQ_RX_ERROP_IP_NOT = 0x41,
+	CQ_RX_ERROP_IP_CSUM_ERR = 0x42,
+	CQ_RX_ERROP_IP_MAL = 0x43,
+	CQ_RX_ERROP_IP_MALD = 0x44,
+	CQ_RX_ERROP_IP_HOP = 0x45,
+	CQ_RX_ERROP_L3_ICRC = 0x46,
+	CQ_RX_ERROP_L3_PCLP = 0x47,
+	CQ_RX_ERROP_L4_MAL = 0x61,
+	CQ_RX_ERROP_L4_CHK = 0x62,
+	CQ_RX_ERROP_UDP_LEN = 0x63,
+	CQ_RX_ERROP_L4_PORT = 0x64,
+	CQ_RX_ERROP_TCP_FLAG = 0x65,
+	CQ_RX_ERROP_TCP_OFFSET = 0x66,
+	CQ_RX_ERROP_L4_PCLP = 0x67,
+	CQ_RX_ERROP_RBDR_TRUNC = 0x70,
+};
+
+enum CQ_TX_ERROP_E {
+	CQ_TX_ERROP_GOOD = 0x0,
+	CQ_TX_ERROP_DESC_FAULT = 0x10,
+	CQ_TX_ERROP_HDR_CONS_ERR = 0x11,
+	CQ_TX_ERROP_SUBDC_ERR = 0x12,
+	CQ_TX_ERROP_IMM_SIZE_OFLOW = 0x80,
+	CQ_TX_ERROP_DATA_SEQUENCE_ERR = 0x81,
+	CQ_TX_ERROP_MEM_SEQUENCE_ERR = 0x82,
+	CQ_TX_ERROP_LOCK_VIOL = 0x83,
+	CQ_TX_ERROP_DATA_FAULT = 0x84,
+	CQ_TX_ERROP_TSTMP_CONFLICT = 0x85,
+	CQ_TX_ERROP_TSTMP_TIMEOUT = 0x86,
+	CQ_TX_ERROP_MEM_FAULT = 0x87,
+	CQ_TX_ERROP_CK_OVERLAP = 0x88,
+	CQ_TX_ERROP_CK_OFLOW = 0x89,
+	CQ_TX_ERROP_ENUM_LAST = 0x8a,
+};
+
+struct cmp_queue_stats {
+	struct rx_stats {
+		struct {
+			u64 mac_errs;
+			u64 l2_errs;
+			u64 l3_errs;
+			u64 l4_errs;
+		} errlvl;
+		struct {
+			u64 good;
+			u64 partial_pkts;
+			u64 jabber_errs;
+			u64 fcs_errs;
+			u64 terminate_errs;
+			u64 bgx_rx_errs;
+			u64 prel2_errs;
+			u64 l2_frags;
+			u64 l2_overruns;
+			u64 l2_pfcs;
+			u64 l2_puny;
+			u64 l2_hdr_malformed;
+			u64 l2_oversize;
+			u64 l2_undersize;
+			u64 l2_len_mismatch;
+			u64 l2_pclp;
+			u64 non_ip;
+			u64 ip_csum_err;
+			u64 ip_hdr_malformed;
+			u64 ip_payload_malformed;
+			u64 ip_hop_errs;
+			u64 l3_icrc_errs;
+			u64 l3_pclp;
+			u64 l4_malformed;
+			u64 l4_csum_errs;
+			u64 udp_len_err;
+			u64 bad_l4_port;
+			u64 bad_tcp_flag;
+			u64 tcp_offset_errs;
+			u64 l4_pclp;
+			u64 pkt_truncated;
+		} errop;
+	} rx;
+	struct tx_stats {
+		u64 good;
+		u64 desc_fault;
+		u64 hdr_cons_err;
+		u64 subdesc_err;
+		u64 imm_size_oflow;
+		u64 data_seq_err;
+		u64 mem_seq_err;
+		u64 lock_viol;
+		u64 data_fault;
+		u64 tstmp_conflict;
+		u64 tstmp_timeout;
+		u64 mem_fault;
+		u64 csum_overlap;
+		u64 csum_overflow;
+	} tx;
+};
+
+enum RQ_SQ_STATS {
+	RQ_SQ_STATS_OCTS,
+	RQ_SQ_STATS_PKTS,
+};
+
+struct rx_tx_queue_stats {
+	u64 bytes;
+	u64 pkts;
+};
+
+struct q_desc_mem {
+	uintptr_t dma;
+	uint64_t size;
+	uint16_t q_len;
+	uintptr_t phys_base;
+	void *base;
+	void *unalign_base;
+};
+
+struct rbdr {
+	bool enable;
+	uint32_t buf_size;
+	uint32_t thresh;	/* Threshold level for interrupt */
+	void *desc;
+	uint32_t head;
+	uint32_t tail;
+	struct q_desc_mem dmem;
+};
+
+struct rcv_queue {
+	bool enable;
+	struct rbdr *rbdr_start;
+	struct rbdr *rbdr_cont;
+	bool en_tcp_reassembly;
+	uint8_t cq_qs;		/* CQ's QS to which this RQ is assigned */
+	uint8_t cq_idx;		/* CQ index (0 to 7) in the QS */
+	uint8_t cont_rbdr_qs;	/* Continue buffer ptrs - QS num */
+	uint8_t cont_qs_rbdr_idx;	/* RBDR idx in the cont QS */
+	uint8_t start_rbdr_qs;	/* First buffer ptrs - QS num */
+	uint8_t start_qs_rbdr_idx;	/* RBDR idx in the above QS */
+	uint8_t caching;
+	struct rx_tx_queue_stats stats;
+};
+
+struct cmp_queue {
+	bool enable;
+	uint16_t intr_timer_thresh;
+	uint16_t thresh;
+	void *desc;
+	struct q_desc_mem dmem;
+	struct cmp_queue_stats stats;
+};
+
+struct snd_queue {
+	bool enable;
+	uint8_t cq_qs;		/* CQ's QS to which this SQ is pointing */
+	uint8_t cq_idx;		/* CQ index (0 to 7) in the above QS */
+	uint16_t thresh;
+	uint32_t free_cnt;
+	uint32_t head;
+	uint32_t tail;
+	uint64_t *skbuff;
+	void *desc;
+	struct q_desc_mem dmem;
+	struct rx_tx_queue_stats stats;
+};
+
+struct queue_set {
+	bool enable;
+	bool be_en;
+	uint8_t vnic_id;
+	uint8_t rq_cnt;
+	uint8_t cq_cnt;
+	uint64_t cq_len;
+	uint8_t sq_cnt;
+	uint64_t sq_len;
+	uint8_t rbdr_cnt;
+	uint64_t rbdr_len;
+	struct rcv_queue rq[MAX_RCV_QUEUES_PER_QS];
+	struct cmp_queue cq[MAX_CMP_QUEUES_PER_QS];
+	struct snd_queue sq[MAX_SND_QUEUES_PER_QS];
+	struct rbdr rbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];
+};
+
+#define GET_RBDR_DESC(RING, idx)\
+		(&(((struct rbdr_entry_t *)((RING)->desc))[idx]))
+#define GET_SQ_DESC(RING, idx)\
+		(&(((struct sq_hdr_subdesc *)((RING)->desc))[idx]))
+#define GET_CQ_DESC(RING, idx)\
+		(&(((union cq_desc_t *)((RING)->desc))[idx]))
+
+/* CQ status bits */
+#define	CQ_WR_FULL	(1 << 26)
+#define	CQ_WR_DISABLE	(1 << 25)
+#define	CQ_WR_FAULT	(1 << 24)
+#define	CQ_CQE_COUNT	(0xFFFF << 0)
+
+#define	CQ_ERR_MASK	(CQ_WR_FULL | CQ_WR_DISABLE | CQ_WR_FAULT)
+
+int nicvf_set_qset_resources(struct nicvf *nic);
+int nicvf_config_data_transfer(struct nicvf *nic, bool enable);
+void nicvf_qset_config(struct nicvf *nic, bool enable);
+void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
+			    int qidx, bool enable);
+
+void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx);
+void nicvf_sq_disable(struct nicvf *nic, int qidx);
+void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt);
+void nicvf_sq_free_used_descs(struct eth_device *netdev,
+			      struct snd_queue *sq, int qidx);
+int nicvf_sq_append_pkt(struct nicvf *nic, void *pkt, size_t pkt_len);
+
+void *nicvf_get_rcv_pkt(struct nicvf *nic, void *cq_desc, size_t *pkt_len);
+void nicvf_refill_rbdr(struct nicvf *nic);
+
+void nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx);
+void nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx);
+void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx);
+int nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx);
+
+/* Register access APIs */
+void nicvf_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val);
+uint64_t nicvf_reg_read(struct nicvf *nic, uint64_t offset);
+void nicvf_qset_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val);
+uint64_t nicvf_qset_reg_read(struct nicvf *nic, uint64_t offset);
+void nicvf_queue_reg_write(struct nicvf *nic, uint64_t offset,
+			   uint64_t qidx, uint64_t val);
+uint64_t nicvf_queue_reg_read(struct nicvf *nic,
+			      uint64_t offset, uint64_t qidx);
+
+/* Stats */
+void nicvf_update_rq_stats(struct nicvf *nic, int rq_idx);
+void nicvf_update_sq_stats(struct nicvf *nic, int sq_idx);
+int nicvf_check_cqe_rx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, void *cq_desc);
+int nicvf_check_cqe_tx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, void *cq_desc);
+#endif /* NICVF_QUEUES_H */
diff --git a/drivers/net/cavium/q_struct.h b/drivers/net/cavium/q_struct.h
new file mode 100644
index 0000000000..9ac49ce03c
--- /dev/null
+++ b/drivers/net/cavium/q_struct.h
@@ -0,0 +1,702 @@
+/*
+ * This file contains HW queue descriptor formats, config register
+ * structures e.t.c
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#ifndef Q_STRUCT_H
+#define Q_STRUCT_H
+
+/* Load transaction types for reading segment bytes specified by
+ * NIC_SEND_GATHER_S[LD_TYPE].
+ */
+enum nic_send_ld_type_e {
+	NIC_SEND_LD_TYPE_E_LDD = 0x0,
+	NIC_SEND_LD_TYPE_E_LDT = 0x1,
+	NIC_SEND_LD_TYPE_E_LDWB = 0x2,
+	NIC_SEND_LD_TYPE_E_ENUM_LAST = 0x3,
+};
+
+enum ether_type_algorithm {
+	ETYPE_ALG_NONE = 0x0,
+	ETYPE_ALG_SKIP = 0x1,
+	ETYPE_ALG_ENDPARSE = 0x2,
+	ETYPE_ALG_VLAN = 0x3,
+	ETYPE_ALG_VLAN_STRIP = 0x4,
+};
+
+enum layer3_type {
+	L3TYPE_NONE = 0x00,
+	L3TYPE_GRH = 0x01,
+	L3TYPE_IPV4 = 0x04,
+	L3TYPE_IPV4_OPTIONS = 0x05,
+	L3TYPE_IPV6 = 0x06,
+	L3TYPE_IPV6_OPTIONS = 0x07,
+	L3TYPE_ET_STOP = 0x0D,
+	L3TYPE_OTHER = 0x0E,
+};
+
+enum layer4_type {
+	L4TYPE_NONE = 0x00,
+	L4TYPE_IPSEC_ESP = 0x01,
+	L4TYPE_IPFRAG = 0x02,
+	L4TYPE_IPCOMP = 0x03,
+	L4TYPE_TCP = 0x04,
+	L4TYPE_UDP = 0x05,
+	L4TYPE_SCTP = 0x06,
+	L4TYPE_GRE = 0x07,
+	L4TYPE_ROCE_BTH = 0x08,
+	L4TYPE_OTHER = 0x0E,
+};
+
+/* CPI and RSSI configuration */
+enum cpi_algorithm_type {
+	CPI_ALG_NONE = 0x0,
+	CPI_ALG_VLAN = 0x1,
+	CPI_ALG_VLAN16 = 0x2,
+	CPI_ALG_DIFF = 0x3,
+};
+
+enum rss_algorithm_type {
+	RSS_ALG_NONE = 0x00,
+	RSS_ALG_PORT = 0x01,
+	RSS_ALG_IP = 0x02,
+	RSS_ALG_TCP_IP = 0x03,
+	RSS_ALG_UDP_IP = 0x04,
+	RSS_ALG_SCTP_IP = 0x05,
+	RSS_ALG_GRE_IP = 0x06,
+	RSS_ALG_ROCE = 0x07,
+};
+
+enum rss_hash_cfg {
+	RSS_HASH_L2ETC = 0x00,
+	RSS_HASH_IP = 0x01,
+	RSS_HASH_TCP = 0x02,
+	RSS_TCP_SYN_DIS = 0x03,
+	RSS_HASH_UDP = 0x04,
+	RSS_HASH_L4ETC = 0x05,
+	RSS_HASH_ROCE = 0x06,
+	RSS_L3_BIDI = 0x07,
+	RSS_L4_BIDI = 0x08,
+};
+
+/* Completion queue entry types */
+enum cqe_type {
+	CQE_TYPE_INVALID = 0x0,
+	CQE_TYPE_RX = 0x2,
+	CQE_TYPE_RX_SPLIT = 0x3,
+	CQE_TYPE_RX_TCP = 0x4,
+	CQE_TYPE_SEND = 0x8,
+	CQE_TYPE_SEND_PTP = 0x9,
+};
+
+enum cqe_rx_tcp_status {
+	CQE_RX_STATUS_VALID_TCP_CNXT = 0x00,
+	CQE_RX_STATUS_INVALID_TCP_CNXT = 0x0F,
+};
+
+enum cqe_send_status {
+	CQE_SEND_STATUS_GOOD = 0x00,
+	CQE_SEND_STATUS_DESC_FAULT = 0x01,
+	CQE_SEND_STATUS_HDR_CONS_ERR = 0x11,
+	CQE_SEND_STATUS_SUBDESC_ERR = 0x12,
+	CQE_SEND_STATUS_IMM_SIZE_OFLOW = 0x80,
+	CQE_SEND_STATUS_CRC_SEQ_ERR = 0x81,
+	CQE_SEND_STATUS_DATA_SEQ_ERR = 0x82,
+	CQE_SEND_STATUS_MEM_SEQ_ERR = 0x83,
+	CQE_SEND_STATUS_LOCK_VIOL = 0x84,
+	CQE_SEND_STATUS_LOCK_UFLOW = 0x85,
+	CQE_SEND_STATUS_DATA_FAULT = 0x86,
+	CQE_SEND_STATUS_TSTMP_CONFLICT = 0x87,
+	CQE_SEND_STATUS_TSTMP_TIMEOUT = 0x88,
+	CQE_SEND_STATUS_MEM_FAULT = 0x89,
+	CQE_SEND_STATUS_CSUM_OVERLAP = 0x8A,
+	CQE_SEND_STATUS_CSUM_OVERFLOW = 0x8B,
+};
+
+enum cqe_rx_tcp_end_reason {
+	CQE_RX_TCP_END_FIN_FLAG_DET = 0,
+	CQE_RX_TCP_END_INVALID_FLAG = 1,
+	CQE_RX_TCP_END_TIMEOUT = 2,
+	CQE_RX_TCP_END_OUT_OF_SEQ = 3,
+	CQE_RX_TCP_END_PKT_ERR = 4,
+	CQE_RX_TCP_END_QS_DISABLED = 0x0F,
+};
+
+/* Packet protocol level error enumeration */
+enum cqe_rx_err_level {
+	CQE_RX_ERRLVL_RE = 0x0,
+	CQE_RX_ERRLVL_L2 = 0x1,
+	CQE_RX_ERRLVL_L3 = 0x2,
+	CQE_RX_ERRLVL_L4 = 0x3,
+};
+
+/* Packet protocol level error type enumeration */
+enum cqe_rx_err_opcode {
+	CQE_RX_ERR_RE_NONE = 0x0,
+	CQE_RX_ERR_RE_PARTIAL = 0x1,
+	CQE_RX_ERR_RE_JABBER = 0x2,
+	CQE_RX_ERR_RE_FCS = 0x7,
+	CQE_RX_ERR_RE_TERMINATE = 0x9,
+	CQE_RX_ERR_RE_RX_CTL = 0xb,
+	CQE_RX_ERR_PREL2_ERR = 0x1f,
+	CQE_RX_ERR_L2_FRAGMENT = 0x20,
+	CQE_RX_ERR_L2_OVERRUN = 0x21,
+	CQE_RX_ERR_L2_PFCS = 0x22,
+	CQE_RX_ERR_L2_PUNY = 0x23,
+	CQE_RX_ERR_L2_MAL = 0x24,
+	CQE_RX_ERR_L2_OVERSIZE = 0x25,
+	CQE_RX_ERR_L2_UNDERSIZE = 0x26,
+	CQE_RX_ERR_L2_LENMISM = 0x27,
+	CQE_RX_ERR_L2_PCLP = 0x28,
+	CQE_RX_ERR_IP_NOT = 0x41,
+	CQE_RX_ERR_IP_CHK = 0x42,
+	CQE_RX_ERR_IP_MAL = 0x43,
+	CQE_RX_ERR_IP_MALD = 0x44,
+	CQE_RX_ERR_IP_HOP = 0x45,
+	CQE_RX_ERR_L3_ICRC = 0x46,
+	CQE_RX_ERR_L3_PCLP = 0x47,
+	CQE_RX_ERR_L4_MAL = 0x61,
+	CQE_RX_ERR_L4_CHK = 0x62,
+	CQE_RX_ERR_UDP_LEN = 0x63,
+	CQE_RX_ERR_L4_PORT = 0x64,
+	CQE_RX_ERR_TCP_FLAG = 0x65,
+	CQE_RX_ERR_TCP_OFFSET = 0x66,
+	CQE_RX_ERR_L4_PCLP = 0x67,
+	CQE_RX_ERR_RBDR_TRUNC = 0x70,
+};
+
+struct cqe_rx_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t cqe_type:4;	/* W0 */
+	uint64_t stdn_fault:1;
+	uint64_t rsvd0:1;
+	uint64_t rq_qs:7;
+	uint64_t rq_idx:3;
+	uint64_t rsvd1:12;
+	uint64_t rss_alg:4;
+	uint64_t rsvd2:4;
+	uint64_t rb_cnt:4;
+	uint64_t vlan_found:1;
+	uint64_t vlan_stripped:1;
+	uint64_t vlan2_found:1;
+	uint64_t vlan2_stripped:1;
+	uint64_t l4_type:4;
+	uint64_t l3_type:4;
+	uint64_t l2_present:1;
+	uint64_t err_level:3;
+	uint64_t err_opcode:8;
+
+	uint64_t pkt_len:16;	/* W1 */
+	uint64_t l2_ptr:8;
+	uint64_t l3_ptr:8;
+	uint64_t l4_ptr:8;
+	uint64_t cq_pkt_len:8;
+	uint64_t align_pad:3;
+	uint64_t rsvd3:1;
+	uint64_t chan:12;
+
+	uint64_t rss_tag:32;	/* W2 */
+	uint64_t vlan_tci:16;
+	uint64_t vlan_ptr:8;
+	uint64_t vlan2_ptr:8;
+
+	uint64_t rb3_sz:16;	/* W3 */
+	uint64_t rb2_sz:16;
+	uint64_t rb1_sz:16;
+	uint64_t rb0_sz:16;
+
+	uint64_t rb7_sz:16;	/* W4 */
+	uint64_t rb6_sz:16;
+	uint64_t rb5_sz:16;
+	uint64_t rb4_sz:16;
+
+	uint64_t rb11_sz:16;	/* W5 */
+	uint64_t rb10_sz:16;
+	uint64_t rb9_sz:16;
+	uint64_t rb8_sz:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t err_opcode:8;
+	uint64_t err_level:3;
+	uint64_t l2_present:1;
+	uint64_t l3_type:4;
+	uint64_t l4_type:4;
+	uint64_t vlan2_stripped:1;
+	uint64_t vlan2_found:1;
+	uint64_t vlan_stripped:1;
+	uint64_t vlan_found:1;
+	uint64_t rb_cnt:4;
+	uint64_t rsvd2:4;
+	uint64_t rss_alg:4;
+	uint64_t rsvd1:12;
+	uint64_t rq_idx:3;
+	uint64_t rq_qs:7;
+	uint64_t rsvd0:1;
+	uint64_t stdn_fault:1;
+	uint64_t cqe_type:4;	/* W0 */
+	uint64_t chan:12;
+	uint64_t rsvd3:1;
+	uint64_t align_pad:3;
+	uint64_t cq_pkt_len:8;
+	uint64_t l4_ptr:8;
+	uint64_t l3_ptr:8;
+	uint64_t l2_ptr:8;
+	uint64_t pkt_len:16;	/* W1 */
+	uint64_t vlan2_ptr:8;
+	uint64_t vlan_ptr:8;
+	uint64_t vlan_tci:16;
+	uint64_t rss_tag:32;	/* W2 */
+	uint64_t rb0_sz:16;
+	uint64_t rb1_sz:16;
+	uint64_t rb2_sz:16;
+	uint64_t rb3_sz:16;	/* W3 */
+	uint64_t rb4_sz:16;
+	uint64_t rb5_sz:16;
+	uint64_t rb6_sz:16;
+	uint64_t rb7_sz:16;	/* W4 */
+	uint64_t rb8_sz:16;
+	uint64_t rb9_sz:16;
+	uint64_t rb10_sz:16;
+	uint64_t rb11_sz:16;	/* W5 */
+#endif
+	uint64_t rb0_ptr:64;
+	uint64_t rb1_ptr:64;
+	uint64_t rb2_ptr:64;
+	uint64_t rb3_ptr:64;
+	uint64_t rb4_ptr:64;
+	uint64_t rb5_ptr:64;
+	uint64_t rb6_ptr:64;
+	uint64_t rb7_ptr:64;
+	uint64_t rb8_ptr:64;
+	uint64_t rb9_ptr:64;
+	uint64_t rb10_ptr:64;
+	uint64_t rb11_ptr:64;
+};
+
+struct cqe_rx_tcp_err_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t cqe_type:4;	/* W0 */
+	uint64_t rsvd0:60;
+
+	uint64_t rsvd1:4;	/* W1 */
+	uint64_t partial_first:1;
+	uint64_t rsvd2:27;
+	uint64_t rbdr_bytes:8;
+	uint64_t rsvd3:24;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t rsvd0:60;
+	uint64_t cqe_type:4;
+
+	uint64_t rsvd3:24;
+	uint64_t rbdr_bytes:8;
+	uint64_t rsvd2:27;
+	uint64_t partial_first:1;
+	uint64_t rsvd1:4;
+#endif
+};
+
+struct cqe_rx_tcp_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t cqe_type:4;	/* W0 */
+	uint64_t rsvd0:52;
+	uint64_t cq_tcp_status:8;
+
+	uint64_t rsvd1:32;	/* W1 */
+	uint64_t tcp_cntx_bytes:8;
+	uint64_t rsvd2:8;
+	uint64_t tcp_err_bytes:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t cq_tcp_status:8;
+	uint64_t rsvd0:52;
+	uint64_t cqe_type:4;	/* W0 */
+
+	uint64_t tcp_err_bytes:16;
+	uint64_t rsvd2:8;
+	uint64_t tcp_cntx_bytes:8;
+	uint64_t rsvd1:32;	/* W1 */
+#endif
+};
+
+struct cqe_send_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t cqe_type:4;	/* W0 */
+	uint64_t rsvd0:4;
+	uint64_t sqe_ptr:16;
+	uint64_t rsvd1:4;
+	uint64_t rsvd2:10;
+	uint64_t sq_qs:7;
+	uint64_t sq_idx:3;
+	uint64_t rsvd3:8;
+	uint64_t send_status:8;
+
+	uint64_t ptp_timestamp:64;	/* W1 */
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t send_status:8;
+	uint64_t rsvd3:8;
+	uint64_t sq_idx:3;
+	uint64_t sq_qs:7;
+	uint64_t rsvd2:10;
+	uint64_t rsvd1:4;
+	uint64_t sqe_ptr:16;
+	uint64_t rsvd0:4;
+	uint64_t cqe_type:4;	/* W0 */
+
+	uint64_t ptp_timestamp:64;	/* W1 */
+#endif
+};
+
+union cq_desc_t {
+	uint64_t u[64];
+	struct cqe_send_t snd_hdr;
+	struct cqe_rx_t rx_hdr;
+	struct cqe_rx_tcp_t rx_tcp_hdr;
+	struct cqe_rx_tcp_err_t rx_tcp_err_hdr;
+};
+
+struct rbdr_entry_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t rsvd0:15;
+	uint64_t buf_addr:42;
+	uint64_t cache_align:7;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t cache_align:7;
+	uint64_t buf_addr:42;
+	uint64_t rsvd0:15;
+#endif
+};
+
+/* TCP reassembly context */
+struct rbe_tcp_cnxt_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t tcp_pkt_cnt:12;
+	uint64_t rsvd1:4;
+	uint64_t align_hdr_bytes:4;
+	uint64_t align_ptr_bytes:4;
+	uint64_t ptr_bytes:16;
+	uint64_t rsvd2:24;
+	uint64_t cqe_type:4;
+	uint64_t rsvd0:54;
+	uint64_t tcp_end_reason:2;
+	uint64_t tcp_status:4;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t tcp_status:4;
+	uint64_t tcp_end_reason:2;
+	uint64_t rsvd0:54;
+	uint64_t cqe_type:4;
+	uint64_t rsvd2:24;
+	uint64_t ptr_bytes:16;
+	uint64_t align_ptr_bytes:4;
+	uint64_t align_hdr_bytes:4;
+	uint64_t rsvd1:4;
+	uint64_t tcp_pkt_cnt:12;
+#endif
+};
+
+/* Always Big endian */
+struct rx_hdr_t {
+	uint64_t opaque:32;
+	uint64_t rss_flow:8;
+	uint64_t skip_length:6;
+	uint64_t disable_rss:1;
+	uint64_t disable_tcp_reassembly:1;
+	uint64_t nodrop:1;
+	uint64_t dest_alg:2;
+	uint64_t rsvd0:2;
+	uint64_t dest_rq:11;
+};
+
+enum send_l4_csum_type {
+	SEND_L4_CSUM_DISABLE = 0x00,
+	SEND_L4_CSUM_UDP = 0x01,
+	SEND_L4_CSUM_TCP = 0x02,
+	SEND_L4_CSUM_SCTP = 0x03,
+};
+
+enum send_crc_alg {
+	SEND_CRCALG_CRC32 = 0x00,
+	SEND_CRCALG_CRC32C = 0x01,
+	SEND_CRCALG_ICRC = 0x02,
+};
+
+enum send_load_type {
+	SEND_LD_TYPE_LDD = 0x00,
+	SEND_LD_TYPE_LDT = 0x01,
+	SEND_LD_TYPE_LDWB = 0x02,
+};
+
+enum send_mem_alg_type {
+	SEND_MEMALG_SET = 0x00,
+	SEND_MEMALG_ADD = 0x08,
+	SEND_MEMALG_SUB = 0x09,
+	SEND_MEMALG_ADDLEN = 0x0A,
+	SEND_MEMALG_SUBLEN = 0x0B,
+};
+
+enum send_mem_dsz_type {
+	SEND_MEMDSZ_B64 = 0x00,
+	SEND_MEMDSZ_B32 = 0x01,
+	SEND_MEMDSZ_B8 = 0x03,
+};
+
+enum sq_subdesc_type {
+	SQ_DESC_TYPE_INVALID = 0x00,
+	SQ_DESC_TYPE_HEADER = 0x01,
+	SQ_DESC_TYPE_CRC = 0x02,
+	SQ_DESC_TYPE_IMMEDIATE = 0x03,
+	SQ_DESC_TYPE_GATHER = 0x04,
+	SQ_DESC_TYPE_MEMORY = 0x05,
+};
+
+struct sq_crc_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t rsvd1:32;
+	uint64_t crc_ival:32;
+	uint64_t subdesc_type:4;
+	uint64_t crc_alg:2;
+	uint64_t rsvd0:10;
+	uint64_t crc_insert_pos:16;
+	uint64_t hdr_start:16;
+	uint64_t crc_len:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t crc_len:16;
+	uint64_t hdr_start:16;
+	uint64_t crc_insert_pos:16;
+	uint64_t rsvd0:10;
+	uint64_t crc_alg:2;
+	uint64_t subdesc_type:4;
+	uint64_t crc_ival:32;
+	uint64_t rsvd1:32;
+#endif
+};
+
+struct sq_gather_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t subdesc_type:4;	/* W0 */
+	uint64_t ld_type:2;
+	uint64_t rsvd0:42;
+	uint64_t size:16;
+
+	uint64_t rsvd1:15;	/* W1 */
+	uint64_t addr:49;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t size:16;
+	uint64_t rsvd0:42;
+	uint64_t ld_type:2;
+	uint64_t subdesc_type:4;	/* W0 */
+
+	uint64_t addr:49;
+	uint64_t rsvd1:15;	/* W1 */
+#endif
+};
+
+/* SQ immediate subdescriptor */
+struct sq_imm_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t subdesc_type:4;	/* W0 */
+	uint64_t rsvd0:46;
+	uint64_t len:14;
+
+	uint64_t data:64;	/* W1 */
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t len:14;
+	uint64_t rsvd0:46;
+	uint64_t subdesc_type:4;	/* W0 */
+
+	uint64_t data:64;	/* W1 */
+#endif
+};
+
+struct sq_mem_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t subdesc_type:4;	/* W0 */
+	uint64_t mem_alg:4;
+	uint64_t mem_dsz:2;
+	uint64_t wmem:1;
+	uint64_t rsvd0:21;
+	uint64_t offset:32;
+
+	uint64_t rsvd1:15;	/* W1 */
+	uint64_t addr:49;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t offset:32;
+	uint64_t rsvd0:21;
+	uint64_t wmem:1;
+	uint64_t mem_dsz:2;
+	uint64_t mem_alg:4;
+	uint64_t subdesc_type:4;	/* W0 */
+
+	uint64_t addr:49;
+	uint64_t rsvd1:15;	/* W1 */
+#endif
+};
+
+struct sq_hdr_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t subdesc_type:4;
+	uint64_t tso:1;
+	uint64_t post_cqe:1;	/* Post CQE on no error also */
+	uint64_t dont_send:1;
+	uint64_t tstmp:1;
+	uint64_t subdesc_cnt:8;
+	uint64_t csum_l4:2;
+	uint64_t csum_l3:1;
+	uint64_t rsvd0:5;
+	uint64_t l4_offset:8;
+	uint64_t l3_offset:8;
+	uint64_t rsvd1:4;
+	uint64_t tot_len:20;	/* W0 */
+
+	uint64_t tso_sdc_cont:8;
+	uint64_t tso_sdc_first:8;
+	uint64_t tso_l4_offset:8;
+	uint64_t tso_flags_last:12;
+	uint64_t tso_flags_first:12;
+	uint64_t rsvd2:2;
+	uint64_t tso_max_paysize:14;	/* W1 */
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t tot_len:20;
+	uint64_t rsvd1:4;
+	uint64_t l3_offset:8;
+	uint64_t l4_offset:8;
+	uint64_t rsvd0:5;
+	uint64_t csum_l3:1;
+	uint64_t csum_l4:2;
+	uint64_t subdesc_cnt:8;
+	uint64_t tstmp:1;
+	uint64_t dont_send:1;
+	uint64_t post_cqe:1;	/* Post CQE on no error also */
+	uint64_t tso:1;
+	uint64_t subdesc_type:4;	/* W0 */
+
+	uint64_t tso_max_paysize:14;
+	uint64_t rsvd2:2;
+	uint64_t tso_flags_first:12;
+	uint64_t tso_flags_last:12;
+	uint64_t tso_l4_offset:8;
+	uint64_t tso_sdc_first:8;
+	uint64_t tso_sdc_cont:8;	/* W1 */
+#endif
+};
+
+/* Queue config register formats */
+struct rq_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_2_63:62;
+	uint64_t ena:1;
+	uint64_t tcp_ena:1;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t tcp_ena:1;
+	uint64_t ena:1;
+	uint64_t reserved_2_63:62;
+#endif
+};
+
+struct cq_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_43_63:21;
+	uint64_t ena:1;
+	uint64_t reset:1;
+	uint64_t caching:1;
+	uint64_t reserved_35_39:5;
+	uint64_t qsize:3;
+	uint64_t reserved_25_31:7;
+	uint64_t avg_con:9;
+	uint64_t reserved_0_15:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t reserved_0_15:16;
+	uint64_t avg_con:9;
+	uint64_t reserved_25_31:7;
+	uint64_t qsize:3;
+	uint64_t reserved_35_39:5;
+	uint64_t caching:1;
+	uint64_t reset:1;
+	uint64_t ena:1;
+	uint64_t reserved_43_63:21;
+#endif
+};
+
+struct sq_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_20_63:44;
+	uint64_t ena:1;
+	uint64_t reserved_18_18:1;
+	uint64_t reset:1;
+	uint64_t ldwb:1;
+	uint64_t reserved_11_15:5;
+	uint64_t qsize:3;
+	uint64_t reserved_3_7:5;
+	uint64_t tstmp_bgx_intf:3;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t tstmp_bgx_intf:3;
+	uint64_t reserved_3_7:5;
+	uint64_t qsize:3;
+	uint64_t reserved_11_15:5;
+	uint64_t ldwb:1;
+	uint64_t reset:1;
+	uint64_t reserved_18_18:1;
+	uint64_t ena:1;
+	uint64_t reserved_20_63:44;
+#endif
+};
+
+struct rbdr_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_45_63:19;
+	uint64_t ena:1;
+	uint64_t reset:1;
+	uint64_t ldwb:1;
+	uint64_t reserved_36_41:6;
+	uint64_t qsize:4;
+	uint64_t reserved_25_31:7;
+	uint64_t avg_con:9;
+	uint64_t reserved_12_15:4;
+	uint64_t lines:12;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t lines:12;
+	uint64_t reserved_12_15:4;
+	uint64_t avg_con:9;
+	uint64_t reserved_25_31:7;
+	uint64_t qsize:4;
+	uint64_t reserved_36_41:6;
+	uint64_t ldwb:1;
+	uint64_t reset:1;
+	uint64_t ena:1;
+	uint64_t reserved_45_63:19;
+#endif
+};
+
+struct qs_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_32_63:32;
+	uint64_t ena:1;
+	uint64_t reserved_27_30:4;
+	uint64_t sq_ins_ena:1;
+	uint64_t sq_ins_pos:6;
+	uint64_t lock_ena:1;
+	uint64_t lock_viol_cqe_ena:1;
+	uint64_t send_tstmp_ena:1;
+	uint64_t be:1;
+	uint64_t reserved_7_15:9;
+	uint64_t vnic:7;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t vnic:7;
+	uint64_t reserved_7_15:9;
+	uint64_t be:1;
+	uint64_t send_tstmp_ena:1;
+	uint64_t lock_viol_cqe_ena:1;
+	uint64_t lock_ena:1;
+	uint64_t sq_ins_pos:6;
+	uint64_t sq_ins_ena:1;
+	uint64_t reserved_27_30:4;
+	uint64_t ena:1;
+	uint64_t reserved_32_63:32;
+#endif
+};
+
+#endif /* Q_STRUCT_H */
diff --git a/drivers/net/cavium/thunder_bgx.c b/drivers/net/cavium/thunder_bgx.c
new file mode 100644
index 0000000000..9ff1b4b58b
--- /dev/null
+++ b/drivers/net/cavium/thunder_bgx.c
@@ -0,0 +1,787 @@
+/**
+ * (C) Copyright 2014, Cavium Inc.
+ *
+ * SPDX-License-Identifier:	GPL-2.0+
+**/
+
+#include <config.h>
+#include <common.h>
+#include <net.h>
+#include <netdev.h>
+#include <malloc.h>
+#include <miiphy.h>
+#include <asm/io.h>
+
+#include <cavium/thunderx_smi.h>
+#include <cavium/thunderx_vnic.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "thunder_bgx.h"
+
+struct lmac {
+	struct bgx *bgx;
+	int dmac;
+	bool link_up;
+	int lmacid;		/* ID within BGX */
+	int lmacid_bd;		/* ID on board */
+	struct eth_device netdev;
+	struct mii_dev *mii_bus;
+	struct phy_device *phydev;
+	unsigned int last_duplex;
+	unsigned int last_link;
+	unsigned int last_speed;
+	bool is_sgmii;
+} lmac;
+
+struct bgx {
+	uint8_t bgx_id;
+	struct lmac lmac[MAX_LMAC_PER_BGX];
+	int lmac_count;
+	enum lmac_type lmac_type;
+	int lane_to_sds;
+	int use_training;
+	uint64_t reg_base;
+	struct pci_dev *pdev;
+
+	enum qlm_mode qlm_mode;
+} bgx;
+
+int lmac_count = 0;	/* Total no of LMACs in system */
+struct bgx *bgx_vnic[MAX_BGX_THUNDER];
+
+static int bgx_lmac_xaui_link_init(struct lmac *lmac);
+
+/* Register read/write APIs */
+static uint64_t bgx_reg_read(struct bgx *bgx, uint8_t lmac, uint64_t offset)
+{
+	uint64_t addr = bgx->reg_base + ((uint32_t) lmac << 20) + offset;
+
+	return readq((void *)addr);
+}
+
+static void bgx_reg_write(struct bgx *bgx, uint8_t lmac,
+			  uint64_t offset, uint64_t val)
+{
+	uint64_t addr = bgx->reg_base + ((uint32_t) lmac << 20) + offset;
+
+	writeq(val, (void *)addr);
+}
+
+static void bgx_reg_modify(struct bgx *bgx, uint8_t lmac,
+			   uint64_t offset, uint64_t val)
+{
+	uint64_t addr = bgx->reg_base + ((uint32_t) lmac << 20) + offset;
+
+	writeq(val | bgx_reg_read(bgx, lmac, offset), (void *)addr);
+}
+
+static int bgx_poll_reg(struct bgx *bgx, uint8_t lmac,
+			uint64_t reg, uint64_t mask, bool zero)
+{
+	int timeout = 200;
+	uint64_t reg_val;
+
+	while (timeout) {
+		reg_val = bgx_reg_read(bgx, lmac, reg);
+		if (zero && !(reg_val & mask))
+			return 0;
+		if (!zero && (reg_val & mask))
+			return 0;
+		mdelay(1);
+		timeout--;
+	}
+	return 1;
+}
+
+/* Return number of BGX present in HW */
+void bgx_get_count(int node, int *bgx_count)
+{
+	int i;
+	struct bgx *bgx;
+
+	*bgx_count = 0;
+	for (i = 0; i < MAX_BGX_PER_CN88XX; i++) {
+		bgx = bgx_vnic[node * MAX_BGX_PER_CN88XX + i];
+		debug("bgx_vnic[%u]: %p\n", node * MAX_BGX_PER_CN88XX + i, bgx);
+		if (bgx)
+			*bgx_count |= (1 << i);
+	}
+}
+
+/* Return number of LMAC configured for this BGX */
+int bgx_get_lmac_count(int node, int bgx_idx)
+{
+	struct bgx *bgx;
+
+	bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+	if (bgx)
+		return bgx->lmac_count;
+
+	return 0;
+}
+
+static void bgx_lmac_change_link_state(struct lmac *lmac)
+{
+	struct bgx *bgx = lmac->bgx;
+	uint64_t cmr_cfg;
+	uint64_t port_cfg = 0;
+	uint64_t misc_ctl = 0;
+
+	cmr_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_CMRX_CFG);
+	cmr_cfg &= ~CMR_EN;
+	bgx_reg_write(bgx, lmac->lmacid, BGX_CMRX_CFG, cmr_cfg);
+
+	port_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG);
+	misc_ctl = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_PCS_MISCX_CTL);
+
+	if (lmac->link_up) {
+		misc_ctl &= ~PCS_MISC_CTL_GMX_ENO;
+		port_cfg &= ~GMI_PORT_CFG_DUPLEX;
+		port_cfg |= (lmac->last_duplex << 2);
+	} else {
+		misc_ctl |= PCS_MISC_CTL_GMX_ENO;
+	}
+
+	switch (lmac->last_speed) {
+	case 10:
+		port_cfg &= ~GMI_PORT_CFG_SPEED;	/* speed 0 */
+		port_cfg |= GMI_PORT_CFG_SPEED_MSB;	/* speed_msb 1 */
+		port_cfg &= ~GMI_PORT_CFG_SLOT_TIME;	/* slottime 0 */
+		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
+		misc_ctl |= 50;	/* samp_pt */
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 64);
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_BURST, 0);
+		break;
+	case 100:
+		port_cfg &= ~GMI_PORT_CFG_SPEED;	/* speed 0 */
+		port_cfg &= ~GMI_PORT_CFG_SPEED_MSB;	/* speed_msb 0 */
+		port_cfg &= ~GMI_PORT_CFG_SLOT_TIME;	/* slottime 0 */
+		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
+		misc_ctl |= 5;	/* samp_pt */
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 64);
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_BURST, 0);
+		break;
+	case 1000:
+		port_cfg |= GMI_PORT_CFG_SPEED;	/* speed 1 */
+		port_cfg &= ~GMI_PORT_CFG_SPEED_MSB;	/* speed_msb 0 */
+		port_cfg |= GMI_PORT_CFG_SLOT_TIME;	/* slottime 1 */
+		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
+		misc_ctl |= 1;	/* samp_pt */
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 512);
+		if (lmac->last_duplex)
+			bgx_reg_write(bgx, lmac->lmacid,
+				      BGX_GMP_GMI_TXX_BURST, 0);
+		else
+			bgx_reg_write(bgx, lmac->lmacid,
+				      BGX_GMP_GMI_TXX_BURST, 8192);
+		break;
+	default:
+		break;
+	}
+	bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_PCS_MISCX_CTL, misc_ctl);
+	bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG, port_cfg);
+
+	port_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG);
+
+	/* renable lmac */
+	cmr_cfg |= CMR_EN;
+	bgx_reg_write(bgx, lmac->lmacid, BGX_CMRX_CFG, cmr_cfg);
+}
+
+static void bgx_lmac_handler(struct eth_device *netdev)
+{
+	struct lmac *lmac = container_of(netdev, struct lmac, netdev);
+	struct phy_device *phydev = lmac->phydev;
+	int link_changed = 0;
+
+	if (!lmac)
+		return;
+
+	if (!phydev->link && lmac->last_link)
+		link_changed = -1;
+
+	if (phydev->link &&
+	    (lmac->last_duplex != phydev->duplex ||
+	     lmac->last_link != phydev->link ||
+	     lmac->last_speed != phydev->speed)) {
+		link_changed = 1;
+	}
+
+	lmac->last_link = phydev->link;
+	lmac->last_speed = phydev->speed;
+	lmac->last_duplex = phydev->duplex;
+
+	if (!link_changed)
+		return;
+
+	if (link_changed > 0) {
+		printf("LMAC%d: Link is up - %d/%s\n", lmac->lmacid_bd,
+		       phydev->speed,
+		       DUPLEX_FULL == phydev->duplex ? "Full" : "Half");
+		lmac->link_up = true;
+	} else {
+		lmac->link_up = false;
+		printf("LMAC%d: Link is down\n", lmac->lmacid_bd);
+	}
+
+	if (lmac->is_sgmii) {
+		bgx_lmac_change_link_state(lmac);
+	} else {
+		if (!lmac->link_up)
+			bgx_lmac_xaui_link_init(lmac);
+	}
+}
+
+static void bgx_flush_dmac_addrs(struct bgx *bgx, uint64_t lmac)
+{
+	uint64_t dmac = 0x00;
+	uint64_t offset, addr;
+
+	while (bgx->lmac[lmac].dmac > 0) {
+		offset = ((bgx->lmac[lmac].dmac - 1) * sizeof(dmac)) +
+		    (lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
+		addr = bgx->reg_base + BGX_CMR_RX_DMACX_CAM + offset;
+		writeq(dmac, (void *)addr);
+		bgx->lmac[lmac].dmac--;
+	}
+}
+
+static int bgx_lmac_sgmii_init(struct bgx *bgx, int lmacid)
+{
+	uint64_t cfg;
+
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_THRESH, 0x30);
+	/* max packet size */
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_RXX_JABBER, 9216);
+
+	/* Disable frame alignment if using preamble */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND);
+
+	if (cfg & 1)
+		bgx_reg_write(bgx, lmacid, BGX_GMP_GMI_TXX_SGMII_CTL, 0);
+	/* Enable lmac */
+	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG, CMR_EN);
+	/* PCS reset */
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_PCS_MRX_CTL, PCS_MRX_CTL_RESET);
+	if (bgx_poll_reg(bgx, lmacid, BGX_GMP_PCS_MRX_CTL,
+			 PCS_MRX_CTL_RESET, true)) {
+		printf("BGX PCS reset not completed\n");
+		return -1;
+	}
+
+	/* power down, reset autoneg, autoneg enable */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_PCS_MRX_CTL);
+	cfg &= ~PCS_MRX_CTL_PWR_DN;
+	cfg |= (PCS_MRX_CTL_RST_AN | PCS_MRX_CTL_AN_EN);
+
+	bgx_reg_write(bgx, lmacid, BGX_GMP_PCS_MRX_CTL, cfg);
+	if (bgx_poll_reg(bgx, lmacid, BGX_GMP_PCS_MRX_STATUS,
+			 PCS_MRX_STATUS_AN_CPT, false)) {
+		printf("BGX AN_CPT not completed\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+static int bgx_lmac_xaui_init(struct bgx *bgx, int lmacid, int lmac_type)
+{
+	uint64_t cfg;
+
+	/* Reset SPU */
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET);
+	if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET, true)) {
+		printf("BGX SPU reset not completed\n");
+		return -1;
+	}
+
+	/* Disable LMAC */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_CFG);
+	cfg &= ~CMR_EN;
+	bgx_reg_write(bgx, lmacid, BGX_CMRX_CFG, cfg);
+
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_LOW_POWER);
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
+
+	/* clear all interrupts */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_RX_INT);
+	bgx_reg_write(bgx, lmacid, BGX_SMUX_RX_INT, cfg);
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_INT);
+	bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_INT, cfg);
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_INT);
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_INT, cfg);
+
+	if (bgx->use_training) {
+		bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_LP_CUP, 0x00);
+		bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_LD_CUP, 0x00);
+		bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_LD_REP, 0x00);
+		/* training enable */
+		bgx_reg_modify(bgx, lmacid,
+			       BGX_SPUX_BR_PMD_CRTL, SPU_PMD_CRTL_TRAIN_EN);
+	}
+
+	/* Append FCS to each packet */
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_APPEND, SMU_TX_APPEND_FCS_D);
+
+	/* Disable forward error correction */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_FEC_CONTROL);
+	cfg &= ~SPU_FEC_CTL_FEC_EN;
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_FEC_CONTROL, cfg);
+
+	/* Disable autoneg */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_AN_CONTROL);
+	cfg = cfg & ~(SPU_AN_CTL_AN_EN | SPU_AN_CTL_XNP_EN);
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_AN_CONTROL, cfg);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_AN_ADV);
+	if (bgx->lmac_type == BGX_MODE_10G_KR)
+		cfg |= (1 << 23);
+	else if (bgx->lmac_type == BGX_MODE_40G_KR)
+		cfg |= (1 << 24);
+	else
+		cfg &= ~((1 << 23) | (1 << 24));
+	cfg = cfg & (~((1ULL << 25) | (1ULL << 22) | (1ULL << 12)));
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_AN_ADV, cfg);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPU_DBG_CONTROL);
+	cfg &= ~SPU_DBG_CTL_AN_ARB_LINK_CHK_EN;
+	bgx_reg_write(bgx, lmacid, BGX_SPU_DBG_CONTROL, cfg);
+
+	/* Enable lmac */
+	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG, CMR_EN);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_CONTROL1);
+	cfg &= ~SPU_CTL_LOW_POWER;
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_CONTROL1, cfg);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_CTL);
+	cfg &= ~SMU_TX_CTL_UNI_EN;
+	cfg |= SMU_TX_CTL_DIC_EN;
+	bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_CTL, cfg);
+
+	/* take lmac_count into account */
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_THRESH, (0x100 - 1));
+	/* max packet size */
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_RX_JABBER, 9216);
+
+	return 0;
+}
+
+static int bgx_lmac_xaui_link_init(struct lmac *lmac)
+{
+	struct bgx *bgx = lmac->bgx;
+	uint64_t cfg;
+	int lmacid = lmac->lmacid;
+	int tx_link_ok, rx_link_ok, rcv_link;
+	int lmac_type = bgx->lmac_type;
+
+	tx_link_ok = bgx_reg_read(bgx, lmacid,
+				  BGX_SMUX_TX_CTL) & SMU_TX_CTL_LNK_STATUS;
+	rx_link_ok = bgx_reg_read(bgx, lmacid,
+				  BGX_SMUX_RX_CTL) & SMU_RX_CTL_STATUS;
+	rcv_link = bgx_reg_read(bgx, lmacid,
+				BGX_SPUX_STATUS1) & SPU_STATUS1_RCV_LNK;
+
+	if ((tx_link_ok == 0) && (rx_link_ok == 0) && rcv_link)
+		return 0;
+
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
+	if (bgx->use_training) {
+		cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_INT);
+		if (!(cfg & (1ull << 13))) {
+			cfg = (1ull << 13) | (1ull << 14);
+			bgx_reg_write(bgx, lmacid, BGX_SPUX_INT, cfg);
+			cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_BR_PMD_CRTL);
+			cfg |= (1ull << 0);
+			bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_CRTL, cfg);
+			return -1;
+		}
+	}
+
+	/* wait for PCS to come out of reset */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET, true)) {
+		printf("BGX SPU reset not completed\n");
+		return -1;
+	}
+
+	if ((lmac_type == BGX_MODE_10G_KR) || (lmac_type == BGX_MODE_XFI) ||
+	    (lmac_type == BGX_MODE_40G_KR) || (lmac_type == BGX_MODE_XLAUI)) {
+		if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_BR_STATUS1,
+				 SPU_BR_STATUS_BLK_LOCK, false)) {
+			printf("SPU_BR_STATUS_BLK_LOCK not completed\n");
+			return -1;
+		}
+	} else {
+		if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_BX_STATUS,
+				 SPU_BX_STATUS_RX_ALIGN, false)) {
+			printf("SPU_BX_STATUS_RX_ALIGN not completed\n");
+			return -1;
+		}
+	}
+
+	/* Clear rcvflt bit (latching high) and read it back */
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_STATUS2, SPU_STATUS2_RCVFLT);
+	if (bgx_reg_read(bgx, lmacid, BGX_SPUX_STATUS2) & SPU_STATUS2_RCVFLT) {
+		printf("Receive fault, retry training\n");
+		if (bgx->use_training) {
+			cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_INT);
+			if (!(cfg & (1ull << 13))) {
+				cfg = (1ull << 13) | (1ull << 14);
+				bgx_reg_write(bgx, lmacid, BGX_SPUX_INT, cfg);
+				cfg = bgx_reg_read(bgx, lmacid,
+						   BGX_SPUX_BR_PMD_CRTL);
+				cfg |= (1ull << 0);
+				bgx_reg_write(bgx, lmacid,
+					      BGX_SPUX_BR_PMD_CRTL, cfg);
+				return -1;
+			}
+		}
+		return -1;
+	}
+
+	/* Wait for MAC RX to be ready */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_RX_CTL,
+			 SMU_RX_CTL_STATUS, true)) {
+		printf("SMU RX link not okay\n");
+		return -1;
+	}
+
+	/* Wait for BGX RX to be idle */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL,
+			 SMU_CTL_RX_IDLE, false)) {
+		printf("SMU RX not idle\n");
+		return -1;
+	}
+
+	/* Wait for BGX TX to be idle */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL,
+			 SMU_CTL_TX_IDLE, false)) {
+		printf("SMU TX not idle\n");
+		return -1;
+	}
+
+	if (bgx_reg_read(bgx, lmacid, BGX_SPUX_STATUS2) & SPU_STATUS2_RCVFLT) {
+		printf("Receive fault\n");
+		return -1;
+	}
+
+	/* Receive link is latching low. Force it high and verify it */
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_STATUS1, SPU_STATUS1_RCV_LNK);
+	if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_STATUS1,
+			 SPU_STATUS1_RCV_LNK, false)) {
+		printf("SPU receive link down\n");
+		return -1;
+	}
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_MISC_CONTROL);
+	cfg &= ~SPU_MISC_CTL_RX_DIS;
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_MISC_CONTROL, cfg);
+	return 0;
+}
+
+int bgx_lmac_enable(int bgxid, int8_t lmacid)
+{
+	struct lmac *lmac;
+	struct bgx *bgx = bgx_vnic[bgxid];
+	uint64_t cfg;
+	int ret;
+
+	lmac = &bgx->lmac[lmacid];
+	lmac->bgx = bgx;
+
+	debug("lmac: %p", lmac);
+
+	if (bgx->lmac_type == BGX_MODE_SGMII) {
+		lmac->is_sgmii = 1;
+		if (bgx_lmac_sgmii_init(bgx, lmacid))
+			return -1;
+	} else {
+		lmac->is_sgmii = 0;
+		if (bgx_lmac_xaui_init(bgx, lmacid, bgx->lmac_type))
+			return -1;
+	}
+
+	if (lmac->is_sgmii) {
+		cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND);
+		cfg |= ((1ull << 2) | (1ull << 1));	/* FCS and PAD */
+		bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND, cfg);
+		bgx_reg_write(bgx, lmacid, BGX_GMP_GMI_TXX_MIN_PKT, 60 - 1);
+	} else {
+		cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_APPEND);
+		cfg |= ((1ull << 2) | (1ull << 1));	/* FCS and PAD */
+		bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_APPEND, cfg);
+		bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_MIN_PKT, 60 + 4);
+	}
+
+	/* Enable lmac */
+	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG,
+		       CMR_EN | CMR_PKT_RX_EN | CMR_PKT_TX_EN);
+
+	if (!lmac->is_sgmii) {
+		printf("%s: Only SGMII is implemented currently\n", __func__);
+		return 1;
+	}
+
+	lmac->phydev = phy_connect(lmac->mii_bus, lmac->lmacid,
+				   &lmac->netdev, PHY_INTERFACE_MODE_SGMII);
+
+	if (!lmac->phydev)
+		return -1;
+
+	ret = phy_config(lmac->phydev);
+	if (ret) {
+		printf("%s: Could not initialize PHY %s\n",
+		       lmac->netdev.name, lmac->phydev->dev->name);
+		return ret;
+	}
+
+	ret = phy_startup(lmac->phydev);
+	if (ret) {
+		printf("%s: Could not initialize PHY %s\n",
+		       lmac->netdev.name, lmac->phydev->dev->name);
+		return ret;
+	}
+
+	bgx_lmac_handler(&lmac->netdev);
+
+	return 0;
+}
+
+void bgx_lmac_disable(int bgxid, uint8_t lmacid)
+{
+	struct lmac *lmac;
+	struct bgx *bgx = bgx_vnic[bgxid];
+	uint64_t cmrx_cfg;
+
+	lmac = &bgx->lmac[lmacid];
+
+	cmrx_cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_CFG);
+	cmrx_cfg &= ~(1 << 15);
+	bgx_reg_write(bgx, lmacid, BGX_CMRX_CFG, cmrx_cfg);
+	bgx_flush_dmac_addrs(bgx, lmacid);
+
+	if (lmac->phydev)
+		phy_shutdown(lmac->phydev);
+
+	lmac->phydev = NULL;
+}
+
+static int bgx_lmac_count(struct bgx *bgx)
+{
+	const char *boardtype = getenv("board");
+	int lmac_count = 0;
+
+	if (!boardtype)
+		return lmac_count;
+
+	if (!strcmp(boardtype, "ebb8800")) {
+		switch (bgx->qlm_mode) {
+		case QLM_MODE_SGMII:
+		case QLM_MODE_XFI_4X1:
+		case QLM_MODE_10G_KR_4X1:
+			lmac_count = 4;
+		break;
+		case QLM_MODE_XAUI_1X4:
+		case QLM_MODE_XLAUI_1X4:
+		case QLM_MODE_40G_KR4_1X4:
+			lmac_count = 1;
+		break;
+		case QLM_MODE_RXAUI_2X2:
+			lmac_count = 2;
+		break;
+		}
+	} else if (!strcmp(boardtype, "crb_1s")) {
+		switch (bgx->bgx_id) {
+		case 0:
+			lmac_count = 1;
+			break;
+		case 1:
+			lmac_count = 2;
+			break;
+		}
+	} else if (!strcmp(boardtype, "crb_2s")) {
+		lmac_count = 1;
+	} else {
+		printf("Unsupported board\n");
+		lmac_count = 0;
+	}
+
+	return lmac_count;
+}
+
+static void bgx_set_num_ports(struct bgx *bgx)
+{
+	switch (bgx->qlm_mode) {
+	case QLM_MODE_SGMII:
+		bgx->lmac_type = BGX_MODE_SGMII;
+		bgx->lane_to_sds = 0;
+		break;
+	case QLM_MODE_XAUI_1X4:
+		bgx->lmac_type = BGX_MODE_XAUI;
+		bgx->lane_to_sds = 0xE4;
+		break;
+	case QLM_MODE_RXAUI_2X2:
+		bgx->lmac_type = BGX_MODE_RXAUI;
+		bgx->lane_to_sds = 0xE4;
+		break;
+	case QLM_MODE_XFI_4X1:
+		bgx->lmac_type = BGX_MODE_XFI;
+		bgx->lane_to_sds = 0;
+		break;
+	case QLM_MODE_XLAUI_1X4:
+		bgx->lmac_type = BGX_MODE_XLAUI;
+		bgx->lane_to_sds = 0xE4;
+		break;
+	case QLM_MODE_10G_KR_4X1:
+		bgx->lmac_type = BGX_MODE_10G_KR;
+		bgx->lane_to_sds = 0;
+		bgx->use_training = 1;
+		break;
+	case QLM_MODE_40G_KR4_1X4:
+		bgx->lmac_type = BGX_MODE_40G_KR;
+		bgx->lane_to_sds = 0xE4;
+		bgx->use_training = 1;
+		break;
+	default:
+		break;
+	}
+
+	bgx->lmac_count = bgx_lmac_count(bgx);
+
+	printf("BGX%d LMACs: %d\n", bgx->bgx_id, bgx->lmac_count);
+}
+
+/* Most of this will eventually need to be set by bootloader */
+static void bgx_init_hw(struct bgx *bgx)
+{
+	int i;
+
+	bgx_set_num_ports(bgx);
+
+	bgx_reg_modify(bgx, 0, BGX_CMR_GLOBAL_CFG, CMR_GLOBAL_CFG_FCS_STRIP);
+	if (bgx_reg_read(bgx, 0, BGX_CMR_BIST_STATUS))
+		printf("BGX%d BIST failed\n", bgx->bgx_id);
+
+	/* Set lmac type and lane2serdes mapping */
+	for (i = 0; i < bgx->lmac_count; i++) {
+		if (bgx->lmac_type == BGX_MODE_RXAUI) {
+			if (i)
+				bgx->lane_to_sds = 0x0e;
+			else
+				bgx->lane_to_sds = 0x04;
+			bgx_reg_write(bgx, i, BGX_CMRX_CFG,
+				      (bgx->lmac_type << 8) | bgx->lane_to_sds);
+			continue;
+		}
+		bgx_reg_write(bgx, i, BGX_CMRX_CFG,
+			      (bgx->lmac_type << 8) | (bgx->lane_to_sds + i));
+		bgx->lmac[i].lmacid_bd = lmac_count;
+		lmac_count++;
+	}
+
+	bgx_reg_write(bgx, 0, BGX_CMR_TX_LMACS, bgx->lmac_count);
+	bgx_reg_write(bgx, 0, BGX_CMR_RX_LMACS, bgx->lmac_count);
+
+	/* Set the backpressure AND mask */
+	for (i = 0; i < bgx->lmac_count; i++)
+		bgx_reg_modify(bgx, 0, BGX_CMR_CHAN_MSK_AND,
+			       ((1ULL << MAX_BGX_CHANS_PER_LMAC) - 1) <<
+			       (i * MAX_BGX_CHANS_PER_LMAC));
+
+	/* Disable all MAC filtering */
+	for (i = 0; i < RX_DMAC_COUNT; i++)
+		bgx_reg_write(bgx, 0, BGX_CMR_RX_DMACX_CAM + (i * 8), 0x00);
+
+	/* Disable MAC steering (NCSI traffic) */
+	for (i = 0; i < RX_TRAFFIC_STEER_RULE_COUNT; i++)
+		bgx_reg_write(bgx, 0, BGX_CMR_RX_STREERING + (i * 8), 0x00);
+}
+
+static void bgx_get_qlm_mode(struct bgx *bgx)
+{
+	int lmac_type;
+	int train_en;
+
+	/* Read LMAC0 type to figure out QLM mode
+	 * This is configured by low level firmware
+	 **/
+	lmac_type = bgx_reg_read(bgx, 0, BGX_CMRX_CFG);
+	lmac_type = (lmac_type >> 8) & 0x07;
+
+	train_en = bgx_reg_read(bgx, 0, BGX_SPUX_BR_PMD_CRTL) &
+	    SPU_PMD_CRTL_TRAIN_EN;
+
+	switch (lmac_type) {
+	case BGX_MODE_SGMII:
+		bgx->qlm_mode = QLM_MODE_SGMII;
+		printf("BGX%d QLM mode: SGMII\n", bgx->bgx_id);
+		break;
+	case BGX_MODE_XAUI:
+		bgx->qlm_mode = QLM_MODE_XAUI_1X4;
+		printf("BGX%d QLM mode: XAUI\n", bgx->bgx_id);
+		break;
+	case BGX_MODE_RXAUI:
+		bgx->qlm_mode = QLM_MODE_RXAUI_2X2;
+		printf("BGX%d QLM mode: RXAUI\n", bgx->bgx_id);
+		break;
+	case BGX_MODE_XFI:
+		if (!train_en) {
+			bgx->qlm_mode = QLM_MODE_XFI_4X1;
+			printf("BGX%d QLM mode: XFI\n", bgx->bgx_id);
+		} else {
+			bgx->qlm_mode = QLM_MODE_10G_KR_4X1;
+			printf("BGX%d QLM mode: 10G_KR\n", bgx->bgx_id);
+		}
+		break;
+	case BGX_MODE_XLAUI:
+		if (!train_en) {
+			bgx->qlm_mode = QLM_MODE_XLAUI_1X4;
+			printf("BGX%d QLM mode: XLAUI\n", bgx->bgx_id);
+		} else {
+			bgx->qlm_mode = QLM_MODE_40G_KR4_1X4;
+			printf("BGX%d QLM mode: 40G_KR4\n", bgx->bgx_id);
+		}
+		break;
+	default:
+		bgx->qlm_mode = QLM_MODE_SGMII;
+		printf("BGX%d QLM default mode: SGMII\n", bgx->bgx_id);
+	}
+}
+
+int thunderx_bgx_initialize(unsigned int bgx_idx,
+			    unsigned int smi_idx, unsigned int node)
+{
+	int err;
+	struct bgx *bgx = NULL;
+	uint8_t lmac = 0;
+	char mii_name[10];
+
+	bgx = malloc(sizeof(struct bgx));
+
+	/* MAP configuration registers */
+	bgx->reg_base = CSR_PA(BGXX_PF_BAR0(bgx_idx), node);
+	bgx->bgx_id = bgx_idx + node * MAX_BGX_PER_CN88XX;
+
+	bgx_vnic[bgx->bgx_id] = bgx;
+	debug("bgx_vnic[%u]: %p\n", bgx->bgx_id, bgx);
+
+	snprintf(mii_name, sizeof(mii_name), "thunderx%d", smi_idx);
+
+	for (lmac = 0; lmac < 4; lmac++) {
+		bgx->lmac[lmac].lmacid_bd = bgx->bgx_id;
+		bgx->lmac[lmac].lmacid = lmac + bgx->bgx_id * 4;
+		bgx->lmac[lmac].mii_bus = miiphy_get_dev_by_name(mii_name);
+
+		bgx->lmac_count++;
+	}
+
+	bgx_get_qlm_mode(bgx);
+
+	bgx_init_hw(bgx);
+
+	return 0;
+
+	free(bgx);
+	return err;
+}
diff --git a/drivers/net/cavium/thunder_bgx.h b/drivers/net/cavium/thunder_bgx.h
new file mode 100644
index 0000000000..4e520051d3
--- /dev/null
+++ b/drivers/net/cavium/thunder_bgx.h
@@ -0,0 +1,222 @@
+/*
+ * Copyright (C) 2014 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
+
+#ifndef THUNDER_BGX_H
+#define THUNDER_BGX_H
+
+#define MAX_BGX_THUNDER		8	/* Max 4 nodes, 2 per node */
+#define MAX_BGX_PER_CN88XX		2
+#define MAX_LMAC_PER_BGX		4
+#define MAX_BGX_CHANS_PER_LMAC	16
+#define MAX_DMAC_PER_LMAC		8
+
+#define MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE	2
+
+#define MAX_LMAC	(MAX_BGX_PER_CN88XX * MAX_LMAC_PER_BGX)
+
+#define NODE_ID_MASK	0x300000000000
+#define NODE_ID(x)	((x & NODE_ID_MASK) >> 44)
+
+/* Registers */
+#define BGX_CMRX_CFG			0x00
+#define CMR_PKT_TX_EN			(1ull << 13)
+#define CMR_PKT_RX_EN			(1ull << 14)
+#define CMR_EN				(1ull << 15)
+#define BGX_CMR_GLOBAL_CFG		0x08
+#define CMR_GLOBAL_CFG_FCS_STRIP	(1ull << 6)
+#define BGX_CMRX_RX_ID_MAP		0x60
+#define BGX_CMRX_RX_STAT0		0x70
+#define BGX_CMRX_RX_STAT1		0x78
+#define BGX_CMRX_RX_STAT2		0x80
+#define BGX_CMRX_RX_STAT3		0x88
+#define BGX_CMRX_RX_STAT4		0x90
+#define BGX_CMRX_RX_STAT5		0x98
+#define BGX_CMRX_RX_STAT6		0xA0
+#define BGX_CMRX_RX_STAT7		0xA8
+#define BGX_CMRX_RX_STAT8		0xB0
+#define BGX_CMRX_RX_STAT9		0xB8
+#define BGX_CMRX_RX_STAT10		0xC0
+#define BGX_CMRX_RX_BP_DROP		0xC8
+#define BGX_CMRX_RX_DMAC_CTL		0x0E8
+#define BGX_CMR_RX_DMACX_CAM		0x200
+#define RX_DMACX_CAM_EN			(1ull << 48)
+#define RX_DMACX_CAM_LMACID(x)		(x << 49)
+#define RX_DMAC_COUNT			32
+#define BGX_CMR_RX_STREERING		0x300
+#define RX_TRAFFIC_STEER_RULE_COUNT	8
+#define BGX_CMR_CHAN_MSK_AND		0x450
+#define BGX_CMR_BIST_STATUS		0x460
+#define BGX_CMR_RX_LMACS		0x468
+#define BGX_CMRX_TX_STAT0		0x600
+#define BGX_CMRX_TX_STAT1		0x608
+#define BGX_CMRX_TX_STAT2		0x610
+#define BGX_CMRX_TX_STAT3		0x618
+#define BGX_CMRX_TX_STAT4		0x620
+#define BGX_CMRX_TX_STAT5		0x628
+#define BGX_CMRX_TX_STAT6		0x630
+#define BGX_CMRX_TX_STAT7		0x638
+#define BGX_CMRX_TX_STAT8		0x640
+#define BGX_CMRX_TX_STAT9		0x648
+#define BGX_CMRX_TX_STAT10		0x650
+#define BGX_CMRX_TX_STAT11		0x658
+#define BGX_CMRX_TX_STAT12		0x660
+#define BGX_CMRX_TX_STAT13		0x668
+#define BGX_CMRX_TX_STAT14		0x670
+#define BGX_CMRX_TX_STAT15		0x678
+#define BGX_CMRX_TX_STAT16		0x680
+#define BGX_CMRX_TX_STAT17		0x688
+#define BGX_CMR_TX_LMACS		0x1000
+
+#define BGX_SPUX_CONTROL1		0x10000
+#define SPU_CTL_LOW_POWER		(1ull << 11)
+#define SPU_CTL_RESET			(1ull << 15)
+#define BGX_SPUX_STATUS1		0x10008
+#define SPU_STATUS1_RCV_LNK		(1ull << 2)
+#define BGX_SPUX_STATUS2		0x10020
+#define SPU_STATUS2_RCVFLT		(1ull << 10)
+#define BGX_SPUX_BX_STATUS		0x10028
+#define SPU_BX_STATUS_RX_ALIGN		(1ull << 12)
+#define BGX_SPUX_BR_STATUS1		0x10030
+#define SPU_BR_STATUS_BLK_LOCK		(1ull << 0)
+#define SPU_BR_STATUS_RCV_LNK		(1ull << 12)
+#define BGX_SPUX_BR_PMD_CRTL		0x10068
+#define SPU_PMD_CRTL_TRAIN_EN		(1ull << 1)
+#define BGX_SPUX_BR_PMD_LP_CUP		0x10078
+#define BGX_SPUX_BR_PMD_LD_CUP		0x10088
+#define BGX_SPUX_BR_PMD_LD_REP		0x10090
+#define BGX_SPUX_FEC_CONTROL		0x100A0
+#define SPU_FEC_CTL_FEC_EN		(1ull << 0)
+#define SPU_FEC_CTL_ERR_EN		(1ull << 1)
+#define BGX_SPUX_AN_CONTROL		0x100C8
+#define SPU_AN_CTL_AN_EN		(1ull << 12)
+#define SPU_AN_CTL_XNP_EN		(1ull << 13)
+#define BGX_SPUX_AN_ADV			0x100D8
+#define BGX_SPUX_MISC_CONTROL		0x10218
+#define SPU_MISC_CTL_RX_DIS		(1ull << 12)
+#define BGX_SPUX_INT			0x10220	/* +(0..3) << 20 */
+#define BGX_SPUX_INT_W1S		0x10228
+#define BGX_SPUX_INT_ENA_W1C		0x10230
+#define BGX_SPUX_INT_ENA_W1S		0x10238
+#define BGX_SPU_DBG_CONTROL		0x10300
+#define SPU_DBG_CTL_AN_ARB_LINK_CHK_EN	(1ull << 18)
+#define SPU_DBG_CTL_AN_NONCE_MCT_DIS	(1ull << 29)
+
+#define BGX_SMUX_RX_INT			0x20000
+#define BGX_SMUX_RX_JABBER		0x20030
+#define BGX_SMUX_RX_CTL			0x20048
+#define SMU_RX_CTL_STATUS		(3ull << 0)
+#define BGX_SMUX_TX_APPEND		0x20100
+#define SMU_TX_APPEND_FCS_D		(1ull << 2)
+#define BGX_SMUX_TX_MIN_PKT		0x20118
+#define BGX_SMUX_TX_INT			0x20140
+#define BGX_SMUX_TX_CTL			0x20178
+#define SMU_TX_CTL_DIC_EN		(1ull << 0)
+#define SMU_TX_CTL_UNI_EN		(1ull << 1)
+#define SMU_TX_CTL_LNK_STATUS		(3ull << 4)
+#define BGX_SMUX_TX_THRESH		0x20180
+#define BGX_SMUX_CTL			0x20200
+#define SMU_CTL_RX_IDLE			(1ull << 0)
+#define SMU_CTL_TX_IDLE			(1ull << 1)
+
+#define BGX_GMP_PCS_MRX_CTL		0x30000
+#define	PCS_MRX_CTL_RST_AN		(1ull << 9)
+#define	PCS_MRX_CTL_PWR_DN		(1ull << 11)
+#define	PCS_MRX_CTL_AN_EN		(1ull << 12)
+#define	PCS_MRX_CTL_RESET		(1ull << 15)
+#define BGX_GMP_PCS_MRX_STATUS		0x30008
+#define	PCS_MRX_STATUS_AN_CPT		(1ull << 5)
+#define BGX_GMP_PCS_ANX_AN_RESULTS	0x30020
+#define BGX_GMP_PCS_SGM_AN_ADV		0x30068
+#define BGX_GMP_PCS_MISCX_CTL		0x30078
+#define PCS_MISC_CTL_GMX_ENO		(1ull << 11)
+#define PCS_MISC_CTL_SAMP_PT_MASK	0x7Full
+#define BGX_GMP_GMI_PRTX_CFG		0x38020
+#define GMI_PORT_CFG_SPEED		(1ull << 1)
+#define GMI_PORT_CFG_DUPLEX		(1ull << 2)
+#define GMI_PORT_CFG_SLOT_TIME		(1ull << 3)
+#define GMI_PORT_CFG_SPEED_MSB		(1ull << 8)
+#define BGX_GMP_GMI_RXX_JABBER		0x38038
+#define BGX_GMP_GMI_TXX_THRESH		0x38210
+#define BGX_GMP_GMI_TXX_APPEND		0x38218
+#define BGX_GMP_GMI_TXX_SLOT		0x38220
+#define BGX_GMP_GMI_TXX_BURST		0x38228
+#define BGX_GMP_GMI_TXX_MIN_PKT		0x38240
+#define BGX_GMP_GMI_TXX_SGMII_CTL	0x38300
+
+#define BGX_MSIX_VEC_0_29_ADDR		0x400000	/* +(0..29) << 4 */
+#define BGX_MSIX_VEC_0_29_CTL		0x400008
+#define BGX_MSIX_PBA_0			0x4F0000
+
+/* MSI-X interrupts */
+#define BGX_MSIX_VECTORS	30
+#define BGX_LMAC_VEC_OFFSET	7
+#define BGX_MSIX_VEC_SHIFT	4
+
+#define CMRX_INT		0
+#define SPUX_INT		1
+#define SMUX_RX_INT		2
+#define SMUX_TX_INT		3
+#define GMPX_PCS_INT		4
+#define GMPX_GMI_RX_INT		5
+#define GMPX_GMI_TX_INT		6
+#define CMR_MEM_INT		28
+#define SPU_MEM_INT		29
+
+#define LMAC_INTR_LINK_UP	(1 << 0)
+#define LMAC_INTR_LINK_DOWN	(1 << 1)
+
+/*  RX_DMAC_CTL configuration*/
+enum MCAST_MODE {
+	MCAST_MODE_REJECT,
+	MCAST_MODE_ACCEPT,
+	MCAST_MODE_CAM_FILTER,
+	RSVD
+};
+
+#define BCAST_ACCEPT	1
+#define CAM_ACCEPT	1
+
+void bgx_get_count(int node, int *bgx_count);
+int bgx_get_lmac_count(int node, int bgx);
+
+int bgx_lmac_enable(int bgxid, int8_t lmacid);
+void bgx_lmac_disable(int bgxid, uint8_t lmacid);
+
+#undef LINK_INTR_ENABLE
+
+enum lmac_type {
+	BGX_MODE_SGMII = 0,	/* 1 lane, 1.250 Gbaud */
+	BGX_MODE_XAUI = 1,	/* 4 lanes, 3.125 Gbaud */
+	BGX_MODE_DXAUI = 1,	/* 4 lanes, 6.250 Gbaud */
+	BGX_MODE_RXAUI = 2,	/* 2 lanes, 6.250 Gbaud */
+	BGX_MODE_XFI = 3,	/* 1 lane, 10.3125 Gbaud */
+	BGX_MODE_XLAUI = 4,	/* 4 lanes, 10.3125 Gbaud */
+	BGX_MODE_10G_KR = 3,	/* 1 lane, 10.3125 Gbaud */
+	BGX_MODE_40G_KR = 4,	/* 4 lanes, 10.3125 Gbaud */
+};
+
+enum qlm_mode {
+	QLM_MODE_SGMII,		/* SGMII, each lane independent */
+	QLM_MODE_XAUI_1X4,	/* 1 XAUI or DXAUI, 4 lanes */
+	QLM_MODE_RXAUI_2X2,	/* 2 RXAUI, 2 lanes each */
+	QLM_MODE_XFI_4X1,	/* 4 XFI, 1 lane each */
+	QLM_MODE_XLAUI_1X4,	/* 1 XLAUI, 4 lanes each */
+	QLM_MODE_10G_KR_4X1,	/* 4 10GBASE-KR, 1 lane each */
+	QLM_MODE_40G_KR4_1X4,	/* 1 40GBASE-KR4, 4 lanes each */
+};
+
+#ifndef XAUI
+#define  QLM0_MODE   QLM_MODE_SGMII
+#define  QLM1_MODE   QLM_MODE_SGMII
+#else
+#define  QLM0_MODE   QLM_MODE_XAUI_1X4
+#define  QLM1_MODE   QLM_MODE_XAUI_1X4
+#endif
+
+#endif /* THUNDER_BGX_H */
diff --git a/drivers/net/cavium/thunderx_smi.c b/drivers/net/cavium/thunderx_smi.c
new file mode 100644
index 0000000000..6083e807f5
--- /dev/null
+++ b/drivers/net/cavium/thunderx_smi.c
@@ -0,0 +1,272 @@
+/**
+ * (C) Copyright 2014, Cavium Inc.
+ *
+ * SPDX-License-Identifier:	GPL-2.0+
+**/
+
+#include <common.h>
+#include <phy.h>
+#include <miiphy.h>
+#include <malloc.h>
+#include <asm/io.h>
+
+enum thunderx_smi_mode {
+	CLAUSE22 = 0,
+	CLAUSE45 = 1,
+};
+
+enum {
+	SMI_OP_C22_WRITE = 0,
+	SMI_OP_C22_READ = 1,
+
+	SMI_OP_C45_ADDR = 0,
+	SMI_OP_C45_WRITE = 1,
+	SMI_OP_C45_PRIA = 2,
+	SMI_OP_C45_READ = 3,
+};
+
+union smi_x_clk {
+	u64 u;
+	struct smi_x_clk_s {
+		int phase:8;
+		int sample:4;
+		int preamble:1;
+		int clk_idle:1;
+		int reserved_14_14:1;
+		int sample_mode:1;
+		int sample_hi:5;
+		int reserved_21_23:3;
+		int mode:1;
+	} s;
+};
+
+union smi_x_cmd {
+	u64 u;
+	struct smi_x_cmd_s {
+		int reg_adr:5;
+		int reserved_5_7:3;
+		int phy_adr:5;
+		int reserved_13_15:3;
+		int phy_op:2;
+	} s;
+};
+
+union smi_x_wr_dat {
+	u64 u;
+	struct smi_x_wr_dat_s {
+		int dat:16;
+		int val:1;
+		int pending:1;
+	} s;
+};
+
+union smi_x_rd_dat {
+	u64 u;
+	struct smi_x_rd_dat_s {
+		int dat:16;
+		int val:1;
+		int pending:1;
+	} s;
+};
+
+union smi_x_en {
+	u64 u;
+	struct smi_x_en_s {
+		int en:1;
+	} s;
+};
+
+#define SMI_X_RD_DAT(x)	(0x87E005003810ull + (x) * 0x80ull)
+#define SMI_X_WR_DAT(x)	(0x87E005003808ull + (x) * 0x80ull)
+#define SMI_X_CMD(x)	(0x87E005003800ull + (x) * 0x80ull)
+#define SMI_X_CLK(x)	(0x87E005003818ull + (x) * 0x80ull)
+#define SMI_X_EN(x)	(0x87E005003820ull + (x) * 0x80ull)
+
+struct thunderx_priv {
+	int bus_num;
+	enum thunderx_smi_mode mode;
+};
+
+#define MDIO_TIMEOUT 256
+
+void thunderx_smi_setmode(struct mii_dev *bus, enum thunderx_smi_mode mode)
+{
+	struct thunderx_priv *priv = bus->priv;
+	union smi_x_clk smix_clk;
+
+	if (priv->mode != mode) {
+		smix_clk.u = readq(SMI_X_CLK(priv->bus_num));
+
+		smix_clk.s.mode = mode;
+		smix_clk.s.preamble = 1;
+
+		writeq(smix_clk.u, SMI_X_CLK(priv->bus_num));
+
+		priv->mode = mode;
+	}
+}
+
+int thunderx_c45_addr(struct mii_dev *bus, int addr, int devad, int regnum)
+{
+	struct thunderx_priv *priv = bus->priv;
+
+	union smi_x_cmd smix_cmd;
+	union smi_x_wr_dat smix_wr_dat;
+	unsigned long timeout = MDIO_TIMEOUT;
+
+	thunderx_smi_setmode(bus, CLAUSE45);
+
+	smix_wr_dat.u = 0;
+	smix_wr_dat.s.dat = regnum;
+
+	writeq(smix_wr_dat.u, SMI_X_WR_DAT(priv->bus_num));
+
+	smix_cmd.u = 0;
+	smix_cmd.s.phy_op = SMI_OP_C45_ADDR;
+	smix_cmd.s.phy_adr = addr;
+	smix_cmd.s.reg_adr = devad;
+
+	writeq(smix_cmd.u, SMI_X_CMD(priv->bus_num));
+
+	do {
+		smix_wr_dat.u = readq(SMI_X_WR_DAT(priv->bus_num));
+		udelay(1);
+		timeout--;
+	} while (smix_wr_dat.s.pending && timeout);
+
+	return timeout == 0;
+}
+
+int thunderx_phy_read(struct mii_dev *bus, int addr, int devad, int regnum)
+{
+	struct thunderx_priv *priv = bus->priv;
+	union smi_x_cmd smix_cmd;
+	union smi_x_rd_dat smix_rd_dat;
+	unsigned long timeout = MDIO_TIMEOUT;
+	int ret;
+
+	enum thunderx_smi_mode mode = (devad < 0) ? CLAUSE22 : CLAUSE45;
+
+	debug("RD: Mode: %u, index: %u, addr: %d, devad: %d, reg: %d\n",
+	      mode, priv->bus_num, addr, devad, regnum);
+
+	thunderx_smi_setmode(bus, mode);
+
+	if (mode == CLAUSE45) {
+		ret = thunderx_c45_addr(bus, addr, devad, regnum);
+
+		if (ret)
+			return 0;
+	}
+
+	smix_cmd.u = 0;
+	smix_cmd.s.phy_adr = addr;
+	smix_cmd.s.reg_adr = regnum;
+
+	if (mode == CLAUSE45)
+		smix_cmd.s.phy_op = SMI_OP_C45_READ;
+	else
+		smix_cmd.s.phy_op = SMI_OP_C22_READ;
+
+	writeq(smix_cmd.u, SMI_X_CMD(priv->bus_num));
+
+	do {
+		smix_rd_dat.u = readq(SMI_X_RD_DAT(priv->bus_num));
+		udelay(1);
+		timeout--;
+	} while (smix_rd_dat.s.pending && timeout);
+
+	debug("SMIX_RD_DAT: %lx\n", (unsigned long)smix_rd_dat.u);
+
+	return smix_rd_dat.s.dat;
+}
+
+int thunderx_phy_write(struct mii_dev *bus, int addr, int devad, int regnum,
+		       u16 value)
+{
+	struct thunderx_priv *priv = bus->priv;
+	union smi_x_cmd smix_cmd;
+	union smi_x_wr_dat smix_wr_dat;
+	unsigned long timeout = MDIO_TIMEOUT;
+	int ret;
+
+	enum thunderx_smi_mode mode = (devad < 0) ? CLAUSE22 : CLAUSE45;
+
+	debug("WR: Mode: %u, index: %u, addr: %d, devad: %d, reg: %d\n",
+	      mode, priv->bus_num, addr, devad, regnum);
+
+	if (mode == CLAUSE45) {
+		ret = thunderx_c45_addr(bus, addr, devad, regnum);
+
+		if (ret)
+			return ret;
+	}
+
+	smix_wr_dat.u = 0;
+	smix_wr_dat.s.dat = value;
+
+	writeq(smix_wr_dat.u, SMI_X_WR_DAT(priv->bus_num));
+
+	smix_cmd.u = 0;
+	smix_cmd.s.phy_adr = addr;
+	smix_cmd.s.reg_adr = regnum;
+
+	if (mode == CLAUSE45)
+		smix_cmd.s.phy_op = SMI_OP_C45_READ;
+	else
+		smix_cmd.s.phy_op = SMI_OP_C22_READ;
+
+	writeq(smix_cmd.u, SMI_X_CMD(priv->bus_num));
+
+	do {
+		smix_wr_dat.u = readq(SMI_X_WR_DAT(priv->bus_num));
+		udelay(1);
+		timeout--;
+	} while (smix_wr_dat.s.pending && timeout);
+
+	debug("SMIX_WR_DAT: %lx\n", (unsigned long)smix_wr_dat.u);
+
+	return timeout == 0;
+}
+
+int thunderx_smi_reset(struct mii_dev *bus)
+{
+	struct thunderx_priv *priv = bus->priv;
+
+	union smi_x_en smi_en;
+
+	smi_en.s.en = 0;
+	writeq(smi_en.u, SMI_X_EN(priv->bus_num));
+
+	smi_en.s.en = 1;
+	writeq(smi_en.u, SMI_X_EN(priv->bus_num));
+
+	thunderx_smi_setmode(bus, CLAUSE22);
+
+	return 0;
+}
+
+int thunderx_smi_initialize(bd_t *bis, unsigned int index)
+{
+	struct mii_dev *bus = mdio_alloc();
+	struct thunderx_priv *priv = malloc(sizeof(*priv));
+
+	if (!bus || !priv) {
+		printf("Failed to allocate ThunderX MDIO bus # %u\n", index);
+		return -1;
+	}
+
+	bus->read = thunderx_phy_read;
+	bus->write = thunderx_phy_write;
+	bus->reset = thunderx_smi_reset;
+
+	bus->priv = priv;
+
+	priv->bus_num = index;
+	priv->mode = CLAUSE22;
+
+	/* use given name or generate its own unique name */
+	snprintf(bus->name, MDIO_NAME_LEN, "thunderx%d", priv->bus_num);
+
+	return mdio_register(bus);
+}
diff --git a/include/cavium/thunderx_smi.h b/include/cavium/thunderx_smi.h
new file mode 100644
index 0000000000..a3929cab8f
--- /dev/null
+++ b/include/cavium/thunderx_smi.h
@@ -0,0 +1,12 @@
+/**
+ * (C) Copyright 2014, Cavium Inc.
+ *
+ * SPDX-License-Identifier:	GPL-2.0+
+**/
+
+#ifndef THUNDERX_SMI_H_
+#define THUNDERX_SMI_H_
+
+int thunderx_smi_initialize(bd_t *bis, unsigned int index);
+
+#endif /* THUNDERX_SMI_H_ */
diff --git a/include/cavium/thunderx_vnic.h b/include/cavium/thunderx_vnic.h
new file mode 100644
index 0000000000..9839681114
--- /dev/null
+++ b/include/cavium/thunderx_vnic.h
@@ -0,0 +1,23 @@
+/**
+ * (C) Copyright 2014, Cavium Inc.
+ *
+ * SPDX-License-Identifier:	GPL-2.0+
+**/
+
+#ifndef VNIC_H_
+#define VNIC_H_
+
+struct nicpf;
+struct nicvf;
+
+struct nicpf *nic_initialize(unsigned int node);
+int nicvf_initialize(struct nicpf *, int vf_num, unsigned int node);
+int bgx_initialize(unsigned int bgx_idx,
+		   unsigned int smi_idx, unsigned int node);
+
+void bgx_get_count(int node, int *bgx_count);
+int bgx_get_lmac_count(int node, int bgx_idx);
+
+
+
+#endif /* VNIC_H_ */
-- 
2.29.0

