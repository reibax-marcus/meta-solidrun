From 3a7f0b4250685a6d29cd73ebd924204a0cd6afc3 Mon Sep 17 00:00:00 2001
From: Aaron Williams <aaron.williams@cavium.com>
Date: Tue, 27 Mar 2018 19:35:33 -0700
Subject: [PATCH 0804/1239] net: Cavium NIX: Initial checkin of generated CSR
 files.

Note that the format of these files is subject to change as the script that
generates these files matures, likely eliminating many of the chip checks.

Signed-off-by: Aaron Williams <aaron.williams@cavium.com>
---
 drivers/net/cavium/octeontx2/cavm-csrs-cgx.h |  8414 ++++++++++++++
 drivers/net/cavium/octeontx2/cavm-csrs-lmt.h |    98 +
 drivers/net/cavium/octeontx2/cavm-csrs-nix.h | 10177 +++++++++++++++++
 drivers/net/cavium/octeontx2/cavm-csrs-npc.h |  1761 +++
 drivers/net/cavium/octeontx2/cavm-csrs-rvu.h |  2465 ++++
 5 files changed, 22915 insertions(+)
 create mode 100644 drivers/net/cavium/octeontx2/cavm-csrs-cgx.h
 create mode 100644 drivers/net/cavium/octeontx2/cavm-csrs-lmt.h
 create mode 100644 drivers/net/cavium/octeontx2/cavm-csrs-nix.h
 create mode 100644 drivers/net/cavium/octeontx2/cavm-csrs-npc.h
 create mode 100644 drivers/net/cavium/octeontx2/cavm-csrs-rvu.h

diff --git a/drivers/net/cavium/octeontx2/cavm-csrs-cgx.h b/drivers/net/cavium/octeontx2/cavm-csrs-cgx.h
new file mode 100644
index 0000000000..adeb58e962
--- /dev/null
+++ b/drivers/net/cavium/octeontx2/cavm-csrs-cgx.h
@@ -0,0 +1,8414 @@
+#ifndef __CAVM_CSRS_CGX_H__
+#define __CAVM_CSRS_CGX_H__
+/* This file is auto-generated. Do not edit */
+
+/***********************license start***************
+ * Copyright (c) 2003-2018  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM  NETWORKS MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Configuration and status register (CSR) address and type definitions for
+ * Cavium CGX.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ */
+
+/**
+ * Enumeration cgx_bar_e
+ *
+ * CGX Base Address Register Enumeration
+ * Enumerates the base address registers.
+ */
+#define CAVM_CGX_BAR_E_CGXX_PF_BAR0(a) (0x87e0e0000000ll + 0x1000000ll * (a))
+#define CAVM_CGX_BAR_E_CGXX_PF_BAR0_SIZE 0x100000ull
+#define CAVM_CGX_BAR_E_CGXX_PF_BAR4(a) (0x87e0e0400000ll + 0x1000000ll * (a))
+#define CAVM_CGX_BAR_E_CGXX_PF_BAR4_SIZE 0x100000ull
+
+/**
+ * Enumeration cgx_int_vec_e
+ *
+ * CGX MSI-X Vector Enumeration
+ * Enumeration the MSI-X interrupt vectors.
+ */
+#define CAVM_CGX_INT_VEC_E_CMRX_INT(a) (0 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_CMR_MEM_INT (0x24)
+#define CAVM_CGX_INT_VEC_E_GMPX_GMI_RX_INT(a) (5 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_GMPX_GMI_TX_INT(a) (6 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_GMPX_GMI_WOL_INT(a) (7 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_GMPX_PCS_INT(a) (4 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_SMUX_RX_INT(a) (2 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_SMUX_RX_WOL_INT(a) (8 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_SMUX_TX_INT(a) (3 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_SPUX_INT(a) (1 + 9 * (a))
+#define CAVM_CGX_INT_VEC_E_SW (0x25)
+
+/**
+ * Enumeration cgx_lmac_types_e
+ *
+ * CGX LMAC Type Enumeration
+ * Enumerates the LMAC Types that CGX supports.
+ */
+#define CAVM_CGX_LMAC_TYPES_E_FIFTYG_R (8)
+#define CAVM_CGX_LMAC_TYPES_E_FORTYG_R (4)
+#define CAVM_CGX_LMAC_TYPES_E_HUNDREDG_R (9)
+#define CAVM_CGX_LMAC_TYPES_E_QSGMII (6)
+#define CAVM_CGX_LMAC_TYPES_E_RGMII (5)
+#define CAVM_CGX_LMAC_TYPES_E_RXAUI (2)
+#define CAVM_CGX_LMAC_TYPES_E_SGMII (0)
+#define CAVM_CGX_LMAC_TYPES_E_TENG_R (3)
+#define CAVM_CGX_LMAC_TYPES_E_TWENTYFIVEG_R (7)
+#define CAVM_CGX_LMAC_TYPES_E_USXGMII (0xa)
+#define CAVM_CGX_LMAC_TYPES_E_XAUI (1)
+
+/**
+ * Enumeration cgx_opcode_e
+ *
+ * INTERNAL: CGX Error Opcode Enumeration
+ *
+ * Enumerates the error opcodes created by CGX and presented to NCSI/NIX.
+ */
+#define CAVM_CGX_OPCODE_E_RE_FCS (7)
+#define CAVM_CGX_OPCODE_E_RE_FCS_RCV (8)
+#define CAVM_CGX_OPCODE_E_RE_JABBER (2)
+#define CAVM_CGX_OPCODE_E_RE_NONE (0)
+#define CAVM_CGX_OPCODE_E_RE_PARTIAL (1)
+#define CAVM_CGX_OPCODE_E_RE_RX_CTL (0xb)
+#define CAVM_CGX_OPCODE_E_RE_SKIP (0xc)
+#define CAVM_CGX_OPCODE_E_RE_TERMINATE (9)
+
+/**
+ * Enumeration cgx_spu_br_train_cst_e
+ *
+ * CGX Training Coefficient Status Enumeration
+ * 2-bit status for each coefficient as defined in IEEE 802.3, Table 72-5.
+ */
+#define CAVM_CGX_SPU_BR_TRAIN_CST_E_MAXIMUM (3)
+#define CAVM_CGX_SPU_BR_TRAIN_CST_E_MINIMUM (2)
+#define CAVM_CGX_SPU_BR_TRAIN_CST_E_NOT_UPDATED (0)
+#define CAVM_CGX_SPU_BR_TRAIN_CST_E_UPDATED (1)
+
+/**
+ * Enumeration cgx_spu_br_train_cup_e
+ *
+ * CGX Training Coefficient Enumeration
+ * 2-bit command for each coefficient as defined in IEEE 802.3, Table 72-4.
+ */
+#define CAVM_CGX_SPU_BR_TRAIN_CUP_E_DECREMENT (1)
+#define CAVM_CGX_SPU_BR_TRAIN_CUP_E_HOLD (0)
+#define CAVM_CGX_SPU_BR_TRAIN_CUP_E_INCREMENT (2)
+#define CAVM_CGX_SPU_BR_TRAIN_CUP_E_RSV_CMD (3)
+
+/**
+ * Enumeration cgx_usxgmii_rate_e
+ *
+ * CGX USXGMII Rate Enumeration
+ * Enumerates the USXGMII sub-port type rate, CGX()_SPU()_CONTROL1[USXGMII_RATE].
+ *
+ * Selecting a rate higher than the maximum allowed for a given port sub-type
+ * (specified by CGX()_SPU()_CONTROL1[USXGMII_TYPE]), e.g., selecting ::RATE_2HG (2.5
+ * Gbps) for CGX_USXGMII_TYPE_E::SXGMII_2G, will cause unpredictable behavior. USXGMII
+ * hardware-based autonegotiation may change this setting.
+ */
+#define CAVM_CGX_USXGMII_RATE_E_RATE_100M (1)
+#define CAVM_CGX_USXGMII_RATE_E_RATE_10G (5)
+#define CAVM_CGX_USXGMII_RATE_E_RATE_10M (0)
+#define CAVM_CGX_USXGMII_RATE_E_RATE_1G (2)
+#define CAVM_CGX_USXGMII_RATE_E_RATE_20G (6)
+#define CAVM_CGX_USXGMII_RATE_E_RATE_2HG (3)
+#define CAVM_CGX_USXGMII_RATE_E_RATE_5G (4)
+#define CAVM_CGX_USXGMII_RATE_E_RSV_RATE (7)
+
+/**
+ * Enumeration cgx_usxgmii_type_e
+ *
+ * CGX USXGMII Port Sub-Type Enumeration
+ * Enumerates the USXGMII sub-port type, CGX()_SPU()_CONTROL1[USXGMII_TYPE].
+ *
+ * The description indicates the maximum rate and the maximum number of ports (LMACs)
+ * for each sub-type. The minimum rate for any port is 10M.
+ * The rate selection for each LMAC is made using CGX()_SPU()_CONTROL1[USXGMII_RATE]
+ * and the number of active ports/LMACs is implicitly determined by the value given to
+ * CGX()_CMR()_CONFIG[ENABLE] for each LMAC.
+ *
+ * Selecting a rate higher than the maximum allowed for a given port sub-type or
+ * enabling more LMACs than the maximum allowed for a given port sub-type will cause
+ * unpredictable behavior.
+ */
+#define CAVM_CGX_USXGMII_TYPE_E_DXGMII_10G (3)
+#define CAVM_CGX_USXGMII_TYPE_E_DXGMII_20G (5)
+#define CAVM_CGX_USXGMII_TYPE_E_DXGMII_5G (4)
+#define CAVM_CGX_USXGMII_TYPE_E_QXGMII_10G (7)
+#define CAVM_CGX_USXGMII_TYPE_E_QXGMII_20G (6)
+#define CAVM_CGX_USXGMII_TYPE_E_SXGMII_10G (0)
+#define CAVM_CGX_USXGMII_TYPE_E_SXGMII_2G (2)
+#define CAVM_CGX_USXGMII_TYPE_E_SXGMII_5G (1)
+
+/**
+ * Structure cgx_spu_br_lane_train_status_s
+ *
+ * CGX Lane Training Status Structure
+ * This is the group of lane status bits for a single lane in the BASE-R PMD status register
+ * (MDIO address 1.151) as defined in IEEE 802.3ba-2010, Table 45-55.
+ */
+union cavm_cgx_spu_br_lane_train_status_s {
+	u32 u;
+	struct cavm_cgx_spu_br_lane_train_status_s_s {
+		u32 rx_trained                       : 1;
+		u32 frame_lock                       : 1;
+		u32 training                         : 1;
+		u32 training_failure                 : 1;
+		u32 reserved_4_31                    : 28;
+	} s;
+	/* struct cavm_cgx_spu_br_lane_train_status_s_s cn; */
+};
+
+/**
+ * Structure cgx_spu_br_train_cup_s
+ *
+ * CGX Lane Training Coeffiecient Structure
+ * This is the coefficient update field of the BASE-R link training packet as defined in
+ * IEEE 802.3, Table 72-4.
+ */
+union cavm_cgx_spu_br_train_cup_s {
+	u32 u;
+	struct cavm_cgx_spu_br_train_cup_s_s {
+		u32 pre_cup                          : 2;
+		u32 main_cup                         : 2;
+		u32 post_cup                         : 2;
+		u32 reserved_6_11                    : 6;
+		u32 init                             : 1;
+		u32 preset                           : 1;
+		u32 reserved_14_31                   : 18;
+	} s;
+	struct cavm_cgx_spu_br_train_cup_s_cn {
+		u32 pre_cup                          : 2;
+		u32 main_cup                         : 2;
+		u32 post_cup                         : 2;
+		u32 reserved_6_11                    : 6;
+		u32 init                             : 1;
+		u32 preset                           : 1;
+		u32 reserved_14_15                   : 2;
+		u32 reserved_16_31                   : 16;
+	} cn;
+};
+
+/**
+ * Structure cgx_spu_br_train_rep_s
+ *
+ * CGX Training Report Structure
+ * This is the status report field of the BASE-R link training packet as defined in IEEE 802.3,
+ * Table 72-5.
+ */
+union cavm_cgx_spu_br_train_rep_s {
+	u32 u;
+	struct cavm_cgx_spu_br_train_rep_s_s {
+		u32 pre_cst                          : 2;
+		u32 main_cst                         : 2;
+		u32 post_cst                         : 2;
+		u32 reserved_6_14                    : 9;
+		u32 rx_ready                         : 1;
+		u32 reserved_16_31                   : 16;
+	} s;
+	/* struct cavm_cgx_spu_br_train_rep_s_s cn; */
+};
+
+/**
+ * Structure cgx_spu_sds_cu_s
+ *
+ * INTERNAL: CGX Training Coeffiecient Structure
+ *
+ * This structure is similar to CGX_SPU_BR_TRAIN_CUP_S format, but with reserved fields removed
+ * and [RCVR_READY] field added.
+ */
+union cavm_cgx_spu_sds_cu_s {
+	u32 u;
+	struct cavm_cgx_spu_sds_cu_s_s {
+		u32 pre_cu                           : 2;
+		u32 main_cu                          : 2;
+		u32 post_cu                          : 2;
+		u32 initialize                       : 1;
+		u32 preset                           : 1;
+		u32 rcvr_ready                       : 1;
+		u32 reserved_9_31                    : 23;
+	} s;
+	/* struct cavm_cgx_spu_sds_cu_s_s cn; */
+};
+
+/**
+ * Structure cgx_spu_sds_skew_status_s
+ *
+ * CGX Skew Status Structure
+ * Provides receive skew information detected for a physical SerDes lane when it is assigned to a
+ * multilane LMAC/LPCS. Contents are valid when RX deskew is done for the associated LMAC/LPCS.
+ */
+union cavm_cgx_spu_sds_skew_status_s {
+	u32 u;
+	struct cavm_cgx_spu_sds_skew_status_s_s {
+		u32 am_timestamp                     : 12;
+		u32 reserved_12_15                   : 4;
+		u32 am_lane_id                       : 5;
+		u32 reserved_21_22                   : 2;
+		u32 lane_skew                        : 7;
+		u32 reserved_30_31                   : 2;
+	} s;
+	/* struct cavm_cgx_spu_sds_skew_status_s_s cn; */
+};
+
+/**
+ * Structure cgx_spu_sds_sr_s
+ *
+ * INTERNAL: CGX Lane Training Coefficient Structure
+ *
+ * Similar to CGX_SPU_BR_TRAIN_REP_S format, but with reserved and RX ready fields removed.
+ */
+union cavm_cgx_spu_sds_sr_s {
+	u32 u;
+	struct cavm_cgx_spu_sds_sr_s_s {
+		u32 pre_status                       : 2;
+		u32 main_status                      : 2;
+		u32 post_status                      : 2;
+		u32 reserved_6_31                    : 26;
+	} s;
+	/* struct cavm_cgx_spu_sds_sr_s_s cn; */
+};
+
+/**
+ * Register (RSL) cgx#_active_pc
+ *
+ * CGX ACTIVE PC Register
+ * This register counts the conditional clocks for power management.
+ */
+union cavm_cgxx_active_pc {
+	u64 u;
+	struct cavm_cgxx_active_pc_s {
+		u64 cnt                              : 64;
+	} s;
+	/* struct cavm_cgxx_active_pc_s cn; */
+};
+
+static inline u64 CAVM_CGXX_ACTIVE_PC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_ACTIVE_PC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2010;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_activity
+ *
+ * CGX CMR Activity Registers
+ */
+union cavm_cgxx_cmrx_activity {
+	u64 u;
+	struct cavm_cgxx_cmrx_activity_s {
+		u64 act_tx_lo                        : 1;
+		u64 act_tx_hi                        : 1;
+		u64 pause_tx                         : 1;
+		u64 act_rx_lo                        : 1;
+		u64 act_rx_hi                        : 1;
+		u64 pause_rx                         : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_cmrx_activity_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_ACTIVITY(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_ACTIVITY(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x5f8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_config
+ *
+ * CGX CMR Configuration Registers
+ * Logical MAC/PCS configuration registers; one per LMAC. The maximum number of LMACs (and
+ * maximum LMAC ID) that can be enabled by these registers is limited by
+ * CGX()_CMR_RX_LMACS[LMACS] and CGX()_CMR_TX_LMACS[LMACS].
+ *
+ * Internal:
+ * \<pre\>
+ * Example configurations:
+ *   ---------------------------------------------------------------------------
+ *   Configuration           LMACS  Register             [ENABLE]    [LMAC_TYPE]
+ *   ---------------------------------------------------------------------------
+ *   1x50G+1x25G+1xSGMII     4      CGXn_CMR0_CONFIG     1           8
+ *                                  CGXn_CMR1_CONFIG     0           --
+ *                                  CGXn_CMR2_CONFIG     1           7
+ *                                  CGXn_CMR3_CONFIG     1           0
+ *   ---------------------------------------------------------------------------
+ *   USXGMII                 1-4    CGXn_CMR0_CONFIG     1           a
+ *                                  CGXn_CMR1_CONFIG     1           a
+ *                                  CGXn_CMR2_CONFIG     1           a
+ *                                  CGXn_CMR3_CONFIG     1           a
+ *   ---------------------------------------------------------------------------
+ *   1x100GBASE-R4           1      CGXn_CMR0_CONFIG     1           9
+ *                                  CGXn_CMR1_CONFIG     0           --
+ *                                  CGXn_CMR2_CONFIG     0           --
+ *                                  CGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   2x50GBASE-R2            2      CGXn_CMR0_CONFIG     1           8
+ *                                  CGXn_CMR1_CONFIG     1           8
+ *                                  CGXn_CMR2_CONFIG     0           --
+ *                                  CGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   4x25GBASE-R             4      CGXn_CMR0_CONFIG     1           7
+ *                                  CGXn_CMR1_CONFIG     1           7
+ *                                  CGXn_CMR2_CONFIG     1           7
+ *                                  CGXn_CMR3_CONFIG     1           7
+ *   ---------------------------------------------------------------------------
+ *   QSGMII                  4      CGXn_CMR0_CONFIG     1           6
+ *                                  CGXn_CMR1_CONFIG     1           6
+ *                                  CGXn_CMR2_CONFIG     1           6
+ *                                  CGXn_CMR3_CONFIG     1           6
+ *   ---------------------------------------------------------------------------
+ *   1x40GBASE-R4            1      CGXn_CMR0_CONFIG     1           4
+ *                                  CGXn_CMR1_CONFIG     0           --
+ *                                  CGXn_CMR2_CONFIG     0           --
+ *                                  CGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   4x10GBASE-R             4      CGXn_CMR0_CONFIG     1           3
+ *                                  CGXn_CMR1_CONFIG     1           3
+ *                                  CGXn_CMR2_CONFIG     1           3
+ *                                  CGXn_CMR3_CONFIG     1           3
+ *   ---------------------------------------------------------------------------
+ *   2xRXAUI                 2      CGXn_CMR0_CONFIG     1           2
+ *                                  CGXn_CMR1_CONFIG     1           2
+ *                                  CGXn_CMR2_CONFIG     0           --
+ *                                  CGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   1x10GBASE-X/XAUI/DXAUI  1      CGXn_CMR0_CONFIG     1           1
+ *                                  CGXn_CMR1_CONFIG     0           --
+ *                                  CGXn_CMR2_CONFIG     0           --
+ *                                  CGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   4xSGMII/1000BASE-X      4      CGXn_CMR0_CONFIG     1           0
+ *                                  CGXn_CMR1_CONFIG     1           0
+ *                                  CGXn_CMR2_CONFIG     1           0
+ *                                  CGXn_CMR3_CONFIG     1           0
+ *   ---------------------------------------------------------------------------
+ * \</pre\>
+ */
+union cavm_cgxx_cmrx_config {
+	u64 u;
+	struct cavm_cgxx_cmrx_config_s {
+		u64 lane_to_sds                      : 8;
+		u64 reserved_8_39                    : 32;
+		u64 lmac_type                        : 4;
+		u64 unused                           : 8;
+		u64 int_beat_gen                     : 1;
+		u64 data_pkt_tx_en                   : 1;
+		u64 data_pkt_rx_en                   : 1;
+		u64 enable                           : 1;
+		u64 x2p_select                       : 3;
+		u64 p2x_select                       : 3;
+		u64 reserved_62_63                   : 2;
+	} s;
+	/* struct cavm_cgxx_cmrx_config_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_CONFIG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_CONFIG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_int
+ *
+ * CGX CMR Interrupt Register
+ */
+union cavm_cgxx_cmrx_int {
+	u64 u;
+	struct cavm_cgxx_cmrx_int_s {
+		u64 pause_drp                        : 1;
+		u64 overflw                          : 1;
+		u64 nic_nxc                          : 1;
+		u64 nix0_nxc                         : 1;
+		u64 nix1_nxc                         : 1;
+		u64 nix0_e_nxc                       : 1;
+		u64 nix1_e_nxc                       : 1;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_cmrx_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x40 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_int_ena_w1c
+ *
+ * CGX CMR Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_cmrx_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_cmrx_int_ena_w1c_s {
+		u64 pause_drp                        : 1;
+		u64 overflw                          : 1;
+		u64 nic_nxc                          : 1;
+		u64 nix0_nxc                         : 1;
+		u64 nix1_nxc                         : 1;
+		u64 nix0_e_nxc                       : 1;
+		u64 nix1_e_nxc                       : 1;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_cmrx_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x50 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_int_ena_w1s
+ *
+ * CGX CMR Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_cmrx_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_cmrx_int_ena_w1s_s {
+		u64 pause_drp                        : 1;
+		u64 overflw                          : 1;
+		u64 nic_nxc                          : 1;
+		u64 nix0_nxc                         : 1;
+		u64 nix1_nxc                         : 1;
+		u64 nix0_e_nxc                       : 1;
+		u64 nix1_e_nxc                       : 1;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_cmrx_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x58 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_int_w1s
+ *
+ * CGX CMR Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_cmrx_int_w1s {
+	u64 u;
+	struct cavm_cgxx_cmrx_int_w1s_s {
+		u64 pause_drp                        : 1;
+		u64 overflw                          : 1;
+		u64 nic_nxc                          : 1;
+		u64 nix0_nxc                         : 1;
+		u64 nix1_nxc                         : 1;
+		u64 nix0_e_nxc                       : 1;
+		u64 nix1_e_nxc                       : 1;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_cmrx_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x48 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_led_timing
+ *
+ * CGX MAC LED Activity Timing Registers
+ */
+union cavm_cgxx_cmrx_led_timing {
+	u64 u;
+	struct cavm_cgxx_cmrx_led_timing_s {
+		u64 extension                        : 8;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_cgxx_cmrx_led_timing_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_LED_TIMING(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_LED_TIMING(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x5f0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_prt_cbfc_ctl
+ *
+ * CGX CMR LMAC PFC Control Registers
+ * See CGX()_CMR()_RX_LOGL_XOFF[XOFF].
+ */
+union cavm_cgxx_cmrx_prt_cbfc_ctl {
+	u64 u;
+	struct cavm_cgxx_cmrx_prt_cbfc_ctl_s {
+		u64 reserved_0_15                    : 16;
+		u64 phys_bp                          : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_cmrx_prt_cbfc_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_PRT_CBFC_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_PRT_CBFC_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x608 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_bp_drop
+ *
+ * CGX Receive Backpressure Drop Register
+ */
+union cavm_cgxx_cmrx_rx_bp_drop {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_bp_drop_s {
+		u64 mark                             : 7;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_bp_drop_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_BP_DROP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_BP_DROP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xd8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_bp_off
+ *
+ * CGX Receive Backpressure Off Register
+ */
+union cavm_cgxx_cmrx_rx_bp_off {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_bp_off_s {
+		u64 mark                             : 7;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_bp_off_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_BP_OFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_BP_OFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xe8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_bp_on
+ *
+ * CGX Receive Backpressure On Register
+ */
+union cavm_cgxx_cmrx_rx_bp_on {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_bp_on_s {
+		u64 mark                             : 13;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_bp_on_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_BP_ON(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_BP_ON(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xe0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_bp_status
+ *
+ * CGX CMR Receive Backpressure Status Registers
+ */
+union cavm_cgxx_cmrx_rx_bp_status {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_bp_status_s {
+		u64 bp                               : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_bp_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_BP_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_BP_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xf0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_dmac_ctl0
+ *
+ * CGX CMR Receive DMAC Address-Control0 Register
+ * DMAC CAM control register for use by X2P/NIX bound traffic.
+ * Received packets are only passed to X2P/NIX when the DMAC0 filter result is
+ * ACCEPT and STEERING0 filter result is PASS. See also CGX()_CMR_RX_DMAC()_CAM0
+ * and CGX()_CMR_RX_STEERING0().
+ *
+ * Internal:
+ * "* ALGORITHM
+ * Here is some pseudo code that represents the address filter behavior.
+ * \<pre\>
+ * dmac_addr_filter(uint8 prt, uint48 dmac) {
+ * for (lmac=0, lmac\<4, lmac++) {
+ *   if (is_bcst(dmac))                               // broadcast accept
+ *     return (CGX()_CMR(lmac)_RX_DMAC_CTL0[BCST_ACCEPT] ? ACCEPT : REJECT);
+ *   if (is_mcst(dmac) && CGX()_CMR(lmac)_RX_DMAC_CTL0[MCST_MODE] == 0)   // multicast reject
+ *     return REJECT;
+ *   if (is_mcst(dmac) && CGX()_CMR(lmac)_RX_DMAC_CTL0[MCST_MODE] == 1)   // multicast accept
+ *     return ACCEPT;
+ *   else        // DMAC CAM filter
+ *     cam_hit = 0;
+ *   for (i=0; i\<32; i++) {
+ *     cam = CGX()_CMR_RX_DMAC(i)_CAM0;
+ *     if (cam[EN] && cam[ID] == lmac && cam[ADR] == dmac) {
+ *       cam_hit = 1;
+ *       break;
+ *     }
+ *   }
+ *   if (cam_hit) {
+ *     return (CGX()_CMR(lmac)_RX_DMAC_CTL0[CAM_ACCEPT] ? ACCEPT : REJECT);
+ *   else
+ *     return (CGX()_CMR(lmac)_RX_DMAC_CTL0[CAM_ACCEPT] ? REJECT : ACCEPT);
+ *   }
+ * }
+ * \</pre\>"
+ */
+union cavm_cgxx_cmrx_rx_dmac_ctl0 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_dmac_ctl0_s {
+		u64 bcst_accept                      : 1;
+		u64 mcst_mode                        : 2;
+		u64 cam_accept                       : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_dmac_ctl0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_DMAC_CTL0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_DMAC_CTL0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x1f8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_dmac_ctl1
+ *
+ * CGX CMR Receive DMAC Address-Control1 Register
+ * DMAC CAM control register for use by NCSI bound traffic.
+ * Received packets are only passed to NCSI when the DMAC1 filter result is ACCEPT and
+ * STEERING1 filter result is PASS. See also CGX()_CMR_RX_DMAC()_CAM1 and
+ * CGX()_CMR_RX_STEERING1().
+ * For use with the LMAC associated with NCSI; see CGX()_CMR_GLOBAL_CONFIG[NCSI_LMAC_ID].
+ *
+ * Internal:
+ * ALGORITHM: See CGX()_CMR()_RX_DMAC_CTL0.
+ */
+union cavm_cgxx_cmrx_rx_dmac_ctl1 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_dmac_ctl1_s {
+		u64 bcst_accept                      : 1;
+		u64 mcst_mode                        : 2;
+		u64 cam_accept                       : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_dmac_ctl1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_DMAC_CTL1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_DMAC_CTL1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x3f8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_fifo_len
+ *
+ * CGX CMR Receive Fifo Length Registers
+ */
+union cavm_cgxx_cmrx_rx_fifo_len {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_fifo_len_s {
+		u64 fifo_len                         : 14;
+		u64 busy                             : 1;
+		u64 fifo_len_e                       : 14;
+		u64 busy_e                           : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_fifo_len_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_FIFO_LEN(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_FIFO_LEN(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x108 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_id_map
+ *
+ * CGX CMR Receive ID Map Register
+ * These registers set the RX LMAC ID mapping for X2P/NIX.
+ */
+union cavm_cgxx_cmrx_rx_id_map {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_id_map_s {
+		u64 pknd                             : 6;
+		u64 unused                           : 2;
+		u64 rid                              : 7;
+		u64 reserved_15_63                   : 49;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_id_map_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_ID_MAP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_ID_MAP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x60 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_logl_xoff
+ *
+ * CGX CMR Receive Logical XOFF Registers
+ */
+union cavm_cgxx_cmrx_rx_logl_xoff {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_logl_xoff_s {
+		u64 xoff                             : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_logl_xoff_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_LOGL_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_LOGL_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xf8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_logl_xon
+ *
+ * CGX CMR Receive Logical XON Registers
+ */
+union cavm_cgxx_cmrx_rx_logl_xon {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_logl_xon_s {
+		u64 xon                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_logl_xon_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_LOGL_XON(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_LOGL_XON(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_merge_stat0
+ *
+ * CGX RX Preemption Status Register 0
+ */
+union cavm_cgxx_cmrx_rx_merge_stat0 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_merge_stat0_s {
+		u64 fa_err_cnt                       : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_merge_stat0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x138 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_merge_stat1
+ *
+ * CGX RX Preemption Status Register 1
+ */
+union cavm_cgxx_cmrx_rx_merge_stat1 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_merge_stat1_s {
+		u64 fs_err_cnt                       : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_merge_stat1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x140 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_merge_stat2
+ *
+ * CGX RX Preemption Status Register 2
+ */
+union cavm_cgxx_cmrx_rx_merge_stat2 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_merge_stat2_s {
+		u64 fa_ok_cnt                        : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_merge_stat2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x148 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_merge_stat3
+ *
+ * CGX RX Preemption Status Register 3
+ */
+union cavm_cgxx_cmrx_rx_merge_stat3 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_merge_stat3_s {
+		u64 ff_cnt                           : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_merge_stat3_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x150 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_merge_stat4
+ *
+ * CGX RX Preemption Status Register 4
+ */
+union cavm_cgxx_cmrx_rx_merge_stat4 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_merge_stat4_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_merge_stat4_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT4(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_MERGE_STAT4(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x158 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_pause_drop_time
+ *
+ * CGX CMR Receive Pause Drop-Time Register
+ */
+union cavm_cgxx_cmrx_rx_pause_drop_time {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_pause_drop_time_s {
+		u64 pause_time                       : 16;
+		u64 pause_time_e                     : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_pause_drop_time_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_PAUSE_DROP_TIME(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_PAUSE_DROP_TIME(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x68 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat0
+ *
+ * CGX Receive Status Register 0
+ * These registers provide a count of received packets that meet the following conditions:
+ * * are not recognized as ERROR packets(any OPCODE).
+ * * are not recognized as PAUSE packets.
+ * * are not dropped due FIFO full status.
+ * * are not dropped due DMAC0 or STEERING0 filtering.
+ *
+ * Internal:
+ * "This pseudo code represents the RX STAT0 through STAT8 accounting:
+ * \<pre\>
+ * If (errored)
+ *   incr RX_STAT8
+ * else if (ctrl packet, i.e. Pause/PFC)
+ *   incr RX_STAT2,3
+ * else if (fifo full drop)
+ *   incr RX_STAT6,7
+ * else if (DMAC0/VLAN0 filter drop)
+ *   incr RX_STAT4,5 if not a filter+decision
+ * else
+ *   incr RX_STAT0,1
+ * end
+ * \</pre\>"
+ */
+union cavm_cgxx_cmrx_rx_stat0 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat0_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x70 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat1
+ *
+ * CGX Receive Status Register 1
+ * These registers provide a count of octets of received packets.
+ */
+union cavm_cgxx_cmrx_rx_stat1 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat1_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x78 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat2
+ *
+ * CGX Receive Status Register 2
+ * These registers provide a count of received packets that meet the following conditions:
+ * * are not recognized as ERROR packets(any OPCODE).
+ * * are recognized as PAUSE packets.
+ *
+ * Pause packets can be optionally dropped or forwarded based on
+ * CGX()_SMU()_RX_FRM_CTL[CTL_DRP]/CGX()_GMP_GMI_RX()_FRM_CTL[CTL_DRP].
+ * This count increments regardless of whether the packet is dropped.
+ */
+union cavm_cgxx_cmrx_rx_stat2 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat2_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x80 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat3
+ *
+ * CGX Receive Status Register 3
+ * These registers provide a count of octets of received PAUSE and control packets.
+ */
+union cavm_cgxx_cmrx_rx_stat3 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat3_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat3_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x88 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat4
+ *
+ * CGX Receive Status Register 4
+ * These registers provide a count of received packets that meet the following conditions:
+ * * are not recognized as ERROR packets(any OPCODE).
+ * * are not recognized as PAUSE packets.
+ * * are not dropped due FIFO full status.
+ * * are dropped due DMAC0 or STEERING0 filtering.
+ *
+ * 16B packets or smaller (20B in case of FCS strip) as the result of truncation
+ * or other means are not dropped by CGX (unless filter and decision is also
+ * asserted) and will never appear in this count.
+ * Should the MAC signal to the CMR that the packet be filtered upon decision before the end of
+ * packet, then STAT4 and STAT5 will not be updated.
+ */
+union cavm_cgxx_cmrx_rx_stat4 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat4_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat4_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT4(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT4(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x90 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat5
+ *
+ * CGX Receive Status Register 5
+ * These registers provide a count of octets of filtered DMAC0 or VLAN STEERING0 packets.
+ */
+union cavm_cgxx_cmrx_rx_stat5 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat5_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat5_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT5(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT5(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x98 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat6
+ *
+ * CGX Receive Status Register 6
+ * These registers provide a count of received packets that meet the following conditions:
+ * * are not recognized as ERROR packets(any OPCODE).
+ * * are not recognized as PAUSE packets.
+ * * are dropped due FIFO full status.
+ *
+ * They do not count any packet that is truncated at the point of overflow and sent
+ * on to the NIX. The truncated packet will be marked with error and increment STAT8.
+ * These registers count all entire packets dropped by the FIFO for a given LMAC.
+ */
+union cavm_cgxx_cmrx_rx_stat6 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat6_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat6_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT6(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT6(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xa0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat7
+ *
+ * CGX Receive Status Register 7
+ * These registers provide a count of octets of received packets that were dropped due to a full
+ * receive FIFO.
+ */
+union cavm_cgxx_cmrx_rx_stat7 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat7_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat7_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT7(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT7(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xa8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat8
+ *
+ * CGX Receive Status Register 8
+ * These registers provide a count of received packets that meet the following conditions:
+ *  * are recognized as ERROR packets(any OPCODE).
+ */
+union cavm_cgxx_cmrx_rx_stat8 {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat8_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat8_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT8(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT8(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xb0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_rx_stat_pri#_xoff
+ *
+ * CGX CMR RX XON to XOFF transition Registers
+ */
+union cavm_cgxx_cmrx_rx_stat_prix_xoff {
+	u64 u;
+	struct cavm_cgxx_cmrx_rx_stat_prix_xoff_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_rx_stat_prix_xoff_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_RX_STAT_PRIX_XOFF(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_RX_STAT_PRIX_XOFF(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 7)))
+		return 0x7c0 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_scratch#
+ *
+ * CGX CMR Scratch Registers
+ */
+union cavm_cgxx_cmrx_scratchx {
+	u64 u;
+	struct cavm_cgxx_cmrx_scratchx_s {
+		u64 scratch                          : 64;
+	} s;
+	/* struct cavm_cgxx_cmrx_scratchx_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_SCRATCHX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_SCRATCHX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 1)))
+		return 0x1050 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_channel
+ *
+ * CGX CMR Transmit-Channels Registers
+ */
+union cavm_cgxx_cmrx_tx_channel {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_channel_s {
+		u64 msk                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_channel_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_CHANNEL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_CHANNEL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x600 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_fifo_len
+ *
+ * CGX CMR Transmit Fifo Length Registers
+ */
+union cavm_cgxx_cmrx_tx_fifo_len {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_fifo_len_s {
+		u64 fifo_len                         : 14;
+		u64 lmac_idle                        : 1;
+		u64 fifo_e_len                       : 14;
+		u64 lmac_e_idle                      : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_fifo_len_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_FIFO_LEN(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_FIFO_LEN(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x618 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_hg2_status
+ *
+ * CGX CMR Transmit HiGig2 Status Registers
+ */
+union cavm_cgxx_cmrx_tx_hg2_status {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_hg2_status_s {
+		u64 lgtim2go                         : 16;
+		u64 xof                              : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_hg2_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_HG2_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_HG2_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x610 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_merge_stat0
+ *
+ * CGX TX Preemption Status Register 0
+ */
+union cavm_cgxx_cmrx_tx_merge_stat0 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_merge_stat0_s {
+		u64 ff_cnt                           : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_merge_stat0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_MERGE_STAT0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_MERGE_STAT0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x160 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_ovr_bp
+ *
+ * CGX CMR Transmit-Channels Backpressure Override Registers
+ */
+union cavm_cgxx_cmrx_tx_ovr_bp {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_ovr_bp_s {
+		u64 tx_chan_bp                       : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_ovr_bp_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_OVR_BP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_OVR_BP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x620 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat0
+ *
+ * CGX CMR Transmit Statistics Registers 0
+ */
+union cavm_cgxx_cmrx_tx_stat0 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat0_s {
+		u64 xscol                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x700 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat1
+ *
+ * CGX CMR Transmit Statistics Registers 1
+ */
+union cavm_cgxx_cmrx_tx_stat1 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat1_s {
+		u64 xsdef                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x708 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat10
+ *
+ * CGX CMR Transmit Statistics Registers 10
+ */
+union cavm_cgxx_cmrx_tx_stat10 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat10_s {
+		u64 hist4                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat10_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT10(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT10(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x750 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat11
+ *
+ * CGX CMR Transmit Statistics Registers 11
+ */
+union cavm_cgxx_cmrx_tx_stat11 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat11_s {
+		u64 hist5                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat11_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT11(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT11(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x758 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat12
+ *
+ * CGX CMR Transmit Statistics Registers 12
+ */
+union cavm_cgxx_cmrx_tx_stat12 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat12_s {
+		u64 hist6                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat12_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT12(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT12(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x760 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat13
+ *
+ * CGX CMR Transmit Statistics Registers 13
+ */
+union cavm_cgxx_cmrx_tx_stat13 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat13_s {
+		u64 hist7                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat13_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT13(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT13(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x768 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat14
+ *
+ * CGX CMR Transmit Statistics Registers 14
+ */
+union cavm_cgxx_cmrx_tx_stat14 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat14_s {
+		u64 bcst                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat14_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT14(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT14(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x770 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat15
+ *
+ * CGX CMR Transmit Statistics Registers 15
+ */
+union cavm_cgxx_cmrx_tx_stat15 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat15_s {
+		u64 mcst                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat15_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT15(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT15(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x778 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat16
+ *
+ * CGX CMR Transmit Statistics Registers 16
+ */
+union cavm_cgxx_cmrx_tx_stat16 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat16_s {
+		u64 undflw                           : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat16_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT16(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT16(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x780 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat17
+ *
+ * CGX CMR Transmit Statistics Registers 17
+ */
+union cavm_cgxx_cmrx_tx_stat17 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat17_s {
+		u64 ctl                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat17_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT17(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT17(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x788 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat2
+ *
+ * CGX CMR Transmit Statistics Registers 2
+ */
+union cavm_cgxx_cmrx_tx_stat2 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat2_s {
+		u64 mcol                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x710 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat3
+ *
+ * CGX CMR Transmit Statistics Registers 3
+ */
+union cavm_cgxx_cmrx_tx_stat3 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat3_s {
+		u64 scol                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat3_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x718 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat4
+ *
+ * CGX CMR Transmit Statistics Registers 4
+ */
+union cavm_cgxx_cmrx_tx_stat4 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat4_s {
+		u64 octs                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat4_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT4(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT4(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x720 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat5
+ *
+ * CGX CMR Transmit Statistics Registers 5
+ */
+union cavm_cgxx_cmrx_tx_stat5 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat5_s {
+		u64 pkts                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat5_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT5(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT5(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x728 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat6
+ *
+ * CGX CMR Transmit Statistics Registers 6
+ */
+union cavm_cgxx_cmrx_tx_stat6 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat6_s {
+		u64 hist0                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat6_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT6(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT6(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x730 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat7
+ *
+ * CGX CMR Transmit Statistics Registers 7
+ */
+union cavm_cgxx_cmrx_tx_stat7 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat7_s {
+		u64 hist1                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat7_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT7(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT7(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x738 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat8
+ *
+ * CGX CMR Transmit Statistics Registers 8
+ */
+union cavm_cgxx_cmrx_tx_stat8 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat8_s {
+		u64 hist2                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat8_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT8(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT8(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x740 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat9
+ *
+ * CGX CMR Transmit Statistics Registers 9
+ */
+union cavm_cgxx_cmrx_tx_stat9 {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat9_s {
+		u64 hist3                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat9_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT9(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT9(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x748 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr#_tx_stat_pri#_xoff
+ *
+ * CGX CMR TX XON to XOFF transition Registers
+ */
+union cavm_cgxx_cmrx_tx_stat_prix_xoff {
+	u64 u;
+	struct cavm_cgxx_cmrx_tx_stat_prix_xoff_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmrx_tx_stat_prix_xoff_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMRX_TX_STAT_PRIX_XOFF(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMRX_TX_STAT_PRIX_XOFF(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 7)))
+		return 0x800 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_bad
+ *
+ * CGX CMR Bad Registers
+ */
+union cavm_cgxx_cmr_bad {
+	u64 u;
+	struct cavm_cgxx_cmr_bad_s {
+		u64 rxb_nxl                          : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_cmr_bad_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_BAD
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_BAD
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1020;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_chan_msk_and
+ *
+ * CGX CMR Backpressure Channel Mask AND Registers
+ */
+union cavm_cgxx_cmr_chan_msk_and {
+	u64 u;
+	struct cavm_cgxx_cmr_chan_msk_and_s {
+		u64 msk_and                          : 64;
+	} s;
+	/* struct cavm_cgxx_cmr_chan_msk_and_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_CHAN_MSK_AND
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_CHAN_MSK_AND
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x110;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_chan_msk_or
+ *
+ * CGX Backpressure Channel Mask OR Registers
+ */
+union cavm_cgxx_cmr_chan_msk_or {
+	u64 u;
+	struct cavm_cgxx_cmr_chan_msk_or_s {
+		u64 msk_or                           : 64;
+	} s;
+	/* struct cavm_cgxx_cmr_chan_msk_or_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_CHAN_MSK_OR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_CHAN_MSK_OR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x118;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_eco
+ *
+ * INTERNAL: CGX ECO Registers
+ */
+union cavm_cgxx_cmr_eco {
+	u64 u;
+	struct cavm_cgxx_cmr_eco_s {
+		u64 eco_rw                           : 32;
+		u64 eco_ro                           : 32;
+	} s;
+	/* struct cavm_cgxx_cmr_eco_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_ECO
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_ECO
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1028;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_global_config
+ *
+ * CGX CMR Global Configuration Register
+ * These registers configure the global CMR, PCS, and MAC.
+ */
+union cavm_cgxx_cmr_global_config {
+	u64 u;
+	struct cavm_cgxx_cmr_global_config_s {
+		u64 pmux_sds_sel                     : 1;
+		u64 cgx_clk_enable                   : 1;
+		u64 cmr_x2p_reset                    : 3;
+		u64 interleave_mode                  : 1;
+		u64 fcs_strip                        : 1;
+		u64 ncsi_lmac_id                     : 2;
+		u64 cmr_ncsi_drop                    : 1;
+		u64 cmr_ncsi_reset                   : 1;
+		u64 cmr_ncsi_tag_cnt                 : 13;
+		u64 cmr_clken_ovrd                   : 1;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_cgxx_cmr_global_config_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_GLOBAL_CONFIG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_GLOBAL_CONFIG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 8;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_mem_int
+ *
+ * CGX CMR Memory Interrupt Register
+ */
+union cavm_cgxx_cmr_mem_int {
+	u64 u;
+	struct cavm_cgxx_cmr_mem_int_s {
+		u64 gmp_in_overfl                    : 1;
+		u64 smu_in_overfl                    : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_cmr_mem_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_MEM_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_MEM_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_mem_int_ena_w1c
+ *
+ * CGX CMR Memory Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_cmr_mem_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_cmr_mem_int_ena_w1c_s {
+		u64 gmp_in_overfl                    : 1;
+		u64 smu_in_overfl                    : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_cmr_mem_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_MEM_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_MEM_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x20;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_mem_int_ena_w1s
+ *
+ * CGX CMR Memory Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_cmr_mem_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_cmr_mem_int_ena_w1s_s {
+		u64 gmp_in_overfl                    : 1;
+		u64 smu_in_overfl                    : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_cmr_mem_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_MEM_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_MEM_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_mem_int_w1s
+ *
+ * CGX CMR Memory Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_cmr_mem_int_w1s {
+	u64 u;
+	struct cavm_cgxx_cmr_mem_int_w1s_s {
+		u64 gmp_in_overfl                    : 1;
+		u64 smu_in_overfl                    : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_cmr_mem_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_MEM_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_MEM_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x18;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_nic_nxc_adr
+ *
+ * CGX CMR NIC NXC Exception Registers
+ */
+union cavm_cgxx_cmr_nic_nxc_adr {
+	u64 u;
+	struct cavm_cgxx_cmr_nic_nxc_adr_s {
+		u64 channel                          : 12;
+		u64 lmac_id                          : 4;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_cmr_nic_nxc_adr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_NIC_NXC_ADR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_NIC_NXC_ADR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1030;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_nix0_nxc_adr
+ *
+ * CGX CMR NIX0 NXC Exception Registers
+ */
+union cavm_cgxx_cmr_nix0_nxc_adr {
+	u64 u;
+	struct cavm_cgxx_cmr_nix0_nxc_adr_s {
+		u64 channel                          : 12;
+		u64 lmac_id                          : 4;
+		u64 channel_e                        : 12;
+		u64 lmac_e_id                        : 4;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_cmr_nix0_nxc_adr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_NIX0_NXC_ADR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_NIX0_NXC_ADR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1038;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_nix1_nxc_adr
+ *
+ * CGX CMR NIX1 NXC Exception Registers
+ */
+union cavm_cgxx_cmr_nix1_nxc_adr {
+	u64 u;
+	struct cavm_cgxx_cmr_nix1_nxc_adr_s {
+		u64 channel                          : 12;
+		u64 lmac_id                          : 4;
+		u64 channel_e                        : 12;
+		u64 lmac_e_id                        : 4;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_cmr_nix1_nxc_adr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_NIX1_NXC_ADR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_NIX1_NXC_ADR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1040;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_p2x#_count
+ *
+ * CGX P2X Activity Register
+ */
+union cavm_cgxx_cmr_p2xx_count {
+	u64 u;
+	struct cavm_cgxx_cmr_p2xx_count_s {
+		u64 p2x_cnt                          : 64;
+	} s;
+	/* struct cavm_cgxx_cmr_p2xx_count_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_P2XX_COUNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_P2XX_COUNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 2))
+		return 0x168 + 0x1000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_dmac#_cam0
+ *
+ * CGX CMR Receive CAM Registers
+ * These registers provide access to the 32 DMAC CAM0 entries in CGX,
+ * for use by X2P/NIX bound traffic.
+ */
+union cavm_cgxx_cmr_rx_dmacx_cam0 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_dmacx_cam0_s {
+		u64 adr                              : 48;
+		u64 en                               : 1;
+		u64 id                               : 2;
+		u64 reserved_51_63                   : 13;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_dmacx_cam0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_DMACX_CAM0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_DMACX_CAM0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 31))
+		return 0x200 + 8 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_dmac#_cam1
+ *
+ * CGX CMR Receive CAM Registers
+ * These registers provide access to the 32 DMAC CAM entries in CGX for use by NCSI bound
+ * traffic. See CGX()_CMR_GLOBAL_CONFIG[NCSI_LMAC_ID] and
+ * CGX()_CMR_RX_STEERING1() registers.
+ */
+union cavm_cgxx_cmr_rx_dmacx_cam1 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_dmacx_cam1_s {
+		u64 adr                              : 48;
+		u64 en                               : 1;
+		u64 id                               : 2;
+		u64 reserved_51_63                   : 13;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_dmacx_cam1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_DMACX_CAM1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_DMACX_CAM1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 31))
+		return 0x400 + 8 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_lmacs
+ *
+ * CGX CMR Receive Logical MACs Registers
+ */
+union cavm_cgxx_cmr_rx_lmacs {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_lmacs_s {
+		u64 lmacs                            : 3;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_lmacs_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_LMACS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_LMACS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x128;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_ovr_bp
+ *
+ * CGX CMR Receive-Ports Backpressure Override Registers
+ * Per-LMAC backpressure override register.
+ * For SMU, CGX()_CMR_RX_OVR_BP[EN]\<0\> must be set to one and CGX()_CMR_RX_OVR_BP[BP]\<0\> must be
+ * cleared to zero (to forcibly disable hardware-automatic 802.3 PAUSE packet generation) with
+ * the HiGig2 Protocol when CGX()_SMU()_HG2_CONTROL[HG2TX_EN]=0. (The HiGig2 protocol is
+ * indicated by CGX()_SMU()_TX_CTL[HG_EN]=1 and CGX()_SMU()_RX_UDD_SKP[LEN]=16).
+ * Hardware can only auto-generate backpressure through HiGig2 messages (optionally, when
+ * CGX()_SMU()_HG2_CONTROL[HG2TX_EN]=1) with the HiGig2 protocol.
+ */
+union cavm_cgxx_cmr_rx_ovr_bp {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_ovr_bp_s {
+		u64 ign_fifo_bp                      : 4;
+		u64 bp                               : 4;
+		u64 en                               : 4;
+		u64 reserved_12_63                   : 52;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_ovr_bp_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_OVR_BP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_OVR_BP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x130;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_stat10
+ *
+ * CGX Receive Status Register 10
+ * These registers provide a count of octets of filtered DMAC1 or VLAN STEERING1 packets.
+ */
+union cavm_cgxx_cmr_rx_stat10 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_stat10_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_stat10_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STAT10
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STAT10
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc0;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_stat11
+ *
+ * CGX Receive Status Register 11
+ * This registers provides a count of packets dropped at the NCSI interface. This
+ * includes drops due to CGX()_CMR_GLOBAL_CONFIG[CMR_NCSI_DROP] or NCSI FIFO full.
+ * The count of dropped NCSI packets is not accounted for in any other stats
+ * registers.
+ */
+union cavm_cgxx_cmr_rx_stat11 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_stat11_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_stat11_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STAT11
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STAT11
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc8;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_stat12
+ *
+ * CGX Receive Status Register 12
+ * This register provide a count of octets of dropped at the NCSI interface.
+ */
+union cavm_cgxx_cmr_rx_stat12 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_stat12_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_stat12_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STAT12
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STAT12
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xd0;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_stat9
+ *
+ * CGX Receive Status Register 9
+ * These registers provide a count of all received packets that were dropped by the DMAC1 or VLAN
+ * STEERING1 filter.
+ * Packets that are dropped by the DMAC1 or VLAN STEERING1 filters are counted here
+ * regardless of whether they were ERR packets, but does not include those reported in
+ * CGX()_CMR()_RX_STAT6.
+ * 16B packets or smaller (20B in case of FCS strip) as the result of truncation
+ * or other means are not dropped by CGX (unless filter and decision is also
+ * asserted) and will never appear in this count.
+ * Should the MAC signal to the CMR that the packet be filtered upon decision before the end of
+ * packet, then STAT9 and STAT10 will not be updated.
+ */
+union cavm_cgxx_cmr_rx_stat9 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_stat9_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_stat9_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STAT9
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STAT9
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xb8;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_steering0#
+ *
+ * CGX CMR Receive Steering0 Registers
+ * These registers, along with CGX()_CMR_RX_STEERING_VETYPE0(), provide eight filters for
+ * identifying and steering receive traffic to X2P/NIX.
+ * Received packets are only passed to X2P/NIX when the DMAC0 filter result is
+ * ACCEPT and STEERING0 filter result is PASS. See also
+ * CGX()_CMR()_RX_DMAC_CTL0.
+ *
+ * Internal:
+ * "* ALGORITHM
+ * \<pre\>
+ * rx_steering(uint48 pkt_dmac, uint16 pkt_etype, uint16 pkt_vlan_id) {
+ *    for (int i = 0; i \< 8; i++) {
+ *       steer = CGX()_CMR_RX_STEERING0(i);
+ *       vetype = CGX()_CMR_RX_STEERING_VETYPE0(i);
+ *       if (steer[MCST_EN] || steer[DMAC_EN] || vetype[VLAN_EN] || vetype[VLAN_TAG_EN]) {
+ *          // Filter is enabled.
+ *          if (   (!steer[MCST_EN] || is_mcst(pkt_dmac))
+ *              && (!steer[DMAC_EN] || pkt_dmac == steer[DMAC])
+ *              && (!vetype[VLAN_EN] || pkt_vlan_id == vetype[VLAN_ID])
+ *              && (!vetype[VLAN_TAG_EN] || pkt_etype == vetype[VLAN_ETYPE]) )
+ *          {
+ *             // Filter match (all enabled matching criteria are met).
+ *             return steer[PASS];
+ *          }
+ *       }
+ *    }
+ *    return CGX()_CMR_RX_STEERING_DEFAULT0[PASS]; // No match
+ * }
+ * \</pre\>"
+ */
+union cavm_cgxx_cmr_rx_steering0x {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_steering0x_s {
+		u64 dmac                             : 48;
+		u64 dmac_en                          : 1;
+		u64 mcst_en                          : 1;
+		u64 pass                             : 1;
+		u64 reserved_51_63                   : 13;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_steering0x_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STEERING0X(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STEERING0X(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 7))
+		return 0x300 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_steering1#
+ *
+ * CGX CMR Receive Steering1 Registers
+ * These registers, along with CGX()_CMR_RX_STEERING_VETYPE1(), provide eight filters for
+ * identifying and steering NCSI receive traffic.
+ * Received packets are only passed to NCSI when the DMAC1 filter result is ACCEPT and
+ * STEERING1 filter result is PASS. See also CGX()_CMR_RX_DMAC()_CAM1 and
+ * CGX()_CMR_RX_STEERING1().
+ * For use with the LMAC associated with NCSI. See CGX()_CMR_GLOBAL_CONFIG[NCSI_LMAC_ID].
+ *
+ * Internal:
+ * ALGORITHM: See CGX()_CMR_RX_STEERING0().
+ */
+union cavm_cgxx_cmr_rx_steering1x {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_steering1x_s {
+		u64 dmac                             : 48;
+		u64 dmac_en                          : 1;
+		u64 mcst_en                          : 1;
+		u64 pass                             : 1;
+		u64 reserved_51_63                   : 13;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_steering1x_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STEERING1X(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STEERING1X(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 7))
+		return 0x500 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_steering_default0
+ *
+ * CGX CMR Receive Steering Default0 Destination Register
+ * For determining destination of traffic that does not meet matching algorithm
+ * described in registers CGX()_CMR_RX_STEERING0() and CGX()_CMR_RX_STEERING_VETYPE0().
+ * All 16B packets or smaller (20B in case of FCS strip) as the result of truncation will steer
+ * to default destination
+ */
+union cavm_cgxx_cmr_rx_steering_default0 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_steering_default0_s {
+		u64 pass                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_steering_default0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_DEFAULT0
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_DEFAULT0
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3f0;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_steering_default1
+ *
+ * CGX CMR Receive Steering Default1 Destination Register
+ * For use with the lmac_id associated with NCSI. See CGX()_CMR_GLOBAL_CONFIG[NCSI_LMAC_ID].
+ * For determining destination of traffic that does not meet matching algorithm
+ * described in registers CGX()_CMR_RX_STEERING1() and CGX()_CMR_RX_STEERING_VETYPE1().
+ * All 16B packets or smaller (20B in case of FCS strip) as the result of truncation will steer
+ * to default destination
+ */
+union cavm_cgxx_cmr_rx_steering_default1 {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_steering_default1_s {
+		u64 pass                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_steering_default1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_DEFAULT1
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_DEFAULT1
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x5e0;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_steering_vetype0#
+ *
+ * CGX CMR Receive VLAN Ethertype1 Register
+ * These registers, along with CGX()_CMR_RX_STEERING0(), provide eight filters for identifying
+ * and steering X2P/NIX receive traffic.
+ */
+union cavm_cgxx_cmr_rx_steering_vetype0x {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_steering_vetype0x_s {
+		u64 vlan_etype                       : 16;
+		u64 vlan_tag_en                      : 1;
+		u64 vlan_id                          : 12;
+		u64 vlan_en                          : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_steering_vetype0x_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_VETYPE0X(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_VETYPE0X(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 7))
+		return 0x380 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_rx_steering_vetype1#
+ *
+ * CGX CMR Receive VLAN Ethertype1 Register
+ * For use with the lmac_id associated with NCSI. See CGX()_CMR_GLOBAL_CONFIG[NCSI_LMAC_ID].
+ * These registers, along with CGX()_CMR_RX_STEERING1(), provide eight filters for identifying
+ * and steering NCSI receive traffic.
+ */
+union cavm_cgxx_cmr_rx_steering_vetype1x {
+	u64 u;
+	struct cavm_cgxx_cmr_rx_steering_vetype1x_s {
+		u64 vlan_etype                       : 16;
+		u64 vlan_tag_en                      : 1;
+		u64 vlan_id                          : 12;
+		u64 vlan_en                          : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_cgxx_cmr_rx_steering_vetype1x_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_VETYPE1X(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_RX_STEERING_VETYPE1X(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 7))
+		return 0x580 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_tx_lmacs
+ *
+ * CGX CMR Transmit Logical MACs Registers
+ * This register sets the number of LMACs allowed on the TX interface. The value is important for
+ * defining the partitioning of the transmit FIFO.
+ */
+union cavm_cgxx_cmr_tx_lmacs {
+	u64 u;
+	struct cavm_cgxx_cmr_tx_lmacs_s {
+		u64 lmacs                            : 3;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_cgxx_cmr_tx_lmacs_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_TX_LMACS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_TX_LMACS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1000;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_cmr_x2p#_count
+ *
+ * CGX X2P Activity Register
+ */
+union cavm_cgxx_cmr_x2px_count {
+	u64 u;
+	struct cavm_cgxx_cmr_x2px_count_s {
+		u64 x2p_cnt                          : 64;
+	} s;
+	/* struct cavm_cgxx_cmr_x2px_count_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CMR_X2PX_COUNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CMR_X2PX_COUNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 2))
+		return 0x170 + 0x1000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_const
+ *
+ * CGX CONST Registers
+ * This register contains constants for software discovery.
+ */
+union cavm_cgxx_const {
+	u64 u;
+	struct cavm_cgxx_const_s {
+		u64 tx_fifosz                        : 24;
+		u64 lmacs                            : 8;
+		u64 rx_fifosz                        : 24;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_cgxx_const_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2000;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_const1
+ *
+ * CGX CONST1 Registers
+ * This register contains constants for software discovery.
+ */
+union cavm_cgxx_const1 {
+	u64 u;
+	struct cavm_cgxx_const1_s {
+		u64 types                            : 11;
+		u64 res_types                        : 21;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_const1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_CONST1
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_CONST1
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2008;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_rx_wol_ctrl0
+ *
+ * CGX GMP GMI RX Wake-on-LAN Control 0 Registers
+ */
+union cavm_cgxx_gmp_gmix_rx_wol_ctrl0 {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_rx_wol_ctrl0_s {
+		u64 dmac                             : 48;
+		u64 pswd_len                         : 4;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_rx_wol_ctrl0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_RX_WOL_CTRL0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_RX_WOL_CTRL0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38a00 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_rx_wol_ctrl1
+ *
+ * CGX GMP GMI RX Wake-on-LAN Control 1 Registers
+ */
+union cavm_cgxx_gmp_gmix_rx_wol_ctrl1 {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_rx_wol_ctrl1_s {
+		u64 pswd                             : 64;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_rx_wol_ctrl1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_RX_WOL_CTRL1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_RX_WOL_CTRL1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38a08 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_tx_eee
+ *
+ * INTERNAL: CGX GMP GMI TX EEE Configure Registers
+ *
+ * Reserved.
+ * Internal:
+ * These registers control when GMP GMI TX requests to enter or exist LPI. Those
+ * registers take effect only when EEE is supported and enabled for a given LMAC.
+ */
+union cavm_cgxx_gmp_gmix_tx_eee {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_tx_eee_s {
+		u64 idle_thresh                      : 28;
+		u64 reserved_28                      : 1;
+		u64 force_lpi                        : 1;
+		u64 wakeup                           : 1;
+		u64 auto_lpi                         : 1;
+		u64 idle_cnt                         : 28;
+		u64 tx_lpi                           : 1;
+		u64 tx_lpi_wait                      : 1;
+		u64 sync_status_lpi_enable           : 1;
+		u64 reserved_63                      : 1;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_tx_eee_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_TX_EEE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_TX_EEE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38800 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_tx_eee_cfg1
+ *
+ * INTERNAL: CGX GMP GMI TX EEE Configure More Configuration Registers
+ *
+ * Reserved.
+ * Internal:
+ * Controls the GMP exiting of LPI and starting to send data.
+ */
+union cavm_cgxx_gmp_gmix_tx_eee_cfg1 {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_tx_eee_cfg1_s {
+		u64 wake2data_time                   : 24;
+		u64 reserved_24_35                   : 12;
+		u64 tx_eee_enable                    : 1;
+		u64 reserved_37_39                   : 3;
+		u64 sync2lpi_time                    : 21;
+		u64 reserved_61_63                   : 3;
+	} s;
+	struct cavm_cgxx_gmp_gmix_tx_eee_cfg1_cn {
+		u64 wake2data_time                   : 24;
+		u64 reserved_24_31                   : 8;
+		u64 reserved_32_35                   : 4;
+		u64 tx_eee_enable                    : 1;
+		u64 reserved_37_39                   : 3;
+		u64 sync2lpi_time                    : 21;
+		u64 reserved_61_63                   : 3;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_TX_EEE_CFG1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_TX_EEE_CFG1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38808 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_wol_int
+ *
+ * CGX GMP GMI RX WOL Interrupt Registers
+ * These registers allow WOL interrupts to be sent to the control processor.
+ */
+union cavm_cgxx_gmp_gmix_wol_int {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_wol_int_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_wol_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38a80 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_wol_int_ena_w1c
+ *
+ * CGX GMP GMI RX WOL Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_gmp_gmix_wol_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_wol_int_ena_w1c_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_wol_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38a90 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_wol_int_ena_w1s
+ *
+ * CGX GMP GMI RX WOL Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_gmp_gmix_wol_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_wol_int_ena_w1s_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_wol_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38a98 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi#_wol_int_w1s
+ *
+ * CGX GMP GMI RX WOL Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_gmp_gmix_wol_int_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_gmix_wol_int_w1s_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_gmp_gmix_wol_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMIX_WOL_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38a88 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_prt#_cfg
+ *
+ * CGX GMP GMI LMAC Configuration Registers
+ * This register controls the configuration of the LMAC.
+ */
+union cavm_cgxx_gmp_gmi_prtx_cfg {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_prtx_cfg_s {
+		u64 reserved_0                       : 1;
+		u64 speed                            : 1;
+		u64 duplex                           : 1;
+		u64 slottime                         : 1;
+		u64 reserved_4_7                     : 4;
+		u64 speed_msb                        : 1;
+		u64 reserved_9_11                    : 3;
+		u64 rx_idle                          : 1;
+		u64 tx_idle                          : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_prtx_cfg_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_PRTX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_PRTX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38020 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_decision
+ *
+ * CGX GMP Packet-Decision Registers
+ * This register specifies the byte count used to determine when to accept or to filter a packet.
+ * As each byte in a packet is received by GMI, the L2 byte count is compared against
+ * [CNT]. In normal operation, the L2 header begins after the
+ * PREAMBLE + SFD (CGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] = 1) and any optional UDD skip
+ * data (CGX()_GMP_GMI_RX()_UDD_SKP[LEN]).
+ *
+ * Internal:
+ * Notes:
+ * As each byte in a packet is received by GMI, the L2 byte count is compared
+ * against the [CNT].  The L2 byte count is the number of bytes
+ * from the beginning of the L2 header (DMAC).  In normal operation, the L2
+ * header begins after the PREAMBLE+SFD (CGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK]=1) and any
+ * optional UDD skip data (CGX()_GMP_GMI_RX()_UDD_SKP[LEN]).
+ * When CGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] is clear, PREAMBLE+SFD are prepended to the
+ * packet and would require UDD skip length to account for them.
+ *
+ * Full Duplex:
+ * _   L2 Size \<  [CNT] - Accept packet. No filtering is applied.
+ * _   L2 Size \>= [CNT] - Apply filter. Accept packet based on PAUSE packet filter.
+ *
+ * Half Duplex:
+ * _   L2 Size \<  [CNT] - Drop packet. Packet is unconditionally dropped.
+ * _   L2 Size \>= [CNT] - Accept packet.
+ *
+ * where L2_size = MAX(0, total_packet_size - CGX()_GMP_GMI_RX()_UDD_SKP[LEN] -
+ *                        ((CGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK]==1)*8))
+ */
+union cavm_cgxx_gmp_gmi_rxx_decision {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_decision_s {
+		u64 cnt                              : 5;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_rxx_decision_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_DECISION(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_DECISION(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38040 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_frm_chk
+ *
+ * CGX GMP Frame Check Registers
+ */
+union cavm_cgxx_gmp_gmi_rxx_frm_chk {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_frm_chk_s {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 reserved_2                       : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 reserved_5_6                     : 2;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_rxx_frm_chk_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_FRM_CHK(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_FRM_CHK(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38030 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_frm_ctl
+ *
+ * CGX GMP Frame Control Registers
+ * This register controls the handling of the frames.
+ * The [CTL_BCK] and [CTL_DRP] bits control how the hardware handles incoming PAUSE packets. The
+ * most
+ * common modes of operation:
+ * _ [CTL_BCK] = 1, [CTL_DRP] = 1: hardware handles everything.
+ * _ [CTL_BCK] = 0, [CTL_DRP] = 0: software sees all PAUSE frames.
+ * _ [CTL_BCK] = 0, [CTL_DRP] = 1: all PAUSE frames are completely ignored.
+ *
+ * These control bits should be set to [CTL_BCK] = 0, [CTL_DRP] = 0 in half-duplex mode. Since
+ * PAUSE
+ * packets only apply to full duplex operation, any PAUSE packet would constitute an exception
+ * which should be handled by the processing cores. PAUSE packets should not be forwarded.
+ *
+ * Internal:
+ * Notes:
+ * [PRE_STRP]:
+ * When [PRE_CHK] is set (indicating that the PREAMBLE will be sent), [PRE_STRP]
+ * determines if the PREAMBLE+SFD bytes are thrown away or sent to the Octane
+ * core as part of the packet.
+ * In either mode, the PREAMBLE+SFD bytes are not counted toward the packet
+ * size when checking against the MIN and MAX bounds.  Furthermore, the bytes
+ * are skipped when locating the start of the L2 header for DMAC and Control
+ * frame recognition.
+ */
+union cavm_cgxx_gmp_gmi_rxx_frm_ctl {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_frm_ctl_s {
+		u64 pre_chk                          : 1;
+		u64 pre_strp                         : 1;
+		u64 ctl_drp                          : 1;
+		u64 ctl_bck                          : 1;
+		u64 ctl_mcst                         : 1;
+		u64 ctl_smac                         : 1;
+		u64 pre_free                         : 1;
+		u64 reserved_7_8                     : 2;
+		u64 pre_align                        : 1;
+		u64 null_dis                         : 1;
+		u64 reserved_11                      : 1;
+		u64 ptp_mode                         : 1;
+		u64 rx_fc_type                       : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	struct cavm_cgxx_gmp_gmi_rxx_frm_ctl_cn {
+		u64 pre_chk                          : 1;
+		u64 pre_strp                         : 1;
+		u64 ctl_drp                          : 1;
+		u64 ctl_bck                          : 1;
+		u64 ctl_mcst                         : 1;
+		u64 ctl_smac                         : 1;
+		u64 pre_free                         : 1;
+		u64 reserved_7                       : 1;
+		u64 reserved_8                       : 1;
+		u64 pre_align                        : 1;
+		u64 null_dis                         : 1;
+		u64 reserved_11                      : 1;
+		u64 ptp_mode                         : 1;
+		u64 rx_fc_type                       : 1;
+		u64 reserved_14_63                   : 50;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_FRM_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_FRM_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38028 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_ifg
+ *
+ * CGX GMI Minimum Interframe-Gap Cycles Registers
+ * This register specifies the minimum number of interframe-gap (IFG) cycles between packets.
+ */
+union cavm_cgxx_gmp_gmi_rxx_ifg {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_ifg_s {
+		u64 ifg                              : 4;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_rxx_ifg_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_IFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_IFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38058 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_int
+ *
+ * CGX GMP GMI RX Interrupt Registers
+ * These registers allow interrupts to be sent to the control processor.
+ * * Exception conditions \<10:0\> can also set the rcv/opcode in the received packet's work-queue
+ * entry. CGX()_GMP_GMI_RX()_FRM_CHK provides a bit mask for configuring which conditions
+ * set the error.
+ * In half duplex operation, the expectation is that collisions will appear as either MINERR or
+ * CAREXT errors.
+ *
+ * Internal:
+ * Notes:
+ * (1) exception conditions 10:0 can also set the rcv/opcode in the received
+ * packet's workQ entry.  The CGX()_GMP_GMI_RX()_FRM_CHK register provides a bit mask
+ * for configuring which conditions set the error.
+ *
+ * (2) in half duplex operation, the expectation is that collisions will appear
+ * as either MINERR o r CAREXT errors.
+ *
+ * (3) JABBER An RX jabber error indicates that a packet was received which
+ * is longer than the maximum allowed packet as defined by the
+ * system.  GMI will truncate the packet at the JABBER count.
+ * Failure to do so could lead to system instabilty.
+ *
+ * (4) NIBERR This error is illegal at 1000Mbs speeds
+ * (CGX()_GMP_GMI_PRT()_CFG[SPEED]==0) and will never assert.
+ *
+ * (5) MINERR total frame DA+SA+TL+DATA+PAD+FCS \< 64
+ *
+ * (6) ALNERR Indicates that the packet received was not an integer number of
+ * bytes.  If FCS checking is enabled, ALNERR will only assert if
+ * the FCS is bad.  If FCS checking is disabled, ALNERR will
+ * assert in all non-integer frame cases.
+ *
+ * (7) Collisions Collisions can only occur in half-duplex mode.  A collision
+ * is assumed by the receiver when the slottime
+ * (CGX()_GMP_GMI_PRT()_CFG[SLOTTIME]) is not satisfied.  In 10/100 mode,
+ * this will result in a frame \< SLOTTIME.  In 1000 mode, it
+ * could result either in frame \< SLOTTIME or a carrier extend
+ * error with the SLOTTIME.  These conditions are visible by...
+ * . transfer ended before slottime COLDET
+ * . carrier extend error           CAREXT
+ *
+ * (A) LENERR Length errors occur when the received packet does not match the
+ * length field.  LENERR is only checked for packets between 64
+ * and 1500 bytes.  For untagged frames, the length must exact
+ * match.  For tagged frames the length or length+4 must match.
+ *
+ * (B) PCTERR checks that the frame begins with a valid PREAMBLE sequence.
+ * Does not check the number of PREAMBLE cycles.
+ *
+ * (C) OVRERR *DON'T PUT IN HRM*
+ * OVRERR is an architectural assertion check internal to GMI to
+ * make sure no assumption was violated.  In a correctly operating
+ * system, this interrupt can never fire.
+ * GMI has an internal arbiter which selects which of four ports to
+ * buffer in the main RX FIFO.  If we normally buffer eight bytes,
+ * then each port will typically push a tick every eight cycles if
+ * the packet interface is going as fast as possible.  If there
+ * are four ports, they push every two cycles.  So that's the
+ * assumption.  That the inbound module will always be able to
+ * consume the tick before another is produced.  If that doesn't
+ * happen that's when OVRERR will assert."
+ */
+union cavm_cgxx_gmp_gmi_rxx_int {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_int_s {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_63                   : 52;
+	} s;
+	struct cavm_cgxx_gmp_gmi_rxx_int_cn {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_15                   : 4;
+		u64 reserved_16_63                   : 48;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38000 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_int_ena_w1c
+ *
+ * CGX GMP GMI RX Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_gmp_gmi_rxx_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_int_ena_w1c_s {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_63                   : 52;
+	} s;
+	struct cavm_cgxx_gmp_gmi_rxx_int_ena_w1c_cn {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_15                   : 4;
+		u64 reserved_16_63                   : 48;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38010 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_int_ena_w1s
+ *
+ * CGX GMP GMI RX Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_gmp_gmi_rxx_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_int_ena_w1s_s {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_63                   : 52;
+	} s;
+	struct cavm_cgxx_gmp_gmi_rxx_int_ena_w1s_cn {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_15                   : 4;
+		u64 reserved_16_63                   : 48;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38018 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_int_w1s
+ *
+ * CGX GMP GMI RX Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_gmp_gmi_rxx_int_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_int_w1s_s {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_63                   : 52;
+	} s;
+	struct cavm_cgxx_gmp_gmi_rxx_int_w1s_cn {
+		u64 minerr                           : 1;
+		u64 carext                           : 1;
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 ovrerr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 falerr                           : 1;
+		u64 coldet                           : 1;
+		u64 ifgerr                           : 1;
+		u64 reserved_12_15                   : 4;
+		u64 reserved_16_63                   : 48;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38008 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_jabber
+ *
+ * CGX GMP Maximum Packet-Size Registers
+ * This register specifies the maximum size for packets, beyond which the GMI truncates.
+ */
+union cavm_cgxx_gmp_gmi_rxx_jabber {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_jabber_s {
+		u64 cnt                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_rxx_jabber_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_JABBER(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_JABBER(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38038 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_rx#_udd_skp
+ *
+ * CGX GMP GMI User-Defined Data Skip Registers
+ * This register specifies the amount of user-defined data (UDD) added before the start of the
+ * L2C data.
+ *
+ * Internal:
+ * Notes:
+ * (1) The skip bytes are part of the packet and will be handled by NIX.
+ *
+ * (2) The system can determine if the UDD bytes are included in the FCS check
+ * by using the FCSSEL field - if the FCS check is enabled.
+ *
+ * (3) Assume that the preamble/sfd is always at the start of the frame - even
+ * before UDD bytes.  In most cases, there will be no preamble in these
+ * cases since it will be packet interface in direct communication to
+ * another packet interface (MAC to MAC) without a PHY involved.
+ *
+ * (4) We can still do address filtering and control packet filtering is the
+ * user desires.
+ *
+ * (5) CGX()_GMP_GMI_RX()_UDD_SKP[LEN] must be 0 in half-duplex operation unless
+ * CGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] is clear.  If CGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] is
+ * clear,
+ * then CGX()_GMP_GMI_RX()_UDD_SKP[LEN] will normally be 8.
+ *
+ * (6) In all cases, the UDD bytes will be sent down the packet interface as
+ * part of the packet.  The UDD bytes are never stripped from the actual
+ * packet.
+ */
+union cavm_cgxx_gmp_gmi_rxx_udd_skp {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_rxx_udd_skp_s {
+		u64 len                              : 7;
+		u64 reserved_7                       : 1;
+		u64 fcssel                           : 1;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_rxx_udd_skp_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_UDD_SKP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_RXX_UDD_SKP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38048 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_smac#
+ *
+ * CGX GMI SMAC Registers
+ */
+union cavm_cgxx_gmp_gmi_smacx {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_smacx_s {
+		u64 smac                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_smacx_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_SMACX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_SMACX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38230 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_append
+ *
+ * CGX GMI TX Append Control Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_append {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_append_s {
+		u64 preamble                         : 1;
+		u64 pad                              : 1;
+		u64 fcs                              : 1;
+		u64 force_fcs                        : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_append_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_APPEND(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_APPEND(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38218 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_burst
+ *
+ * CGX GMI TX Burst-Counter Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_burst {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_burst_s {
+		u64 burst                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_burst_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_BURST(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_BURST(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38228 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_ctl
+ *
+ * CGX GMI Transmit Control Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_ctl {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_ctl_s {
+		u64 xscol_en                         : 1;
+		u64 xsdef_en                         : 1;
+		u64 tx_fc_type                       : 1;
+		u64 link_drain                       : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38270 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_int
+ *
+ * CGX GMI TX Interrupt Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_int {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_int_s {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	struct cavm_cgxx_gmp_gmi_txx_int_cn {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_7                     : 3;
+		u64 reserved_8                       : 1;
+		u64 reserved_9_63                    : 55;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38500 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_int_ena_w1c
+ *
+ * CGX GMI TX Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_gmp_gmi_txx_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_int_ena_w1c_s {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	struct cavm_cgxx_gmp_gmi_txx_int_ena_w1c_cn {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_7                     : 3;
+		u64 reserved_8                       : 1;
+		u64 reserved_9_63                    : 55;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38510 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_int_ena_w1s
+ *
+ * CGX GMI TX Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_gmp_gmi_txx_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_int_ena_w1s_s {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	struct cavm_cgxx_gmp_gmi_txx_int_ena_w1s_cn {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_7                     : 3;
+		u64 reserved_8                       : 1;
+		u64 reserved_9_63                    : 55;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38518 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_int_w1s
+ *
+ * CGX GMI TX Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_gmp_gmi_txx_int_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_int_w1s_s {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	struct cavm_cgxx_gmp_gmi_txx_int_w1s_cn {
+		u64 undflw                           : 1;
+		u64 xscol                            : 1;
+		u64 xsdef                            : 1;
+		u64 late_col                         : 1;
+		u64 ptp_lost                         : 1;
+		u64 reserved_5_7                     : 3;
+		u64 reserved_8                       : 1;
+		u64 reserved_9_63                    : 55;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38508 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_min_pkt
+ *
+ * CGX GMI TX Minimum-Size-Packet Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_min_pkt {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_min_pkt_s {
+		u64 min_size                         : 8;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_min_pkt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_MIN_PKT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_MIN_PKT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38240 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_pause_pkt_interval
+ *
+ * CGX GMI TX PAUSE-Packet Transmission-Interval Registers
+ * This register specifies how often PAUSE packets are sent.
+ * Internal:
+ * Notes:
+ * Choosing proper values of CGX()_GMP_GMI_TX()_PAUSE_PKT_TIME[PTIME] and
+ * CGX()_GMP_GMI_TX()_PAUSE_PKT_INTERVAL[INTERVAL] can be challenging to the system
+ * designer.  It is suggested that TIME be much greater than INTERVAL and
+ * CGX()_GMP_GMI_TX()_PAUSE_ZERO[SEND] be set.  This allows a periodic refresh of the PAUSE
+ * count and then when the backpressure condition is lifted, a PAUSE packet
+ * with TIME==0 will be sent indicating that Octane is ready for additional
+ * data.
+ *
+ * If the system chooses to not set CGX()_GMP_GMI_TX()_PAUSE_ZERO[SEND], then it is
+ * suggested that TIME and INTERVAL are programmed such that they satisify the
+ * following rule:
+ *
+ * _ INTERVAL \<= TIME - (largest_pkt_size + IFG + pause_pkt_size)
+ *
+ * where largest_pkt_size is that largest packet that the system can send
+ * (normally 1518B), IFG is the interframe gap and pause_pkt_size is the size
+ * of the PAUSE packet (normally 64B).
+ */
+union cavm_cgxx_gmp_gmi_txx_pause_pkt_interval {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_pause_pkt_interval_s {
+		u64 interval                         : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_pause_pkt_interval_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_PKT_INTERVAL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_PKT_INTERVAL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38248 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_pause_pkt_time
+ *
+ * CGX GMI TX PAUSE Packet PAUSE-Time Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_pause_pkt_time {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_pause_pkt_time_s {
+		u64 ptime                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_pause_pkt_time_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_PKT_TIME(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_PKT_TIME(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38238 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_pause_togo
+ *
+ * CGX GMI TX Time-to-Backpressure Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_pause_togo {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_pause_togo_s {
+		u64 ptime                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_pause_togo_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_TOGO(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_TOGO(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38258 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_pause_zero
+ *
+ * CGX GMI TX PAUSE-Zero-Enable Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_pause_zero {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_pause_zero_s {
+		u64 send                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_pause_zero_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_ZERO(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_PAUSE_ZERO(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38260 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_sgmii_ctl
+ *
+ * CGX SGMII Control Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_sgmii_ctl {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_sgmii_ctl_s {
+		u64 align                            : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_sgmii_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_SGMII_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_SGMII_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38300 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_slot
+ *
+ * CGX GMI TX Slottime Counter Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_slot {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_slot_s {
+		u64 slot                             : 10;
+		u64 reserved_10_63                   : 54;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_slot_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_SLOT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_SLOT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38220 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_soft_pause
+ *
+ * CGX GMI TX Software PAUSE Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_soft_pause {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_soft_pause_s {
+		u64 ptime                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_soft_pause_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_SOFT_PAUSE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_SOFT_PAUSE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38250 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx#_thresh
+ *
+ * CGX GMI TX Threshold Registers
+ */
+union cavm_cgxx_gmp_gmi_txx_thresh {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_txx_thresh_s {
+		u64 cnt                              : 11;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_txx_thresh_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_THRESH(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TXX_THRESH(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x38210 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx_col_attempt
+ *
+ * CGX TX Collision Attempts Before Dropping Frame Registers
+ */
+union cavm_cgxx_gmp_gmi_tx_col_attempt {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_tx_col_attempt_s {
+		u64 limit                            : 5;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_tx_col_attempt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TX_COL_ATTEMPT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TX_COL_ATTEMPT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x39010;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx_ifg
+ *
+ * CGX GMI TX Interframe-Gap Cycles Registers
+ * Consider the following when programming IFG1 and IFG2:
+ * * For 10/100/1000 Mb/s half-duplex systems that require IEEE 802.3 compatibility, IFG1 must be
+ * in the range of 1-8, [IFG2] must be in the range of 4-12, and the [IFG1] + [IFG2] sum must be 12.
+ * * For 10/100/1000 Mb/s full-duplex systems that require IEEE 802.3 compatibility, IFG1 must be
+ * in the range of 1-11, [IFG2] must be in the range of 1-11, and the [IFG1] + [IFG2] sum must be 12.
+ * For all other systems, IFG1 and IFG2 can be any value in the range of 1-15, allowing for a
+ * total possible IFG sum of 2-30.
+ */
+union cavm_cgxx_gmp_gmi_tx_ifg {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_tx_ifg_s {
+		u64 ifg1                             : 4;
+		u64 ifg2                             : 4;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_tx_ifg_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TX_IFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TX_IFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x39000;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx_jam
+ *
+ * CGX GMI TX JAM Pattern Registers
+ * This register provides the pattern used in JAM bytes.
+ */
+union cavm_cgxx_gmp_gmi_tx_jam {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_tx_jam_s {
+		u64 jam                              : 8;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_tx_jam_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TX_JAM
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TX_JAM
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x39008;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx_lfsr
+ *
+ * CGX GMI TX LFSR Registers
+ * This register shows the contents of the linear feedback shift register (LFSR), which is used
+ * to implement truncated binary exponential backoff.
+ */
+union cavm_cgxx_gmp_gmi_tx_lfsr {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_tx_lfsr_s {
+		u64 lfsr                             : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_tx_lfsr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TX_LFSR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TX_LFSR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x39028;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx_pause_pkt_dmac
+ *
+ * CGX TX PAUSE-Packet DMAC-Field Registers
+ */
+union cavm_cgxx_gmp_gmi_tx_pause_pkt_dmac {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_tx_pause_pkt_dmac_s {
+		u64 dmac                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_tx_pause_pkt_dmac_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TX_PAUSE_PKT_DMAC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TX_PAUSE_PKT_DMAC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x39018;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_gmi_tx_pause_pkt_type
+ *
+ * CGX GMI TX PAUSE-Packet-PTYPE Field Registers
+ * This register provides the PTYPE field that is placed in outbound PAUSE packets.
+ */
+union cavm_cgxx_gmp_gmi_tx_pause_pkt_type {
+	u64 u;
+	struct cavm_cgxx_gmp_gmi_tx_pause_pkt_type_s {
+		u64 ptype                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_gmi_tx_pause_pkt_type_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_GMI_TX_PAUSE_PKT_TYPE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_GMI_TX_PAUSE_PKT_TYPE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x39020;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_misc#_cfg
+ *
+ * CGX GMP PCS Miscellaneous Control Registers
+ * This register contains general configuration that should not need to be changed from reset
+ * settings.
+ *
+ * Internal:
+ * Per lmac diagnostic and chicken bits.
+ */
+union cavm_cgxx_gmp_miscx_cfg {
+	u64 u;
+	struct cavm_cgxx_gmp_miscx_cfg_s {
+		u64 tx_eee_quiet_credit_mode         : 1;
+		u64 tx_eee_wait_gmi_fast_idle        : 1;
+		u64 tx_qsgmii_port0_init             : 1;
+		u64 tx_eee_rx_sync_status_enable     : 1;
+		u64 pcs_alt_an                       : 1;
+		u64 reserved_5_7                     : 3;
+		u64 rx_pcs_sync_signal_detect        : 1;
+		u64 rx_pcs_sync_timeout              : 1;
+		u64 rx_pcs_eee_mode_enable           : 1;
+		u64 rx_pcs_lpi_enable                : 1;
+		u64 rx_pcs_802_rx_k                  : 1;
+		u64 rx_pcs_alt_qlb2i                 : 1;
+		u64 reserved_14_15                   : 2;
+		u64 rx_cgp_gser_throttle             : 1;
+		u64 rx_cgp_edet_filter               : 1;
+		u64 rx_cgp_edet_qlm_val              : 1;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_cgxx_gmp_miscx_cfg_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_MISCX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_MISCX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x34000 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_an_expansion
+ *
+ * CGX GMP PCS AN Expansion register Register 6
+ * AN status
+ */
+union cavm_cgxx_gmp_pcsx_an_expansion {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_an_expansion_s {
+		u64 reserved_0                       : 1;
+		u64 page_received                    : 1;
+		u64 next_page_able                   : 1;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_an_expansion_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_AN_EXPANSION(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_AN_EXPANSION(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30a60 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_an_lp_abil_np
+ *
+ * CGX GMP PCS AN Link Partner Ability Next Page Register 8
+ * This register contains the advertised ability of the link partners Next Page. The definition
+ * for this register is provided in 32.5.4.2 for changes to 28.2.4.1.4.
+ */
+union cavm_cgxx_gmp_pcsx_an_lp_abil_np {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_an_lp_abil_np_s {
+		u64 m_u                              : 11;
+		u64 toggle                           : 1;
+		u64 ack2                             : 1;
+		u64 mp                               : 1;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_an_lp_abil_np_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_AN_LP_ABIL_NP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_AN_LP_ABIL_NP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30a80 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_an_np_tx
+ *
+ * CGX GMP PCS AN Next Page Transmit Register 7
+ * Software programs this register with the contents of the AN message next page or unformatted
+ * next page link code word to be transmitted during autonegotiation. Next page exchange occurs
+ * after the base link code words have been exchanged if either end of the link segment sets the
+ * NP bit to 1, indicating that it has at least one next page to send. Once initiated, next page
+ * exchange continues until both ends of the link segment set their NP bits to 0.
+ * Both sides must be NP capable to use NP exchanges.
+ */
+union cavm_cgxx_gmp_pcsx_an_np_tx {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_an_np_tx_s {
+		u64 m_u                              : 11;
+		u64 toggle                           : 1;
+		u64 ack2                             : 1;
+		u64 mp                               : 1;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_an_np_tx_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_AN_NP_TX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_AN_NP_TX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30a70 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_dbg_control
+ *
+ * CGX PCS Debug Control Registers
+ */
+union cavm_cgxx_gmp_pcsx_dbg_control {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_dbg_control_s {
+		u64 us_clk_period                    : 7;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_dbg_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_DBG_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_DBG_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x31000 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_rx_eee_wake
+ *
+ * INTERNAL: CGX GMP PCS  RX EEE Wake Error Counter  Registers
+ *
+ * Reserved.
+ * Internal:
+ * This register is used by PHY types that support EEE to count wake time faults where
+ * the PHY fails to complete its normal wake sequence within the time required for the
+ * specific PHY type. The definition of the fault event to be counted is defined for
+ * each PHY and may occur during a refresh or a wake-up as defined by the PHY. This
+ * 16-bit counter shall be reset to all zeros upon execution of the PCS reset. This
+ * counter shall be held at all ones in the case of overflow.
+ */
+union cavm_cgxx_gmp_pcsx_rx_eee_wake {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_rx_eee_wake_s {
+		u64 error_counter                    : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_rx_eee_wake_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_RX_EEE_WAKE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_RX_EEE_WAKE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30910 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_rx_lpi_timing
+ *
+ * INTERNAL: CGX GMP PCS  RX EEE LPI Timing Parameters Registers
+ *
+ * Reserved.
+ * Internal:
+ * Receiver LPI timing parameters Tqr, Twr and Twtf.
+ */
+union cavm_cgxx_gmp_pcsx_rx_lpi_timing {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_rx_lpi_timing_s {
+		u64 twtf                             : 18;
+		u64 reserved_18_19                   : 2;
+		u64 twr                              : 12;
+		u64 tqr                              : 20;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_rx_lpi_timing_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_RX_LPI_TIMING(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_RX_LPI_TIMING(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30900 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_status1
+ *
+ * CGX GMP PCS Status 1 Register
+ * PCS LPI Status, Link OK.  Register 3.1
+ */
+union cavm_cgxx_gmp_pcsx_status1 {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_status1_s {
+		u64 reserved_0_1                     : 2;
+		u64 receive_link_status              : 1;
+		u64 reserved_3_7                     : 5;
+		u64 rx_lpi_indication                : 1;
+		u64 tx_lpi_indication                : 1;
+		u64 rx_lpi_received                  : 1;
+		u64 tx_lpi_received                  : 1;
+		u64 reserved_12_63                   : 52;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_status1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_STATUS1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_STATUS1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30880 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs#_tx_lpi_timing
+ *
+ * INTERNAL: CGX GMP GMI  TX EEE LPI Timing Parameters Registers
+ *
+ * Reserved.
+ * Internal:
+ * Transmitter LPI timing parameters Tsl, Tql and Tul.
+ */
+union cavm_cgxx_gmp_pcsx_tx_lpi_timing {
+	u64 u;
+	struct cavm_cgxx_gmp_pcsx_tx_lpi_timing_s {
+		u64 tql                              : 19;
+		u64 reserved_19_31                   : 13;
+		u64 tul                              : 12;
+		u64 reserved_44_47                   : 4;
+		u64 tsl                              : 12;
+		u64 reserved_60_63                   : 4;
+	} s;
+	/* struct cavm_cgxx_gmp_pcsx_tx_lpi_timing_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCSX_TX_LPI_TIMING(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCSX_TX_LPI_TIMING(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30800 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_an#_adv
+ *
+ * CGX GMP PCS Autonegotiation Advertisement Registers
+ */
+union cavm_cgxx_gmp_pcs_anx_adv {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_anx_adv_s {
+		u64 reserved_0_4                     : 5;
+		u64 fd                               : 1;
+		u64 hfd                              : 1;
+		u64 pause                            : 2;
+		u64 reserved_9_11                    : 3;
+		u64 rem_flt                          : 2;
+		u64 reserved_14                      : 1;
+		u64 np                               : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_anx_adv_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_ADV(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_ADV(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30010 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_an#_ext_st
+ *
+ * CGX GMO PCS Autonegotiation Extended Status Registers
+ */
+union cavm_cgxx_gmp_pcs_anx_ext_st {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_anx_ext_st_s {
+		u64 reserved_0_11                    : 12;
+		u64 thou_thd                         : 1;
+		u64 thou_tfd                         : 1;
+		u64 thou_xhd                         : 1;
+		u64 thou_xfd                         : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_anx_ext_st_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_EXT_ST(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_EXT_ST(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30028 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_an#_lp_abil
+ *
+ * CGX GMP PCS Autonegotiation Link Partner Ability Registers
+ * This is the autonegotiation link partner ability register 5 as per IEEE 802.3, Clause 37.
+ */
+union cavm_cgxx_gmp_pcs_anx_lp_abil {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_anx_lp_abil_s {
+		u64 reserved_0_4                     : 5;
+		u64 fd                               : 1;
+		u64 hfd                              : 1;
+		u64 pause                            : 2;
+		u64 reserved_9_11                    : 3;
+		u64 rem_flt                          : 2;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_anx_lp_abil_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_LP_ABIL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_LP_ABIL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30018 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_an#_results
+ *
+ * CGX GMP PCS Autonegotiation Results Registers
+ * This register is not valid when CGX()_GMP_PCS_MISC()_CTL[AN_OVRD] is set to 1. If
+ * CGX()_GMP_PCS_MISC()_CTL[AN_OVRD] is set to 0 and
+ * CGX()_GMP_PCS_AN()_RESULTS[AN_CPT] is set to 1, this register is valid.
+ */
+union cavm_cgxx_gmp_pcs_anx_results {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_anx_results_s {
+		u64 link_ok                          : 1;
+		u64 dup                              : 1;
+		u64 an_cpt                           : 1;
+		u64 spd                              : 2;
+		u64 pause                            : 2;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_anx_results_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_RESULTS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_ANX_RESULTS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30020 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_int#
+ *
+ * CGX GMP PCS Interrupt Registers
+ */
+union cavm_cgxx_gmp_pcs_intx {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_intx_s {
+		u64 lnkspd                           : 1;
+		u64 xmit                             : 1;
+		u64 an_err                           : 1;
+		u64 txfifu                           : 1;
+		u64 txfifo                           : 1;
+		u64 txbad                            : 1;
+		u64 rxerr                            : 1;
+		u64 rxbad                            : 1;
+		u64 rxlock                           : 1;
+		u64 an_bad                           : 1;
+		u64 sync_bad                         : 1;
+		u64 dup                              : 1;
+		u64 dbg_sync                         : 1;
+		u64 reserved_13_15                   : 3;
+		u64 an_page_received                 : 1;
+		u64 an_complete                      : 1;
+		u64 reserved_18_19                   : 2;
+		u64 eee_tx_change                    : 1;
+		u64 eee_rx_change                    : 1;
+		u64 eee_rx_link_fail                 : 1;
+		u64 reserved_23_63                   : 41;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_intx_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_INTX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_INTX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30080 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_int#_ena_w1c
+ *
+ * CGX GMP PCS Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_gmp_pcs_intx_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_intx_ena_w1c_s {
+		u64 lnkspd                           : 1;
+		u64 xmit                             : 1;
+		u64 an_err                           : 1;
+		u64 txfifu                           : 1;
+		u64 txfifo                           : 1;
+		u64 txbad                            : 1;
+		u64 rxerr                            : 1;
+		u64 rxbad                            : 1;
+		u64 rxlock                           : 1;
+		u64 an_bad                           : 1;
+		u64 sync_bad                         : 1;
+		u64 dup                              : 1;
+		u64 dbg_sync                         : 1;
+		u64 reserved_13_15                   : 3;
+		u64 an_page_received                 : 1;
+		u64 an_complete                      : 1;
+		u64 reserved_18_19                   : 2;
+		u64 eee_tx_change                    : 1;
+		u64 eee_rx_change                    : 1;
+		u64 eee_rx_link_fail                 : 1;
+		u64 reserved_23_63                   : 41;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_intx_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_INTX_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_INTX_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30090 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_int#_ena_w1s
+ *
+ * CGX GMP PCS Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_gmp_pcs_intx_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_intx_ena_w1s_s {
+		u64 lnkspd                           : 1;
+		u64 xmit                             : 1;
+		u64 an_err                           : 1;
+		u64 txfifu                           : 1;
+		u64 txfifo                           : 1;
+		u64 txbad                            : 1;
+		u64 rxerr                            : 1;
+		u64 rxbad                            : 1;
+		u64 rxlock                           : 1;
+		u64 an_bad                           : 1;
+		u64 sync_bad                         : 1;
+		u64 dup                              : 1;
+		u64 dbg_sync                         : 1;
+		u64 reserved_13_15                   : 3;
+		u64 an_page_received                 : 1;
+		u64 an_complete                      : 1;
+		u64 reserved_18_19                   : 2;
+		u64 eee_tx_change                    : 1;
+		u64 eee_rx_change                    : 1;
+		u64 eee_rx_link_fail                 : 1;
+		u64 reserved_23_63                   : 41;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_intx_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_INTX_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_INTX_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30098 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_int#_w1s
+ *
+ * CGX GMP PCS Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_gmp_pcs_intx_w1s {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_intx_w1s_s {
+		u64 lnkspd                           : 1;
+		u64 xmit                             : 1;
+		u64 an_err                           : 1;
+		u64 txfifu                           : 1;
+		u64 txfifo                           : 1;
+		u64 txbad                            : 1;
+		u64 rxerr                            : 1;
+		u64 rxbad                            : 1;
+		u64 rxlock                           : 1;
+		u64 an_bad                           : 1;
+		u64 sync_bad                         : 1;
+		u64 dup                              : 1;
+		u64 dbg_sync                         : 1;
+		u64 reserved_13_15                   : 3;
+		u64 an_page_received                 : 1;
+		u64 an_complete                      : 1;
+		u64 reserved_18_19                   : 2;
+		u64 eee_tx_change                    : 1;
+		u64 eee_rx_change                    : 1;
+		u64 eee_rx_link_fail                 : 1;
+		u64 reserved_23_63                   : 41;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_intx_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_INTX_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_INTX_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30088 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_link#_timer
+ *
+ * CGX GMP PCS Link Timer Registers
+ * This is the 1.6 ms nominal link timer register.
+ */
+union cavm_cgxx_gmp_pcs_linkx_timer {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_linkx_timer_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_linkx_timer_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_LINKX_TIMER(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_LINKX_TIMER(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30040 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_misc#_ctl
+ *
+ * CGX GMP SGMII Miscellaneous Control Registers
+ * Internal:
+ * SGMII bit [12] is really a misnomer, it is a decode  of pi_qlm_cfg pins to indicate SGMII or
+ * 1000Base-X modes.
+ *
+ * Note: The SGMII AN Advertisement Register above will be sent during Auto Negotiation if
+ * [MAC_PHY] is set (1=PHY mode). If the bit is not set (0=MAC mode), the
+ * tx_Config_Reg\<14\> becomes ACK bit and tx_Config_Reg\<0\> is always 1.
+ * All other bits in tx_Config_Reg sent will be 0. The PHY dictates the Auto Negotiation results.
+ */
+union cavm_cgxx_gmp_pcs_miscx_ctl {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_miscx_ctl_s {
+		u64 samp_pt                          : 7;
+		u64 an_ovrd                          : 1;
+		u64 mode                             : 1;
+		u64 mac_phy                          : 1;
+		u64 loopbck2                         : 1;
+		u64 gmxeno                           : 1;
+		u64 reserved_12                      : 1;
+		u64 disp_en                          : 1;
+		u64 reserved_14_15                   : 2;
+		u64 qsgmii_comma_wd                  : 16;
+		u64 qsgmii_comma_wd_en               : 1;
+		u64 reserved_33_63                   : 31;
+	} s;
+	struct cavm_cgxx_gmp_pcs_miscx_ctl_cn {
+		u64 samp_pt                          : 7;
+		u64 an_ovrd                          : 1;
+		u64 mode                             : 1;
+		u64 mac_phy                          : 1;
+		u64 loopbck2                         : 1;
+		u64 gmxeno                           : 1;
+		u64 reserved_12                      : 1;
+		u64 disp_en                          : 1;
+		u64 reserved_14_15                   : 2;
+		u64 qsgmii_comma_wd                  : 16;
+		u64 qsgmii_comma_wd_en               : 1;
+		u64 reserved_33_35                   : 3;
+		u64 reserved_36_63                   : 28;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_MISCX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_MISCX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30078 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_mr#_control
+ *
+ * CGX GMP PCS Control Registers
+ */
+union cavm_cgxx_gmp_pcs_mrx_control {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_mrx_control_s {
+		u64 reserved_0_4                     : 5;
+		u64 uni                              : 1;
+		u64 spdmsb                           : 1;
+		u64 coltst                           : 1;
+		u64 dup                              : 1;
+		u64 rst_an                           : 1;
+		u64 reserved_10                      : 1;
+		u64 pwr_dn                           : 1;
+		u64 an_en                            : 1;
+		u64 spdlsb                           : 1;
+		u64 loopbck1                         : 1;
+		u64 reset                            : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_mrx_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_MRX_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_MRX_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30000 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_mr#_status
+ *
+ * CGX GMP PCS Status Registers
+ * Bits \<15:9\> in this register indicate the ability to operate when
+ * CGX()_GMP_PCS_MISC()_CTL[MAC_PHY] is set to MAC mode. Bits \<15:9\> are always read as
+ * 0, indicating that the chip cannot operate in the corresponding modes. The field [RM_FLT] is a
+ * 'don't care' when the selected mode is SGMII/QSGMII.
+ */
+union cavm_cgxx_gmp_pcs_mrx_status {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_mrx_status_s {
+		u64 extnd                            : 1;
+		u64 reserved_1                       : 1;
+		u64 lnk_st                           : 1;
+		u64 an_abil                          : 1;
+		u64 rm_flt                           : 1;
+		u64 an_cpt                           : 1;
+		u64 prb_sup                          : 1;
+		u64 reserved_7                       : 1;
+		u64 ext_st                           : 1;
+		u64 hun_t2hd                         : 1;
+		u64 hun_t2fd                         : 1;
+		u64 ten_hd                           : 1;
+		u64 ten_fd                           : 1;
+		u64 hun_xhd                          : 1;
+		u64 hun_xfd                          : 1;
+		u64 hun_t4                           : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_mrx_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_MRX_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_MRX_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30008 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_rx#_states
+ *
+ * CGX GMP PCS RX State-Machines States Registers
+ */
+union cavm_cgxx_gmp_pcs_rxx_states {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_rxx_states_s {
+		u64 an_st                            : 4;
+		u64 an_bad                           : 1;
+		u64 sync                             : 4;
+		u64 sync_bad                         : 1;
+		u64 rx_st                            : 5;
+		u64 rx_bad                           : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_rxx_states_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_RXX_STATES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_RXX_STATES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30058 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_rx#_sync
+ *
+ * CGX GMP PCS Code Group Synchronization Registers
+ */
+union cavm_cgxx_gmp_pcs_rxx_sync {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_rxx_sync_s {
+		u64 bit_lock                         : 1;
+		u64 sync                             : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_rxx_sync_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_RXX_SYNC(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_RXX_SYNC(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30050 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_sgm#_an_adv
+ *
+ * CGX GMP PCS SGMII Autonegotiation Advertisement Registers
+ * This is the SGMII autonegotiation advertisement register (sent out as tx_Config_Reg\<15:0\> as
+ * defined in IEEE 802.3 clause 37). This register is sent during autonegotiation if
+ * CGX()_GMP_PCS_MISC()_CTL[MAC_PHY] is set (1 = PHY mode). If the bit is not set (0 =
+ * MAC mode), then tx_Config_Reg\<14\> becomes ACK bit and tx_Config_Reg\<0\> is always 1. All other
+ * bits in tx_Config_Reg sent will be 0. The PHY dictates the autonegotiation results.
+ */
+union cavm_cgxx_gmp_pcs_sgmx_an_adv {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_sgmx_an_adv_s {
+		u64 one                              : 1;
+		u64 reserved_1_9                     : 9;
+		u64 speed                            : 2;
+		u64 dup                              : 1;
+		u64 reserved_13                      : 1;
+		u64 ack                              : 1;
+		u64 link                             : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_sgmx_an_adv_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_SGMX_AN_ADV(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_SGMX_AN_ADV(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30068 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_sgm#_lp_adv
+ *
+ * CGX GMP PCS SGMII Link-Partner-Advertisement Registers
+ * This is the SGMII link partner advertisement register (received as rx_Config_Reg\<15:0\> as
+ * defined in IEEE 802.3 clause 37).
+ */
+union cavm_cgxx_gmp_pcs_sgmx_lp_adv {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_sgmx_lp_adv_s {
+		u64 one                              : 1;
+		u64 reserved_1_9                     : 9;
+		u64 speed                            : 2;
+		u64 dup                              : 1;
+		u64 reserved_13_14                   : 2;
+		u64 link                             : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	struct cavm_cgxx_gmp_pcs_sgmx_lp_adv_cn {
+		u64 one                              : 1;
+		u64 reserved_1_9                     : 9;
+		u64 speed                            : 2;
+		u64 dup                              : 1;
+		u64 reserved_13                      : 1;
+		u64 reserved_14                      : 1;
+		u64 link                             : 1;
+		u64 reserved_16_63                   : 48;
+	} cn;
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_SGMX_LP_ADV(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_SGMX_LP_ADV(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30070 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_tx#_states
+ *
+ * CGX GMP PCS TX State-Machines States Registers
+ */
+union cavm_cgxx_gmp_pcs_txx_states {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_txx_states_s {
+		u64 ord_st                           : 4;
+		u64 tx_bad                           : 1;
+		u64 xmit                             : 2;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_txx_states_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_TXX_STATES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_TXX_STATES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30060 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_gmp_pcs_tx_rx#_polarity
+ *
+ * CGX GMP PCS TX/RX Polarity Registers
+ * CGX()_GMP_PCS_TX_RX()_POLARITY[AUTORXPL] shows correct polarity needed on the link
+ * receive path after code group synchronization is achieved.  When LMAC_TYPE=QSGMII, only lane 0
+ * polarity data and settings are relevant and settings for lanes 1, 2 and 3 are unused.
+ */
+union cavm_cgxx_gmp_pcs_tx_rxx_polarity {
+	u64 u;
+	struct cavm_cgxx_gmp_pcs_tx_rxx_polarity_s {
+		u64 txplrt                           : 1;
+		u64 rxplrt                           : 1;
+		u64 autorxpl                         : 1;
+		u64 rxovrd                           : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_gmp_pcs_tx_rxx_polarity_s cn; */
+};
+
+static inline u64 CAVM_CGXX_GMP_PCS_TX_RXX_POLARITY(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_GMP_PCS_TX_RXX_POLARITY(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x30048 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_msix_pba#
+ *
+ * CGX MSI-X Pending Bit Array Registers
+ * This register is the MSI-X PBA table, the bit number is indexed by the CGX_INT_VEC_E
+ * enumeration.
+ */
+union cavm_cgxx_msix_pbax {
+	u64 u;
+	struct cavm_cgxx_msix_pbax_s {
+		u64 pend                             : 64;
+	} s;
+	/* struct cavm_cgxx_msix_pbax_s cn; */
+};
+
+static inline u64 CAVM_CGXX_MSIX_PBAX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_MSIX_PBAX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0xf0000 + 8 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_msix_vec#_addr
+ *
+ * CGX MSI-X Vector Table Address Registers
+ * This register is the MSI-X vector table, indexed by the CGX_INT_VEC_E enumeration.
+ */
+union cavm_cgxx_msix_vecx_addr {
+	u64 u;
+	struct cavm_cgxx_msix_vecx_addr_s {
+		u64 secvec                           : 1;
+		u64 reserved_1                       : 1;
+		u64 addr                             : 51;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_cgxx_msix_vecx_addr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_MSIX_VECX_ADDR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_MSIX_VECX_ADDR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 37))
+		return 0 + 0x10 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_msix_vec#_ctl
+ *
+ * CGX MSI-X Vector Table Control and Data Registers
+ * This register is the MSI-X vector table, indexed by the CGX_INT_VEC_E enumeration.
+ */
+union cavm_cgxx_msix_vecx_ctl {
+	u64 u;
+	struct cavm_cgxx_msix_vecx_ctl_s {
+		u64 data                             : 32;
+		u64 mask                             : 1;
+		u64 reserved_33_63                   : 31;
+	} s;
+	/* struct cavm_cgxx_msix_vecx_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_MSIX_VECX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_MSIX_VECX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 37))
+		return 8 + 0x10 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_bp_test
+ *
+ * INTERNAL: CGX SMU TX Backpressure Test Registers
+ */
+union cavm_cgxx_smux_bp_test {
+	u64 u;
+	struct cavm_cgxx_smux_bp_test_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_47                   : 24;
+		u64 enable                           : 4;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_cgxx_smux_bp_test_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_BP_TEST(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_BP_TEST(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20230 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_cbfc_ctl
+ *
+ * CGX SMU PFC Control Registers
+ * Internal:
+ * INTERNAL: XOFF for a specific class/channel \<i\> is XOFF\<i\> = ([PHYS_EN]\<i\> & cmr_rx_phys_bp) |
+ * ([LOGL_EN]\<i\> & cmr_rx_logl_xoff\<i\>).
+ */
+union cavm_cgxx_smux_cbfc_ctl {
+	u64 u;
+	struct cavm_cgxx_smux_cbfc_ctl_s {
+		u64 rx_en                            : 1;
+		u64 tx_en                            : 1;
+		u64 drp_en                           : 1;
+		u64 bck_en                           : 1;
+		u64 reserved_4_31                    : 28;
+		u64 logl_en                          : 16;
+		u64 phys_en                          : 16;
+	} s;
+	/* struct cavm_cgxx_smux_cbfc_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_CBFC_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_CBFC_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20218 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_ctrl
+ *
+ * CGX SMU Control Registers
+ */
+union cavm_cgxx_smux_ctrl {
+	u64 u;
+	struct cavm_cgxx_smux_ctrl_s {
+		u64 rx_idle                          : 1;
+		u64 tx_idle                          : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_smux_ctrl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_CTRL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_CTRL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20200 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_ext_loopback
+ *
+ * CGX SMU External Loopback Registers
+ * In loopback mode, the IFG1+IFG2 of local and remote parties must match exactly; otherwise
+ * loopback FIFO will overrun: CGX()_SMU()_TX_INT[LB_OVRFLW].
+ */
+union cavm_cgxx_smux_ext_loopback {
+	u64 u;
+	struct cavm_cgxx_smux_ext_loopback_s {
+		u64 thresh                           : 6;
+		u64 reserved_6_7                     : 2;
+		u64 depth                            : 6;
+		u64 reserved_14_15                   : 2;
+		u64 en                               : 1;
+		u64 reserved_17_63                   : 47;
+	} s;
+	/* struct cavm_cgxx_smux_ext_loopback_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_EXT_LOOPBACK(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_EXT_LOOPBACK(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20208 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_hg2_control
+ *
+ * CGX SMU HiGig2 Control Registers
+ * HiGig2 TX- and RX-enable are normally set together for HiGig2 messaging. Setting just the TX
+ * or RX bit results in only the HG2 message transmit or receive capability.
+ *
+ * Setting [PHYS_EN] and [LOGL_EN] to 1 allows link PAUSE or backpressure to NIX as per the
+ * received HiGig2 message. Setting these fields to 0 disables link PAUSE and backpressure to NIX
+ * in response to received messages.
+ *
+ * CGX()_SMU()_TX_CTL[HG_EN] must be set (to enable HiGig) whenever either [HG2TX_EN] or
+ * [HG2RX_EN] are set. CGX()_SMU()_RX_UDD_SKP[LEN] must be set to 16 (to select HiGig2)
+ * whenever either [HG2TX_EN] or [HG2RX_EN] are set.
+ *
+ * CGX()_CMR_RX_OVR_BP[EN]\<0\> must be set and CGX()_CMR_RX_OVR_BP[BP]\<0\> must be cleared
+ * to 0 (to forcibly disable hardware-automatic 802.3 PAUSE packet generation) with the HiGig2
+ * Protocol when [HG2TX_EN] = 0. (The HiGig2 protocol is indicated
+ * by CGX()_SMU()_TX_CTL[HG_EN] = 1 and CGX()_SMU()_RX_UDD_SKP[LEN]=16.) Hardware
+ * can only autogenerate backpressure via HiGig2 messages (optionally, when [HG2TX_EN] = 1) with
+ * the HiGig2 protocol.
+ */
+union cavm_cgxx_smux_hg2_control {
+	u64 u;
+	struct cavm_cgxx_smux_hg2_control_s {
+		u64 logl_en                          : 16;
+		u64 phys_en                          : 1;
+		u64 hg2rx_en                         : 1;
+		u64 hg2tx_en                         : 1;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_cgxx_smux_hg2_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_HG2_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_HG2_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20210 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_mmsi_ctl_sta
+ *
+ * CGX SMU MAC Merge Service Interface (MMSI) Control/Status Registers
+ * MMSI control and status registers for frame preemption mode. Refer to IEEE 802.3br, Clause 99.
+ */
+union cavm_cgxx_smux_mmsi_ctl_sta {
+	u64 u;
+	struct cavm_cgxx_smux_mmsi_ctl_sta_s {
+		u64 p_en                             : 1;
+		u64 dis_v                            : 1;
+		u64 afs                              : 2;
+		u64 v_sta                            : 3;
+		u64 tx_pactive                       : 1;
+		u64 reserved_8_31                    : 24;
+		u64 v_time                           : 24;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_cgxx_smux_mmsi_ctl_sta_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_MMSI_CTL_STA(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_MMSI_CTL_STA(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20220 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_bad_col_ctrl
+ *
+ * CGX SMU RX Bad Column High Registers
+ */
+union cavm_cgxx_smux_rx_bad_col_ctrl {
+	u64 u;
+	struct cavm_cgxx_smux_rx_bad_col_ctrl_s {
+		u64 lane_rxc                         : 16;
+		u64 state                            : 3;
+		u64 val                              : 1;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_cgxx_smux_rx_bad_col_ctrl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_BAD_COL_CTRL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_BAD_COL_CTRL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20060 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_bad_col_data_hi
+ *
+ * CGX SMU RX Bad Column Low Registers
+ */
+union cavm_cgxx_smux_rx_bad_col_data_hi {
+	u64 u;
+	struct cavm_cgxx_smux_rx_bad_col_data_hi_s {
+		u64 lane_rxd                         : 64;
+	} s;
+	/* struct cavm_cgxx_smux_rx_bad_col_data_hi_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_BAD_COL_DATA_HI(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_BAD_COL_DATA_HI(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20058 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_bad_col_data_lo
+ *
+ * CGX SMU RX Bad Column Low Registers
+ */
+union cavm_cgxx_smux_rx_bad_col_data_lo {
+	u64 u;
+	struct cavm_cgxx_smux_rx_bad_col_data_lo_s {
+		u64 lane_rxd                         : 64;
+	} s;
+	/* struct cavm_cgxx_smux_rx_bad_col_data_lo_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_BAD_COL_DATA_LO(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_BAD_COL_DATA_LO(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20050 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_ctl
+ *
+ * CGX SMU RX Control Registers
+ */
+union cavm_cgxx_smux_rx_ctl {
+	u64 u;
+	struct cavm_cgxx_smux_rx_ctl_s {
+		u64 status                           : 2;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_smux_rx_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20048 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_decision
+ *
+ * CGX SMU Packet Decision Registers
+ * This register specifies the byte count used to determine when to accept or to filter a packet.
+ * As each byte in a packet is received by CGX, the L2 byte count (i.e. the number of bytes from
+ * the beginning of the L2 header (DMAC)) is compared against CNT. In normal operation, the L2
+ * header begins after the PREAMBLE + SFD (CGX()_SMU()_RX_FRM_CTL[PRE_CHK] = 1) and any
+ * optional UDD skip data (CGX()_SMU()_RX_UDD_SKP[LEN]).
+ */
+union cavm_cgxx_smux_rx_decision {
+	u64 u;
+	struct cavm_cgxx_smux_rx_decision_s {
+		u64 cnt                              : 5;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_cgxx_smux_rx_decision_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_DECISION(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_DECISION(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20038 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_frm_chk
+ *
+ * CGX SMU RX Frame Check Registers
+ * The CSRs provide the enable bits for a subset of errors passed to CMR encoded.
+ */
+union cavm_cgxx_smux_rx_frm_chk {
+	u64 u;
+	struct cavm_cgxx_smux_rx_frm_chk_s {
+		u64 reserved_0_2                     : 3;
+		u64 jabber                           : 1;
+		u64 fcserr_d                         : 1;
+		u64 fcserr_c                         : 1;
+		u64 reserved_6                       : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_cgxx_smux_rx_frm_chk_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_FRM_CHK(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_FRM_CHK(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20028 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_frm_ctl
+ *
+ * CGX SMU RX Frame Control Registers
+ * This register controls the handling of the frames.
+ * The [CTL_BCK] and [CTL_DRP] bits control how the hardware handles incoming PAUSE packets. The
+ * most
+ * common modes of operation:
+ * _ [CTL_BCK] = 1, [CTL_DRP] = 1: hardware handles everything
+ * _ [CTL_BCK] = 0, [CTL_DRP] = 0: software sees all PAUSE frames
+ * _ [CTL_BCK] = 0, [CTL_DRP] = 1: all PAUSE frames are completely ignored
+ *
+ * These control bits should be set to [CTL_BCK] = 0, [CTL_DRP] = 0 in half-duplex mode. Since
+ * PAUSE
+ * packets only apply to full duplex operation, any PAUSE packet would constitute an exception
+ * which should be handled by the processing cores. PAUSE packets should not be forwarded.
+ */
+union cavm_cgxx_smux_rx_frm_ctl {
+	u64 u;
+	struct cavm_cgxx_smux_rx_frm_ctl_s {
+		u64 pre_chk                          : 1;
+		u64 pre_strp                         : 1;
+		u64 ctl_drp                          : 1;
+		u64 ctl_bck                          : 1;
+		u64 ctl_mcst                         : 1;
+		u64 ctl_smac                         : 1;
+		u64 reserved_6_11                    : 6;
+		u64 ptp_mode                         : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_cgxx_smux_rx_frm_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_FRM_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_FRM_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20020 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_int
+ *
+ * CGX SMU Receive Interrupt Registers
+ * SMU Interrupt Register.
+ * Internal:
+ * Exception conditions \<9\> and \<4:0\> can also set the rcv/opcode in the received packet's work
+ * queue entry. CGX()_SMU()_RX_FRM_CHK provides a bit mask for configuring which
+ * conditions set the error.
+ */
+union cavm_cgxx_smux_rx_int {
+	u64 u;
+	struct cavm_cgxx_smux_rx_int_s {
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 loc_fault                        : 1;
+		u64 rem_fault                        : 1;
+		u64 bad_seq                          : 1;
+		u64 bad_term                         : 1;
+		u64 hg2fld                           : 1;
+		u64 hg2cc                            : 1;
+		u64 badver                           : 1;
+		u64 badrsp                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_smux_rx_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20000 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_int_ena_w1c
+ *
+ * CGX SMU Receive Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_smux_rx_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_smux_rx_int_ena_w1c_s {
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 loc_fault                        : 1;
+		u64 rem_fault                        : 1;
+		u64 bad_seq                          : 1;
+		u64 bad_term                         : 1;
+		u64 hg2fld                           : 1;
+		u64 hg2cc                            : 1;
+		u64 badver                           : 1;
+		u64 badrsp                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_smux_rx_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20010 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_int_ena_w1s
+ *
+ * CGX SMU Receive Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_smux_rx_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_smux_rx_int_ena_w1s_s {
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 loc_fault                        : 1;
+		u64 rem_fault                        : 1;
+		u64 bad_seq                          : 1;
+		u64 bad_term                         : 1;
+		u64 hg2fld                           : 1;
+		u64 hg2cc                            : 1;
+		u64 badver                           : 1;
+		u64 badrsp                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_smux_rx_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20018 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_int_w1s
+ *
+ * CGX SMU Receive Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_smux_rx_int_w1s {
+	u64 u;
+	struct cavm_cgxx_smux_rx_int_w1s_s {
+		u64 jabber                           : 1;
+		u64 fcserr                           : 1;
+		u64 rcverr                           : 1;
+		u64 skperr                           : 1;
+		u64 pcterr                           : 1;
+		u64 rsverr                           : 1;
+		u64 loc_fault                        : 1;
+		u64 rem_fault                        : 1;
+		u64 bad_seq                          : 1;
+		u64 bad_term                         : 1;
+		u64 hg2fld                           : 1;
+		u64 hg2cc                            : 1;
+		u64 badver                           : 1;
+		u64 badrsp                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_smux_rx_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20008 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_jabber
+ *
+ * CGX SMU Maximum Packet-Size Registers
+ * This register specifies the maximum size for packets, beyond which the SMU truncates.
+ * Internal:
+ * JABBER[CNT] is checked against the packet that arrives from SPU.  The checking
+ * is performed before preamble is stripped or PTP is inserted.  If present, preamble is
+ * counted as eight bytes of the incoming packet.
+ */
+union cavm_cgxx_smux_rx_jabber {
+	u64 u;
+	struct cavm_cgxx_smux_rx_jabber_s {
+		u64 cnt                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_smux_rx_jabber_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_JABBER(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_JABBER(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20030 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_udd_skp
+ *
+ * CGX SMU User-Defined Data Skip Registers
+ * Internal:
+ * (1) The skip bytes are part of the packet and will be sent down the NCB
+ * packet interface and will be handled by NIX.
+ *
+ * (2) The system can determine if the UDD bytes are included in the FCS check
+ * by using the FCSSEL field if the FCS check is enabled.
+ *
+ * (3) Assume that the preamble/sfd is always at the start of the frame even
+ * before UDD bytes.  In most cases, there will be no preamble in these
+ * cases since it will be packet interface in direct communication to
+ * another packet interface (MAC to MAC) without a PHY involved.
+ *
+ * (4) We can still do address filtering and control packet filtering if the
+ * user desires.
+ *
+ * (5) In all cases, the UDD bytes will be sent down the packet interface as
+ * part of the packet.  The UDD bytes are never stripped from the actual
+ * packet.
+ */
+union cavm_cgxx_smux_rx_udd_skp {
+	u64 u;
+	struct cavm_cgxx_smux_rx_udd_skp_s {
+		u64 len                              : 7;
+		u64 reserved_7                       : 1;
+		u64 fcssel                           : 1;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_cgxx_smux_rx_udd_skp_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_UDD_SKP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_UDD_SKP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20040 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_wol_ctrl0
+ *
+ * CGX SMU RX Wake-on-LAN Control 0 Registers
+ */
+union cavm_cgxx_smux_rx_wol_ctrl0 {
+	u64 u;
+	struct cavm_cgxx_smux_rx_wol_ctrl0_s {
+		u64 dmac                             : 48;
+		u64 pswd_len                         : 4;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_cgxx_smux_rx_wol_ctrl0_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_CTRL0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_CTRL0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20068 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_wol_ctrl1
+ *
+ * CGX SMU RX Wake-on-LAN Control 1 Registers
+ */
+union cavm_cgxx_smux_rx_wol_ctrl1 {
+	u64 u;
+	struct cavm_cgxx_smux_rx_wol_ctrl1_s {
+		u64 pswd                             : 64;
+	} s;
+	/* struct cavm_cgxx_smux_rx_wol_ctrl1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_CTRL1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_CTRL1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20070 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_wol_int
+ *
+ * CGX SMU RX WOL Interrupt Registers
+ * These registers allow WOL interrupts to be sent to the control processor.
+ */
+union cavm_cgxx_smux_rx_wol_int {
+	u64 u;
+	struct cavm_cgxx_smux_rx_wol_int_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_smux_rx_wol_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20078 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_wol_int_ena_w1c
+ *
+ * CGX SMU RX WOL Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_smux_rx_wol_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_smux_rx_wol_int_ena_w1c_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_smux_rx_wol_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20088 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_wol_int_ena_w1s
+ *
+ * CGX SMU RX WOL Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_smux_rx_wol_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_smux_rx_wol_int_ena_w1s_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_smux_rx_wol_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20090 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_rx_wol_int_w1s
+ *
+ * CGX SMU RX WOL Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_smux_rx_wol_int_w1s {
+	u64 u;
+	struct cavm_cgxx_smux_rx_wol_int_w1s_s {
+		u64 wol_rcvd                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_smux_rx_wol_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_RX_WOL_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20080 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_smac
+ *
+ * CGX SMU SMAC Registers
+ */
+union cavm_cgxx_smux_smac {
+	u64 u;
+	struct cavm_cgxx_smux_smac_s {
+		u64 smac                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_smux_smac_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_SMAC(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_SMAC(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20108 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_append
+ *
+ * CGX SMU TX Append Control Registers
+ * For more details on the interactions between FCS and PAD, see also the description of
+ * CGX()_SMU()_TX_MIN_PKT[MIN_SIZE].
+ */
+union cavm_cgxx_smux_tx_append {
+	u64 u;
+	struct cavm_cgxx_smux_tx_append_s {
+		u64 preamble                         : 1;
+		u64 pad                              : 1;
+		u64 fcs_d                            : 1;
+		u64 fcs_c                            : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_smux_tx_append_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_APPEND(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_APPEND(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20100 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_ctl
+ *
+ * CGX SMU Transmit Control Registers
+ */
+union cavm_cgxx_smux_tx_ctl {
+	u64 u;
+	struct cavm_cgxx_smux_tx_ctl_s {
+		u64 dic_en                           : 1;
+		u64 uni_en                           : 1;
+		u64 x4a_dis                          : 1;
+		u64 mia_en                           : 1;
+		u64 ls                               : 2;
+		u64 ls_byp                           : 1;
+		u64 l2p_bp_conv                      : 1;
+		u64 hg_en                            : 1;
+		u64 hg_pause_hgi                     : 2;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_cgxx_smux_tx_ctl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20178 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_dack
+ *
+ * CGX SMU TX Drop Counters Registers
+ */
+union cavm_cgxx_smux_tx_dack {
+	u64 u;
+	struct cavm_cgxx_smux_tx_dack_s {
+		u64 dpi_sdrop_ack                    : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_smux_tx_dack_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_DACK(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_DACK(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x201b0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_dcnt
+ *
+ * CGX SMU TX Drop Counters Registers
+ */
+union cavm_cgxx_smux_tx_dcnt {
+	u64 u;
+	struct cavm_cgxx_smux_tx_dcnt_s {
+		u64 dpi_sdrop_cnt                    : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_smux_tx_dcnt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_DCNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_DCNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x201a8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_eee
+ *
+ * INTERNAL: CGX SMU TX EEE Configure Registers
+ *
+ * Resvered.
+ * Internal:
+ * These registers control when SMU TX requests to enter or exist LPI. Those registers
+ * take effect only when EEE is supported and enabled for a given LMAC.
+ */
+union cavm_cgxx_smux_tx_eee {
+	u64 u;
+	struct cavm_cgxx_smux_tx_eee_s {
+		u64 idle_thresh                      : 28;
+		u64 reserved_28                      : 1;
+		u64 force_lpi                        : 1;
+		u64 wakeup                           : 1;
+		u64 auto_lpi                         : 1;
+		u64 idle_cnt                         : 28;
+		u64 reserved_60_61                   : 2;
+		u64 tx_lpi_wake                      : 1;
+		u64 tx_lpi                           : 1;
+	} s;
+	/* struct cavm_cgxx_smux_tx_eee_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_EEE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_EEE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20190 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_eee_timer_status
+ *
+ * INTERNAL: CGX SMU TX EEE TIMER STATUS Registers
+ *
+ * Reserved.
+ * Internal:
+ * These registers configure SMU TX EEE timing parameters.
+ */
+union cavm_cgxx_smux_tx_eee_timer_status {
+	u64 u;
+	struct cavm_cgxx_smux_tx_eee_timer_status_s {
+		u64 lpi_wake_cnt                     : 16;
+		u64 reserved_16_30                   : 15;
+		u64 wake_timer_done                  : 1;
+		u64 link_ok_cnt                      : 30;
+		u64 reserved_62                      : 1;
+		u64 link_timer_done                  : 1;
+	} s;
+	/* struct cavm_cgxx_smux_tx_eee_timer_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_EEE_TIMER_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_EEE_TIMER_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x201a0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_eee_timing
+ *
+ * INTERNAL: CGX SMU TX EEE TIMING Parameter Registers
+ *
+ * Reserved.
+ * Internal:
+ * These registers configure SMU TX EEE timing parameters.
+ */
+union cavm_cgxx_smux_tx_eee_timing {
+	u64 u;
+	struct cavm_cgxx_smux_tx_eee_timing_s {
+		u64 w_sys_tx_min                     : 16;
+		u64 reserved_16_31                   : 16;
+		u64 link_ok_min                      : 30;
+		u64 reserved_62_63                   : 2;
+	} s;
+	/* struct cavm_cgxx_smux_tx_eee_timing_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_EEE_TIMING(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_EEE_TIMING(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20198 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_ifg
+ *
+ * CGX SMU TX Interframe-Gap Cycles Registers
+ * Programming IFG1 and IFG2:
+ * * For XAUI/RXAUI/10G/25G/40G/50G/100G systems that require IEEE 802.3
+ * compatibility, the [IFG1]+[IFG2] sum must be 12.
+ * * In loopback mode, the [IFG1]+[IFG2] of local and remote parties must match exactly;
+ * otherwise loopback FIFO will overrun: CGX()_SMU()_TX_INT[LB_OVRFLW].
+ * * When CGX()_SMU()_TX_CTL[DIC_EN] is set, [IFG1]+[IFG2] sum must be at least 8.
+ * The behavior of smaller values is un-determined.
+ * * When CGX()_SMU()_TX_CTL[DIC_EN] is cleared, the minimum value of [IFG1]+[IFG2] is 1
+ * for 40G/50G/100G LMAC_TYPE configurations and 5 for all other values. The behavior
+ * of smaller values is un-determined.
+ *
+ * Internal:
+ * When CGX()_SMU()_TX_CTL[DIC_EN] is set, SMU TX treats ([IFG1]+[IFG2]) \< 8 as 8 for
+ * 40G/50G/100G MACs and ([IFG1]+[IFG2]) \< 8 as 8 for other MACs. When
+ * CGX()_SMU()_TX_CTL[DIC_EN] is cleared, SMU TX can work correctly with any IFG1 and
+ * IFG2.
+ */
+union cavm_cgxx_smux_tx_ifg {
+	u64 u;
+	struct cavm_cgxx_smux_tx_ifg_s {
+		u64 ifg1                             : 4;
+		u64 ifg2                             : 4;
+		u64 mia_amt                          : 2;
+		u64 reserved_10_15                   : 6;
+		u64 mia_cnt                          : 8;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_cgxx_smux_tx_ifg_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_IFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_IFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20160 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_int
+ *
+ * CGX SMU TX Interrupt Registers
+ */
+union cavm_cgxx_smux_tx_int {
+	u64 u;
+	struct cavm_cgxx_smux_tx_int_s {
+		u64 undflw                           : 1;
+		u64 xchange                          : 1;
+		u64 fake_commit                      : 1;
+		u64 lb_undflw                        : 1;
+		u64 lb_ovrflw                        : 1;
+		u64 dpi_sdrop                        : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_smux_tx_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20140 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_int_ena_w1c
+ *
+ * CGX SMU TX Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_smux_tx_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_smux_tx_int_ena_w1c_s {
+		u64 undflw                           : 1;
+		u64 xchange                          : 1;
+		u64 fake_commit                      : 1;
+		u64 lb_undflw                        : 1;
+		u64 lb_ovrflw                        : 1;
+		u64 dpi_sdrop                        : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_smux_tx_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20150 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_int_ena_w1s
+ *
+ * CGX SMU TX Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_smux_tx_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_smux_tx_int_ena_w1s_s {
+		u64 undflw                           : 1;
+		u64 xchange                          : 1;
+		u64 fake_commit                      : 1;
+		u64 lb_undflw                        : 1;
+		u64 lb_ovrflw                        : 1;
+		u64 dpi_sdrop                        : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_smux_tx_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20158 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_int_w1s
+ *
+ * CGX SMU TX Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_smux_tx_int_w1s {
+	u64 u;
+	struct cavm_cgxx_smux_tx_int_w1s_s {
+		u64 undflw                           : 1;
+		u64 xchange                          : 1;
+		u64 fake_commit                      : 1;
+		u64 lb_undflw                        : 1;
+		u64 lb_ovrflw                        : 1;
+		u64 dpi_sdrop                        : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_smux_tx_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20148 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_min_pkt
+ *
+ * CGX SMU TX Minimum-Size-Packet Registers
+ * Internal:
+ * [MIN_SIZE] less than 16 will be ignored by hardware which will use 16 instead.
+ */
+union cavm_cgxx_smux_tx_min_pkt {
+	u64 u;
+	struct cavm_cgxx_smux_tx_min_pkt_s {
+		u64 min_size                         : 8;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_cgxx_smux_tx_min_pkt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_MIN_PKT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_MIN_PKT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20118 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_pause_pkt_dmac
+ *
+ * CGX SMU TX PAUSE-Packet DMAC-Field Registers
+ * This register provides the DMAC value that is placed in outbound PAUSE packets.
+ */
+union cavm_cgxx_smux_tx_pause_pkt_dmac {
+	u64 u;
+	struct cavm_cgxx_smux_tx_pause_pkt_dmac_s {
+		u64 dmac                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_smux_tx_pause_pkt_dmac_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_DMAC(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_DMAC(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20168 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_pause_pkt_interval
+ *
+ * CGX SMU TX PAUSE-Packet Transmission-Interval Registers
+ * This register specifies how often PAUSE packets are sent.
+ */
+union cavm_cgxx_smux_tx_pause_pkt_interval {
+	u64 u;
+	struct cavm_cgxx_smux_tx_pause_pkt_interval_s {
+		u64 interval                         : 16;
+		u64 hg2_intra_interval               : 16;
+		u64 hg2_intra_en                     : 1;
+		u64 reserved_33_63                   : 31;
+	} s;
+	/* struct cavm_cgxx_smux_tx_pause_pkt_interval_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_INTERVAL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_INTERVAL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20120 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_pause_pkt_time
+ *
+ * CGX SMU TX PAUSE Packet Time Registers
+ */
+union cavm_cgxx_smux_tx_pause_pkt_time {
+	u64 u;
+	struct cavm_cgxx_smux_tx_pause_pkt_time_s {
+		u64 p_time                           : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_smux_tx_pause_pkt_time_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_TIME(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_TIME(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20110 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_pause_pkt_type
+ *
+ * CGX SMU TX PAUSE-Packet P_TYPE-Field Registers
+ * This register provides the P_TYPE field that is placed in outbound PAUSE packets.
+ */
+union cavm_cgxx_smux_tx_pause_pkt_type {
+	u64 u;
+	struct cavm_cgxx_smux_tx_pause_pkt_type_s {
+		u64 p_type                           : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_smux_tx_pause_pkt_type_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_TYPE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_PKT_TYPE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20170 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_pause_togo
+ *
+ * CGX SMU TX Time-to-Backpressure Registers
+ */
+union cavm_cgxx_smux_tx_pause_togo {
+	u64 u;
+	struct cavm_cgxx_smux_tx_pause_togo_s {
+		u64 p_time                           : 16;
+		u64 msg_time                         : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_smux_tx_pause_togo_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_TOGO(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_TOGO(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20130 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_pause_zero
+ *
+ * CGX SMU TX PAUSE Zero Registers
+ */
+union cavm_cgxx_smux_tx_pause_zero {
+	u64 u;
+	struct cavm_cgxx_smux_tx_pause_zero_s {
+		u64 send                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_smux_tx_pause_zero_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_ZERO(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_PAUSE_ZERO(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20138 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_soft_pause
+ *
+ * CGX SMU TX Soft PAUSE Registers
+ */
+union cavm_cgxx_smux_tx_soft_pause {
+	u64 u;
+	struct cavm_cgxx_smux_tx_soft_pause_s {
+		u64 p_time                           : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_smux_tx_soft_pause_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_SOFT_PAUSE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_SOFT_PAUSE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20128 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_smu#_tx_thresh
+ *
+ * CGX SMU TX Threshold Registers
+ */
+union cavm_cgxx_smux_tx_thresh {
+	u64 u;
+	struct cavm_cgxx_smux_tx_thresh_s {
+		u64 cnt                              : 12;
+		u64 reserved_12_15                   : 4;
+		u64 dpi_thresh                       : 5;
+		u64 reserved_21_23                   : 3;
+		u64 dpi_depth                        : 5;
+		u64 reserved_29_31                   : 3;
+		u64 ecnt                             : 12;
+		u64 reserved_44_63                   : 20;
+	} s;
+	/* struct cavm_cgxx_smux_tx_thresh_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SMUX_TX_THRESH(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SMUX_TX_THRESH(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x20180 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_adv
+ *
+ * CGX SPU Autonegotiation Advertisement Registers
+ * Software programs this register with the contents of the AN-link code word base page to be
+ * transmitted during autonegotiation. (See IEEE 802.3 section 73.6 for details.) Any write
+ * operations to this register prior to completion of autonegotiation, as indicated by
+ * CGX()_SPU()_AN_STATUS[AN_COMPLETE], should be followed by a renegotiation in order for
+ * the new values to take effect. Renegotiation is initiated by setting
+ * CGX()_SPU()_AN_CONTROL[AN_RESTART]. Once autonegotiation has completed, software can
+ * examine this register along with CGX()_SPU()_AN_LP_BASE to determine the highest
+ * common denominator technology.
+ */
+union cavm_cgxx_spux_an_adv {
+	u64 u;
+	struct cavm_cgxx_spux_an_adv_s {
+		u64 s                                : 5;
+		u64 e                                : 5;
+		u64 pause                            : 1;
+		u64 asm_dir                          : 1;
+		u64 xnp_able                         : 1;
+		u64 rf                               : 1;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 t                                : 5;
+		u64 a1g_kx                           : 1;
+		u64 a10g_kx4                         : 1;
+		u64 a10g_kr                          : 1;
+		u64 a40g_kr4                         : 1;
+		u64 a40g_cr4                         : 1;
+		u64 a100g_cr10                       : 1;
+		u64 a100g_kp4                        : 1;
+		u64 a100g_kr4                        : 1;
+		u64 a100g_cr4                        : 1;
+		u64 a25g_krs_crs                     : 1;
+		u64 a25g_kr_cr                       : 1;
+		u64 arsv                             : 12;
+		u64 a25g_rs_fec_req                  : 1;
+		u64 a25g_br_fec_req                  : 1;
+		u64 fec_able                         : 1;
+		u64 fec_req                          : 1;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_spux_an_adv_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_ADV(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_ADV(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10198 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_bp_status
+ *
+ * CGX SPU Autonegotiation Backplane Ethernet & BASE-R Copper Status Registers
+ * The contents of this register are updated during autonegotiation and are valid when
+ * CGX()_SPU()_AN_STATUS[AN_COMPLETE] is set. At that time, one of the port type bits will be
+ * set depending on the AN priority resolution. The port types are listed in order of decreasing
+ * priority. If a BASE-R type is negotiated then [FEC] or [RS_FEC] will be set to indicate
+ * whether/which FEC operation has been negotiated and will be clear otherwise.
+ */
+union cavm_cgxx_spux_an_bp_status {
+	u64 u;
+	struct cavm_cgxx_spux_an_bp_status_s {
+		u64 bp_an_able                       : 1;
+		u64 n1g_kx                           : 1;
+		u64 n10g_kx4                         : 1;
+		u64 n10g_kr                          : 1;
+		u64 n25g_krs_crs                     : 1;
+		u64 n25g_kr_cr                       : 1;
+		u64 n40g_kr4                         : 1;
+		u64 n40g_cr4                         : 1;
+		u64 n100g_cr10                       : 1;
+		u64 n100g_kp4                        : 1;
+		u64 n100g_kr4                        : 1;
+		u64 n100g_cr4                        : 1;
+		u64 fec                              : 1;
+		u64 rs_fec                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_spux_an_bp_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_BP_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_BP_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101b8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_control
+ *
+ * CGX SPU Autonegotiation Control Registers
+ */
+union cavm_cgxx_spux_an_control {
+	u64 u;
+	struct cavm_cgxx_spux_an_control_s {
+		u64 reserved_0_8                     : 9;
+		u64 an_restart                       : 1;
+		u64 reserved_10_11                   : 2;
+		u64 an_en                            : 1;
+		u64 xnp_en                           : 1;
+		u64 reserved_14                      : 1;
+		u64 an_reset                         : 1;
+		u64 an_arb_link_chk_en               : 1;
+		u64 usx_an_arb_link_chk_en           : 1;
+		u64 reserved_18_63                   : 46;
+	} s;
+	/* struct cavm_cgxx_spux_an_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10188 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_lp_base
+ *
+ * CGX SPU Autonegotiation Link-Partner Base-Page Ability Registers
+ * This register captures the contents of the latest AN link code word base page received from
+ * the link partner during autonegotiation. (See IEEE 802.3 section 73.6 for details.)
+ * CGX()_SPU()_AN_STATUS[PAGE_RX] is set when this register is updated by hardware.
+ */
+union cavm_cgxx_spux_an_lp_base {
+	u64 u;
+	struct cavm_cgxx_spux_an_lp_base_s {
+		u64 s                                : 5;
+		u64 e                                : 5;
+		u64 pause                            : 1;
+		u64 asm_dir                          : 1;
+		u64 xnp_able                         : 1;
+		u64 rf                               : 1;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 t                                : 5;
+		u64 a1g_kx                           : 1;
+		u64 a10g_kx4                         : 1;
+		u64 a10g_kr                          : 1;
+		u64 a40g_kr4                         : 1;
+		u64 a40g_cr4                         : 1;
+		u64 a100g_cr10                       : 1;
+		u64 a100g_kp4                        : 1;
+		u64 a100g_kr4                        : 1;
+		u64 a100g_cr4                        : 1;
+		u64 a25g_krs_crs                     : 1;
+		u64 a25g_kr_cr                       : 1;
+		u64 arsv                             : 12;
+		u64 a25g_rs_fec_req                  : 1;
+		u64 a25g_br_fec_req                  : 1;
+		u64 fec_able                         : 1;
+		u64 fec_req                          : 1;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_spux_an_lp_base_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_LP_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_LP_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101a0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_lp_xnp
+ *
+ * CGX SPU Autonegotiation Link Partner Extended Next Page Ability Registers
+ * This register captures the contents of the latest next page code word received from the link
+ * partner during autonegotiation, if any. See IEEE 802.3 section 73.7.7 for details.
+ */
+union cavm_cgxx_spux_an_lp_xnp {
+	u64 u;
+	struct cavm_cgxx_spux_an_lp_xnp_s {
+		u64 m_u                              : 11;
+		u64 toggle                           : 1;
+		u64 ack2                             : 1;
+		u64 mp                               : 1;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 u                                : 32;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_spux_an_lp_xnp_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_LP_XNP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_LP_XNP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101b0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_status
+ *
+ * CGX SPU Autonegotiation Status Registers
+ */
+union cavm_cgxx_spux_an_status {
+	u64 u;
+	struct cavm_cgxx_spux_an_status_s {
+		u64 lp_an_able                       : 1;
+		u64 reserved_1                       : 1;
+		u64 link_status                      : 1;
+		u64 an_able                          : 1;
+		u64 rmt_flt                          : 1;
+		u64 an_complete                      : 1;
+		u64 page_rx                          : 1;
+		u64 xnp_stat                         : 1;
+		u64 reserved_8                       : 1;
+		u64 prl_flt                          : 1;
+		u64 reserved_10_63                   : 54;
+	} s;
+	/* struct cavm_cgxx_spux_an_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10190 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_an_xnp_tx
+ *
+ * CGX SPU Autonegotiation Extended Next Page Transmit Registers
+ * Software programs this register with the contents of the AN message next page or unformatted
+ * next page link code word to be transmitted during autonegotiation. Next page exchange occurs
+ * after the base link code words have been exchanged if either end of the link segment sets the
+ * NP bit to 1, indicating that it has at least one next page to send. Once initiated, next page
+ * exchange continues until both ends of the link segment set their NP bits to 0. See IEEE
+ * 802.3 section 73.7.7 for details.
+ */
+union cavm_cgxx_spux_an_xnp_tx {
+	u64 u;
+	struct cavm_cgxx_spux_an_xnp_tx_s {
+		u64 m_u                              : 11;
+		u64 toggle                           : 1;
+		u64 ack2                             : 1;
+		u64 mp                               : 1;
+		u64 ack                              : 1;
+		u64 np                               : 1;
+		u64 u                                : 32;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_cgxx_spux_an_xnp_tx_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_AN_XNP_TX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_AN_XNP_TX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101a8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_algn_status
+ *
+ * CGX SPU Multilane BASE-R PCS Alignment-Status Registers
+ * This register implements the IEEE 802.3 multilane BASE-R PCS alignment status 1-4 registers
+ * (3.50-3.53). It is valid only when the LPCS type is 40GBASE-R, 50GBASE-R, 100GBASE-R,
+ * (CGX()_CMR()_CONFIG[LMAC_TYPE] = CGX_LMAC_TYPES_E::FORTYG_R,FIFTYG_R,HUNDREDG_R), and always
+ * returns 0x0 for all other LPCS
+ * types. Service interfaces (lanes) 19-0 (100G) and 3-0 (all others) are mapped to PCS lanes
+ * 19-0 or 3-0 via CGX()_SPU()_BR_LANE_MAP()[LN_MAPPING]. For 100G, logical lane 0 fans out to
+ * service interfaces 0-4, logical lane 1 fans out to service interfaces 5-9, ... etc. For all
+ * other modes, logical lanes and service interfaces are identical. Logical interfaces (lanes)
+ * map to SerDes lanes via CGX()_CMR()_CONFIG[LANE_TO_SDS] (programmable).
+ */
+union cavm_cgxx_spux_br_algn_status {
+	u64 u;
+	struct cavm_cgxx_spux_br_algn_status_s {
+		u64 block_lock                       : 20;
+		u64 reserved_20_29                   : 10;
+		u64 alignd                           : 1;
+		u64 reserved_31_40                   : 10;
+		u64 marker_lock                      : 20;
+		u64 reserved_61_63                   : 3;
+	} s;
+	/* struct cavm_cgxx_spux_br_algn_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_ALGN_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_ALGN_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10050 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_lane_map#
+ *
+ * CGX SPU 40,50,100GBASE-R Lane-Mapping Registers
+ * This register implements the IEEE 802.3 lane 0-19 mapping registers
+ * (3.400-3.403). It is valid only when the LPCS type is 40GBASE-R, 50GBASE-R,
+ * 100GBASE-R, USXGMII (CGX()_CMR()_CONFIG[LMAC_TYPE]), and always
+ * returns 0x0 for all other LPCS types. The LNx_MAPPING field for each programmed PCS
+ * lane (called service interface in 802.3) is valid when that lane has achieved
+ * alignment marker lock on the receive side (i.e. the associated
+ * CGX()_SPU()_BR_ALGN_STATUS[MARKER_LOCK] = 1), and is invalid otherwise. When valid,
+ * it returns the actual detected receive PCS lane number based on the received
+ * alignment marker contents received on that service interface.
+ *
+ * In RS-FEC mode the LNx_MAPPING field is valid when that lane has achieved alignment
+ * marker lock on the receive side (i.e. the associated
+ * CGX(0..2)_SPU(0..3)_RSFEC_STATUS[AMPS_LOCK] = 1), and is invalid otherwise. When valid,
+ * it returns the actual detected receive FEC lane number based on the received
+ * alignment marker contents received on that logical lane therefore expect for RS-FEC
+ * that LNx_MAPPING = x.
+ *
+ * The mapping is flexible because IEEE 802.3 allows multilane BASE-R receive lanes to
+ * be re-ordered. Note that for the transmit side, each logical lane is mapped to a
+ * physical SerDes lane based on the programming of
+ * CGX()_CMR()_CONFIG[LANE_TO_SDS]. For the receive side,
+ * CGX()_CMR()_CONFIG[LANE_TO_SDS] specifies the logical lane to physical SerDes
+ * lane mapping, and this register specifies the service interface (or lane) to PCS
+ * lane mapping.
+ */
+union cavm_cgxx_spux_br_lane_mapx {
+	u64 u;
+	struct cavm_cgxx_spux_br_lane_mapx_s {
+		u64 ln_mapping                       : 6;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_spux_br_lane_mapx_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_LANE_MAPX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_LANE_MAPX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 19)))
+		return 0x10600 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_pmd_control
+ *
+ * CGX SPU BASE-R PMD Control Registers
+ */
+union cavm_cgxx_spux_br_pmd_control {
+	u64 u;
+	struct cavm_cgxx_spux_br_pmd_control_s {
+		u64 train_restart                    : 1;
+		u64 train_en                         : 1;
+		u64 use_lane_poly                    : 1;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_cgxx_spux_br_pmd_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100a8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_pmd_ld_cup
+ *
+ * CGX SPU BASE-R PMD Local Device Coefficient Update Registers
+ * This register implements MDIO register 1.154 of 802.3-2012 Section 5 CL45 for 10GBASE-R and
+ * and of 802.3by-2016 CL45 for 25GBASE-R. Note that for 10G, 25G LN0_ only is used.
+ *
+ * It implements  MDIO registers 1.1300-1.1303 for all other BASE-R modes (40G, 50G, 100G) per
+ * 802.3bj-2014 CL45. Note that for 50G LN0_ and LN1_ only are used.
+ *
+ * The fields in this register are read/write even though they are specified as read-only in 802.3.
+ *
+ * The register is automatically cleared at the start of training. When link training
+ * is in progress, each field reflects the contents of the coefficient update field in the
+ * associated lane's outgoing training frame.
+ *
+ * If CGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is set, then this register must be updated
+ * by software during link training and hardware updates are disabled. If
+ * CGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is clear, this register is automatically
+ * updated by hardware, and it should not be written by software. The lane fields in this
+ * register are indexed by logical PCS lane ID.
+ */
+union cavm_cgxx_spux_br_pmd_ld_cup {
+	u64 u;
+	struct cavm_cgxx_spux_br_pmd_ld_cup_s {
+		u64 ln0_cup                          : 16;
+		u64 ln1_cup                          : 16;
+		u64 ln2_cup                          : 16;
+		u64 ln3_cup                          : 16;
+	} s;
+	/* struct cavm_cgxx_spux_br_pmd_ld_cup_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LD_CUP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LD_CUP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100c8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_pmd_ld_rep
+ *
+ * CGX SPU BASE-R PMD Local Device Status Report Registers
+ * This register implements MDIO register 1.155 of 802.3-2012 Section 5 CL45 for 10GBASE-R and
+ * and of 802.3by-2016 CL45 for 25GBASE-R. Note that for 10G, 25G LN0_ only is used.
+ *
+ * It implements  MDIO registers 1.1400-1.1403 for all other BASE-R modes (40G, 50G, 100G) per
+ * 802.3bj-2014 CL45. Note that for 50G LN0_ and LN1_ only are used.
+ *
+ * The fields in this register are read/write even though they are specified as read-only in 802.3.
+ *
+ * The register is automatically cleared at the start of training. Each field
+ * reflects the contents of the status report field in the associated lane's outgoing training
+ * frame.
+ *
+ * If CGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is set, then this register must
+ * be updated by software during link training and hardware updates are disabled. If
+ * CGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is clear, this register is automatically
+ * updated by hardware, and it should not be written by software. The lane fields in this
+ * register are indexed by logical PCS lane ID.
+ */
+union cavm_cgxx_spux_br_pmd_ld_rep {
+	u64 u;
+	struct cavm_cgxx_spux_br_pmd_ld_rep_s {
+		u64 ln0_rep                          : 16;
+		u64 ln1_rep                          : 16;
+		u64 ln2_rep                          : 16;
+		u64 ln3_rep                          : 16;
+	} s;
+	/* struct cavm_cgxx_spux_br_pmd_ld_rep_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LD_REP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LD_REP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100d0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_pmd_lp_cup
+ *
+ * CGX SPU BASE-R PMD Link Partner Coefficient Update Registers
+ * This register implements MDIO register 1.152 of 802.3-2012 Section 5 CL45 for 10GBASE-R and
+ * and of 802.3by-2016 CL45 for 25GBASE-R. Note that for 10G, 25G LN0_ only is used.
+ *
+ * It implements  MDIO registers 1.1100-1.1103 for all other BASE-R modes (40G, 50G, 100G) per
+ * 802.3bj-2014 CL45. Note that for 50G LN0_ and LN1_ only are used.
+ *
+ * The register is automatically cleared at the start of training. Each field reflects
+ * the contents of the coefficient update field in the lane's most recently received training
+ * frame. This register should not be written when link training is enabled, i.e. when
+ * CGX()_SPU()_BR_PMD_CONTROL[TRAIN_EN] is set. The lane fields in this register are indexed by
+ * logical PCS lane ID.
+ */
+union cavm_cgxx_spux_br_pmd_lp_cup {
+	u64 u;
+	struct cavm_cgxx_spux_br_pmd_lp_cup_s {
+		u64 ln0_cup                          : 16;
+		u64 ln1_cup                          : 16;
+		u64 ln2_cup                          : 16;
+		u64 ln3_cup                          : 16;
+	} s;
+	/* struct cavm_cgxx_spux_br_pmd_lp_cup_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LP_CUP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LP_CUP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100b8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_pmd_lp_rep
+ *
+ * CGX SPU BASE-R PMD Link Partner Status Report Registers
+ * This register implements MDIO register 1.153 of 802.3-2012 Section 5 CL45 for 10GBASE-R and
+ * and of 802.3by-2016 CL45 for 25GBASE-R. Note that for 10G, 25G LN0_ only is used.
+ *
+ * It implements  MDIO registers 1.1200-1.1203 for all other BASE-R modes (40G, 50G, 100G) per
+ * 802.3bj-2014 CL45. Note that for 50G LN0_ and LN1_ only are used.
+ *
+ * The register is automatically cleared at the start of training. Each field reflects
+ * the contents of the coefficient update field in the lane's most recently received training
+ * frame. This register should not be written when link training is enabled, i.e. when
+ * CGX()_SPU()_BR_PMD_CONTROL[TRAIN_EN] is set. The lane fields in this register are indexed by
+ * logical PCS lane ID.
+ */
+union cavm_cgxx_spux_br_pmd_lp_rep {
+	u64 u;
+	struct cavm_cgxx_spux_br_pmd_lp_rep_s {
+		u64 ln0_rep                          : 16;
+		u64 ln1_rep                          : 16;
+		u64 ln2_rep                          : 16;
+		u64 ln3_rep                          : 16;
+	} s;
+	/* struct cavm_cgxx_spux_br_pmd_lp_rep_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LP_REP(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_LP_REP(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100c0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_pmd_status
+ *
+ * CGX SPU BASE-R PMD Status Registers
+ * The lane fields in this register are indexed by logical PCS lane ID. The lane 0 field (LN0_*)
+ * is valid for 10GBASE-R, 25GBASE-R, 40GBASE-R, 50GBASE-R and 100GBASE-R. The lane 1 field
+ * (LN1_*) is valid for 40GBASE-R, 50GBASE-R and 100GBASE-R. The remaining fields (LN2_*, LN3_*)
+ * are only valid for 40GBASE-R and 100GBASE-R.
+ */
+union cavm_cgxx_spux_br_pmd_status {
+	u64 u;
+	struct cavm_cgxx_spux_br_pmd_status_s {
+		u64 ln0_train_status                 : 4;
+		u64 ln1_train_status                 : 4;
+		u64 ln2_train_status                 : 4;
+		u64 ln3_train_status                 : 4;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_br_pmd_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_PMD_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100b0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_status1
+ *
+ * CGX SPU BASE-R Status 1 Registers
+ */
+union cavm_cgxx_spux_br_status1 {
+	u64 u;
+	struct cavm_cgxx_spux_br_status1_s {
+		u64 blk_lock                         : 1;
+		u64 hi_ber                           : 1;
+		u64 prbs31                           : 1;
+		u64 prbs9                            : 1;
+		u64 reserved_4_11                    : 8;
+		u64 rcv_lnk                          : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_cgxx_spux_br_status1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_STATUS1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_STATUS1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10030 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_status2
+ *
+ * CGX SPU BASE-R Status 2 Registers
+ * This register implements a combination of the following IEEE 802.3 registers:
+ * * BASE-R PCS status 2 (MDIO address 3.33).
+ * * BASE-R BER high-order counter (MDIO address 3.44).
+ * * Errored-blocks high-order counter (MDIO address 3.45).
+ *
+ * Note that the relative locations of some fields have been moved from IEEE 802.3 in order to
+ * make the register layout more software friendly: the BER counter high-order and low-order bits
+ * from sections 3.44 and 3.33 have been combined into the contiguous, 22-bit [BER_CNT] field;
+ * likewise, the errored-blocks counter high-order and low-order bits from section 3.45 have been
+ * combined into the contiguous, 22-bit [ERR_BLKS] field.
+ */
+union cavm_cgxx_spux_br_status2 {
+	u64 u;
+	struct cavm_cgxx_spux_br_status2_s {
+		u64 reserved_0_13                    : 14;
+		u64 latched_ber                      : 1;
+		u64 latched_lock                     : 1;
+		u64 ber_cnt                          : 22;
+		u64 reserved_38_39                   : 2;
+		u64 err_blks                         : 22;
+		u64 reserved_62_63                   : 2;
+	} s;
+	/* struct cavm_cgxx_spux_br_status2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_STATUS2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_STATUS2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10038 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_tp_control
+ *
+ * CGX SPU BASE-R Test-Pattern Control Registers
+ * Refer to the test pattern methodology described in 802.3 sections 49.2.8 and 82.2.10.
+ */
+union cavm_cgxx_spux_br_tp_control {
+	u64 u;
+	struct cavm_cgxx_spux_br_tp_control_s {
+		u64 dp_sel                           : 1;
+		u64 tp_sel                           : 1;
+		u64 rx_tp_en                         : 1;
+		u64 tx_tp_en                         : 1;
+		u64 prbs31_tx                        : 1;
+		u64 prbs31_rx                        : 1;
+		u64 prbs9_tx                         : 1;
+		u64 scramble_tp                      : 2;
+		u64 pr_tp_data_type                  : 1;
+		u64 reserved_10_63                   : 54;
+	} s;
+	/* struct cavm_cgxx_spux_br_tp_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_TP_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_TP_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10040 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_tp_err_cnt
+ *
+ * CGX SPU BASE-R Test-Pattern Error-Count Registers
+ * This register provides the BASE-R PCS test-pattern error counter.
+ */
+union cavm_cgxx_spux_br_tp_err_cnt {
+	u64 u;
+	struct cavm_cgxx_spux_br_tp_err_cnt_s {
+		u64 err_cnt                          : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_br_tp_err_cnt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_TP_ERR_CNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_TP_ERR_CNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10048 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_tp_seed_a
+ *
+ * CGX SPU BASE-R Test-Pattern Seed A Registers
+ * Refer to the test pattern methodology described in 802.3 sections 49.2.8 and 82.2.10.
+ */
+union cavm_cgxx_spux_br_tp_seed_a {
+	u64 u;
+	struct cavm_cgxx_spux_br_tp_seed_a_s {
+		u64 tp_seed_a                        : 58;
+		u64 reserved_58_63                   : 6;
+	} s;
+	/* struct cavm_cgxx_spux_br_tp_seed_a_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_TP_SEED_A(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_TP_SEED_A(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10060 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_br_tp_seed_b
+ *
+ * CGX SPU BASE-R Test-Pattern Seed B Registers
+ * Refer to the test pattern methodology described in 802.3 sections 49.2.8 and 82.2.10.
+ */
+union cavm_cgxx_spux_br_tp_seed_b {
+	u64 u;
+	struct cavm_cgxx_spux_br_tp_seed_b_s {
+		u64 tp_seed_b                        : 58;
+		u64 reserved_58_63                   : 6;
+	} s;
+	/* struct cavm_cgxx_spux_br_tp_seed_b_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BR_TP_SEED_B(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BR_TP_SEED_B(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10068 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_bx_status
+ *
+ * CGX SPU BASE-X Status Registers
+ */
+union cavm_cgxx_spux_bx_status {
+	u64 u;
+	struct cavm_cgxx_spux_bx_status_s {
+		u64 lsync                            : 4;
+		u64 reserved_4_10                    : 7;
+		u64 pattst                           : 1;
+		u64 alignd                           : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_cgxx_spux_bx_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_BX_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_BX_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10028 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_control1
+ *
+ * CGX SPU Control 1 Registers
+ */
+union cavm_cgxx_spux_control1 {
+	u64 u;
+	struct cavm_cgxx_spux_control1_s {
+		u64 reserved_0_1                     : 2;
+		u64 spd                              : 4;
+		u64 spdsel0                          : 1;
+		u64 reserved_7_10                    : 4;
+		u64 lo_pwr                           : 1;
+		u64 reserved_12                      : 1;
+		u64 spdsel1                          : 1;
+		u64 loopbck                          : 1;
+		u64 reset                            : 1;
+		u64 usxgmii_type                     : 3;
+		u64 usxgmii_rate                     : 3;
+		u64 reserved_22_63                   : 42;
+	} s;
+	/* struct cavm_cgxx_spux_control1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_CONTROL1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_CONTROL1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10000 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_control2
+ *
+ * CGX SPU Control 2 Registers
+ */
+union cavm_cgxx_spux_control2 {
+	u64 u;
+	struct cavm_cgxx_spux_control2_s {
+		u64 pcs_type                         : 4;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_cgxx_spux_control2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_CONTROL2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_CONTROL2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10018 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_fec_abil
+ *
+ * CGX SPU Forward Error Correction Ability Registers
+ */
+union cavm_cgxx_spux_fec_abil {
+	u64 u;
+	struct cavm_cgxx_spux_fec_abil_s {
+		u64 fec_abil                         : 1;
+		u64 err_abil                         : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_cgxx_spux_fec_abil_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_FEC_ABIL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_FEC_ABIL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100d8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_fec_control
+ *
+ * CGX SPU Forward Error Correction Control Registers
+ */
+union cavm_cgxx_spux_fec_control {
+	u64 u;
+	struct cavm_cgxx_spux_fec_control_s {
+		u64 fec_en                           : 2;
+		u64 err_en                           : 1;
+		u64 fec_byp_ind_en                   : 1;
+		u64 fec_byp_cor_en                   : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_cgxx_spux_fec_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_FEC_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_FEC_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x100e0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_fec_ln#_rsfec_err
+ *
+ * CGX SPU Reed-Solomon FEC Symbol Error Counter for FEC Lanes 0-3 Registers
+ * This register is valid only when Reed-Solomon FEC is enabled.
+ * The symbol error counters are defined in 802.3 section 91.6.11 (for 100G
+ * and extended to 50G) and 802.3by-2016 section 108.6.9 (for 25G and extended to USXGMII).
+ * The counter is reset to all zeros when the register is read, and held at all ones in case
+ * of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register
+ * is read on the same clock cycle as an increment operation, the counter is reset to
+ * all zeros and the increment operation is lost. The counters are writable for test
+ * purposes, rather than read-only as specified in IEEE 802.3.
+ */
+union cavm_cgxx_spux_fec_lnx_rsfec_err {
+	u64 u;
+	struct cavm_cgxx_spux_fec_lnx_rsfec_err_s {
+		u64 symb_err_cnt                     : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_spux_fec_lnx_rsfec_err_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_FEC_LNX_RSFEC_ERR(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_FEC_LNX_RSFEC_ERR(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 3)))
+		return 0x10900 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_int
+ *
+ * CGX SPU Interrupt Registers
+ */
+union cavm_cgxx_spux_int {
+	u64 u;
+	struct cavm_cgxx_spux_int_s {
+		u64 rx_link_up                       : 1;
+		u64 rx_link_down                     : 1;
+		u64 err_blk                          : 1;
+		u64 bitlckls                         : 1;
+		u64 synlos                           : 1;
+		u64 algnlos                          : 1;
+		u64 dbg_sync                         : 1;
+		u64 bip_err                          : 1;
+		u64 fec_corr                         : 1;
+		u64 fec_uncorr                       : 1;
+		u64 an_page_rx                       : 1;
+		u64 an_link_good                     : 1;
+		u64 an_complete                      : 1;
+		u64 training_done                    : 1;
+		u64 training_failure                 : 1;
+		u64 fec_align_status                 : 1;
+		u64 rsfec_corr                       : 1;
+		u64 rsfec_uncorr                     : 1;
+		u64 hi_ser                           : 1;
+		u64 usx_an_lnk_st                    : 1;
+		u64 usx_an_cpt                       : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_cgxx_spux_int_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10220 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_int_ena_w1c
+ *
+ * CGX SPU Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_cgxx_spux_int_ena_w1c {
+	u64 u;
+	struct cavm_cgxx_spux_int_ena_w1c_s {
+		u64 rx_link_up                       : 1;
+		u64 rx_link_down                     : 1;
+		u64 err_blk                          : 1;
+		u64 bitlckls                         : 1;
+		u64 synlos                           : 1;
+		u64 algnlos                          : 1;
+		u64 dbg_sync                         : 1;
+		u64 bip_err                          : 1;
+		u64 fec_corr                         : 1;
+		u64 fec_uncorr                       : 1;
+		u64 an_page_rx                       : 1;
+		u64 an_link_good                     : 1;
+		u64 an_complete                      : 1;
+		u64 training_done                    : 1;
+		u64 training_failure                 : 1;
+		u64 fec_align_status                 : 1;
+		u64 rsfec_corr                       : 1;
+		u64 rsfec_uncorr                     : 1;
+		u64 hi_ser                           : 1;
+		u64 usx_an_lnk_st                    : 1;
+		u64 usx_an_cpt                       : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_cgxx_spux_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_INT_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_INT_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10230 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_int_ena_w1s
+ *
+ * CGX SPU Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_cgxx_spux_int_ena_w1s {
+	u64 u;
+	struct cavm_cgxx_spux_int_ena_w1s_s {
+		u64 rx_link_up                       : 1;
+		u64 rx_link_down                     : 1;
+		u64 err_blk                          : 1;
+		u64 bitlckls                         : 1;
+		u64 synlos                           : 1;
+		u64 algnlos                          : 1;
+		u64 dbg_sync                         : 1;
+		u64 bip_err                          : 1;
+		u64 fec_corr                         : 1;
+		u64 fec_uncorr                       : 1;
+		u64 an_page_rx                       : 1;
+		u64 an_link_good                     : 1;
+		u64 an_complete                      : 1;
+		u64 training_done                    : 1;
+		u64 training_failure                 : 1;
+		u64 fec_align_status                 : 1;
+		u64 rsfec_corr                       : 1;
+		u64 rsfec_uncorr                     : 1;
+		u64 hi_ser                           : 1;
+		u64 usx_an_lnk_st                    : 1;
+		u64 usx_an_cpt                       : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_cgxx_spux_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_INT_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_INT_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10238 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_int_w1s
+ *
+ * CGX SPU Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_cgxx_spux_int_w1s {
+	u64 u;
+	struct cavm_cgxx_spux_int_w1s_s {
+		u64 rx_link_up                       : 1;
+		u64 rx_link_down                     : 1;
+		u64 err_blk                          : 1;
+		u64 bitlckls                         : 1;
+		u64 synlos                           : 1;
+		u64 algnlos                          : 1;
+		u64 dbg_sync                         : 1;
+		u64 bip_err                          : 1;
+		u64 fec_corr                         : 1;
+		u64 fec_uncorr                       : 1;
+		u64 an_page_rx                       : 1;
+		u64 an_link_good                     : 1;
+		u64 an_complete                      : 1;
+		u64 training_done                    : 1;
+		u64 training_failure                 : 1;
+		u64 fec_align_status                 : 1;
+		u64 rsfec_corr                       : 1;
+		u64 rsfec_uncorr                     : 1;
+		u64 hi_ser                           : 1;
+		u64 usx_an_lnk_st                    : 1;
+		u64 usx_an_cpt                       : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_cgxx_spux_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10228 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_ln#_br_bip_err_cnt
+ *
+ * CGX SPU 40,50,100GBASE-R BIP Error-Counter Registers
+ * This register implements the IEEE 802.3 BIP error-counter registers for PCS lanes
+ * 0-19 (3.200-3.203). It is valid only when the LPCS type is 40GBASE-R, 50GBASE-R,
+ * 100GBASE-R, (CGX()_CMR()_CONFIG[LMAC_TYPE]), and always returns 0x0
+ * for all other LPCS types. The counters are indexed by the RX PCS lane number based
+ * on the alignment marker detected on each lane and captured in
+ * CGX()_SPU()_BR_LANE_MAP(). Each counter counts the BIP errors for its PCS lane, and is
+ * held at all ones in case of overflow. The counters are reset to all zeros when this
+ * register is read by software.
+ *
+ * The reset operation takes precedence over the increment operation; if the register
+ * is read on the same clock cycle as an increment operation, the counter is reset to
+ * all zeros and the increment operation is lost. The counters are writable for test
+ * purposes, rather than read-only as specified in IEEE 802.3.
+ */
+union cavm_cgxx_spux_lnx_br_bip_err_cnt {
+	u64 u;
+	struct cavm_cgxx_spux_lnx_br_bip_err_cnt_s {
+		u64 bip_err_cnt                      : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_lnx_br_bip_err_cnt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_LNX_BR_BIP_ERR_CNT(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_LNX_BR_BIP_ERR_CNT(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 19)))
+		return 0x10500 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_ln#_fec_corr_blks
+ *
+ * CGX SPU FEC Corrected-Blocks Counters 0-19 Registers
+ * This register is valid only when the LPCS type is BASE-R
+ * (CGX()_CMR()_CONFIG[LMAC_TYPE]) and applies
+ * to BASE-R FEC and Reed-Solomon FEC (RS-FEC). When BASE-R FEC is enabled, the FEC
+ * corrected-block counters are defined in IEEE 802.3 section 74.8.4.1. Each
+ * corrected-blocks counter increments by one for a corrected FEC block, i.e. an FEC
+ * block that has been received with invalid parity on the associated PCS lane and has
+ * been corrected by the FEC decoder. The counter is reset to all zeros when the register
+ * is read, and held at all ones in case of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register
+ * is read on the same clock cycle as an increment operation, the counter is reset to
+ * all zeros and the increment operation is lost. The counters are writable for test
+ * purposes, rather than read-only as specified in IEEE 802.3.
+ */
+union cavm_cgxx_spux_lnx_fec_corr_blks {
+	u64 u;
+	struct cavm_cgxx_spux_lnx_fec_corr_blks_s {
+		u64 ln_corr_blks                     : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_spux_lnx_fec_corr_blks_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_LNX_FEC_CORR_BLKS(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_LNX_FEC_CORR_BLKS(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 19)))
+		return 0x10700 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_ln#_fec_uncorr_blks
+ *
+ * CGX SPU FEC Uncorrected-Blocks Counters 0-19 Registers
+ * This register is valid only when the LPCS type is BASE-R
+ * (CGX()_CMR()_CONFIG[LMAC_TYPE]) and applies
+ * to BASE-R FEC and Reed-Solomon FEC (RS-FEC). When BASE-R FEC is enabled, the FEC
+ * corrected-block counters are defined in IEEE 802.3 section 74.8.4.2. Each
+ * uncorrected-blocks counter increments by one for an uncorrected FEC block, i.e. an FEC
+ * block that has been received with invalid parity on the associated PCS lane and has
+ * not been corrected by the FEC decoder. The counter is reset to all zeros when the
+ * register is read, and held at all ones in case of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register
+ * is read on the same clock cycle as an increment operation, the counter is reset to
+ * all zeros and the increment operation is lost. The counters are writable for test
+ * purposes, rather than read-only as specified in IEEE 802.3.
+ */
+union cavm_cgxx_spux_lnx_fec_uncorr_blks {
+	u64 u;
+	struct cavm_cgxx_spux_lnx_fec_uncorr_blks_s {
+		u64 ln_uncorr_blks                   : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_spux_lnx_fec_uncorr_blks_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_LNX_FEC_UNCORR_BLKS(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_LNX_FEC_UNCORR_BLKS(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 19)))
+		return 0x10800 + 0x40000 * ((a) & 0x3) + 8 * ((b) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_lpcs_states
+ *
+ * CGX SPU BASE-X Transmit/Receive States Registers
+ */
+union cavm_cgxx_spux_lpcs_states {
+	u64 u;
+	struct cavm_cgxx_spux_lpcs_states_s {
+		u64 deskew_sm                        : 3;
+		u64 reserved_3                       : 1;
+		u64 deskew_am_found                  : 20;
+		u64 bx_rx_sm                         : 2;
+		u64 reserved_26_27                   : 2;
+		u64 br_rx_sm                         : 3;
+		u64 reserved_31_63                   : 33;
+	} s;
+	/* struct cavm_cgxx_spux_lpcs_states_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_LPCS_STATES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_LPCS_STATES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10208 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_misc_control
+ *
+ * CGX SPU Miscellaneous Control Registers
+ * "* RX logical PCS lane polarity vector \<3:0\> = [XOR_RXPLRT]\<3:0\> ^ {4{[RXPLRT]}}.
+ * * TX logical PCS lane polarity vector \<3:0\> = [XOR_TXPLRT]\<3:0\> ^ {4{[TXPLRT]}}.
+ *
+ * In short, keep [RXPLRT] and [TXPLRT] cleared, and use [XOR_RXPLRT] and [XOR_TXPLRT] fields to
+ * define the polarity per logical PCS lane. Only bit 0 of vector is used for 10GBASE-R, and
+ * only bits 1:0 of vector are used for RXAUI."
+ */
+union cavm_cgxx_spux_misc_control {
+	u64 u;
+	struct cavm_cgxx_spux_misc_control_s {
+		u64 txplrt                           : 1;
+		u64 rxplrt                           : 1;
+		u64 xor_txplrt                       : 4;
+		u64 xor_rxplrt                       : 4;
+		u64 intlv_rdisp                      : 1;
+		u64 skip_after_term                  : 1;
+		u64 rx_packet_dis                    : 1;
+		u64 rx_edet_signal_ok                : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_cgxx_spux_misc_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_MISC_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_MISC_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10218 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rsfec_corr
+ *
+ * CGX SPU Reed-Solomon FEC Corrected Codeword Counter Register
+ * This register implements the IEEE 802.3 RS-FEC corrected codewords counter
+ * described in 802.3 section 91.6.8 (for 100G and extended to 50G) and 802.3by-2016
+ * section 108.6.7 (for 25G and extended to USXGMII).
+ */
+union cavm_cgxx_spux_rsfec_corr {
+	u64 u;
+	struct cavm_cgxx_spux_rsfec_corr_s {
+		u64 cw_cnt                           : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_spux_rsfec_corr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RSFEC_CORR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RSFEC_CORR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10088 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rsfec_status
+ *
+ * CGX SPU Reed-Solomon FEC Status Registers
+ * This register implements the IEEE 802.3 RS-FEC status and lane mapping registers as
+ * described in 802.3 section 91.6 (for 100G and extended to 50G) and 802.3by-2016
+ * section 108-6 (for 25G and extended to USXGMII).
+ */
+union cavm_cgxx_spux_rsfec_status {
+	u64 u;
+	struct cavm_cgxx_spux_rsfec_status_s {
+		u64 fec_lane_mapping                 : 8;
+		u64 fec_align_status                 : 1;
+		u64 amps_lock                        : 4;
+		u64 hi_ser                           : 1;
+		u64 fec_byp_ind_abil                 : 1;
+		u64 fec_byp_cor_abil                 : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_rsfec_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RSFEC_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RSFEC_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10080 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rsfec_uncorr
+ *
+ * CGX SPU Reed-Solomon FEC Uncorrected Codeword Counter Register
+ * This register implements the IEEE 802.3 RS-FEC uncorrected codewords counter
+ * described in 802.3 section 91.6.9 (for 100G and extended to 50G) and 802.3by-2016
+ * section 108.6.8 (for 25G and extended to USXGMII).
+ */
+union cavm_cgxx_spux_rsfec_uncorr {
+	u64 u;
+	struct cavm_cgxx_spux_rsfec_uncorr_s {
+		u64 cw_cnt                           : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_spux_rsfec_uncorr_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RSFEC_UNCORR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RSFEC_UNCORR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10090 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rx_eee_wake
+ *
+ * INTERNAL: CGX SPU  RX EEE Wake Error Counter  Registers
+ *
+ * Reserved.
+ * Internal:
+ * A counter that is incremented each time that the LPI receive state diagram enters
+ * the RX_WTF state indicating that a wake time fault has been detected.
+ */
+union cavm_cgxx_spux_rx_eee_wake {
+	u64 u;
+	struct cavm_cgxx_spux_rx_eee_wake_s {
+		u64 wtf_error_counter                : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_rx_eee_wake_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RX_EEE_WAKE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RX_EEE_WAKE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x103e0 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rx_lpi_timing
+ *
+ * INTERNAL: CGX SPU RX EEE LPI Timing Parameters Registers
+ *
+ * Reserved.
+ * Internal:
+ * This register specifies receiver LPI timing parameters Tqr, Twr and Twtf.
+ */
+union cavm_cgxx_spux_rx_lpi_timing {
+	u64 u;
+	struct cavm_cgxx_spux_rx_lpi_timing_s {
+		u64 twtf                             : 20;
+		u64 twr                              : 20;
+		u64 tqr                              : 20;
+		u64 reserved_60_61                   : 2;
+		u64 rx_lpi_fw                        : 1;
+		u64 rx_lpi_en                        : 1;
+	} s;
+	/* struct cavm_cgxx_spux_rx_lpi_timing_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RX_LPI_TIMING(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RX_LPI_TIMING(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x103c0 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rx_lpi_timing2
+ *
+ * INTERNAL: CGX SPU RX EEE LPI Timing2 Parameters Registers
+ *
+ * Reserved.
+ * Internal:
+ * This register specifies receiver LPI timing parameters hold_off_timer.
+ */
+union cavm_cgxx_spux_rx_lpi_timing2 {
+	u64 u;
+	struct cavm_cgxx_spux_rx_lpi_timing2_s {
+		u64 hold_off_timer                   : 20;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_cgxx_spux_rx_lpi_timing2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RX_LPI_TIMING2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RX_LPI_TIMING2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10420 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_rx_mrk_cnt
+ *
+ * CGX SPU Receiver Marker Interval Count Control Registers
+ */
+union cavm_cgxx_spux_rx_mrk_cnt {
+	u64 u;
+	struct cavm_cgxx_spux_rx_mrk_cnt_s {
+		u64 mrk_cnt                          : 20;
+		u64 reserved_20_43                   : 24;
+		u64 by_mrk_100g                      : 1;
+		u64 reserved_45_47                   : 3;
+		u64 ram_mrk_cnt                      : 8;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_cgxx_spux_rx_mrk_cnt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_RX_MRK_CNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_RX_MRK_CNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x103a0 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_spd_abil
+ *
+ * CGX SPU PCS Speed Ability Registers
+ */
+union cavm_cgxx_spux_spd_abil {
+	u64 u;
+	struct cavm_cgxx_spux_spd_abil_s {
+		u64 tengb                            : 1;
+		u64 tenpasst                         : 1;
+		u64 usxgmii                          : 1;
+		u64 twentyfivegb                     : 1;
+		u64 fortygb                          : 1;
+		u64 fiftygb                          : 1;
+		u64 hundredgb                        : 1;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_cgxx_spux_spd_abil_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_SPD_ABIL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_SPD_ABIL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10010 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_status1
+ *
+ * CGX SPU Status 1 Registers
+ */
+union cavm_cgxx_spux_status1 {
+	u64 u;
+	struct cavm_cgxx_spux_status1_s {
+		u64 reserved_0                       : 1;
+		u64 lpable                           : 1;
+		u64 rcv_lnk                          : 1;
+		u64 reserved_3_6                     : 4;
+		u64 flt                              : 1;
+		u64 rx_lpi_indication                : 1;
+		u64 tx_lpi_indication                : 1;
+		u64 rx_lpi_received                  : 1;
+		u64 tx_lpi_received                  : 1;
+		u64 reserved_12_63                   : 52;
+	} s;
+	/* struct cavm_cgxx_spux_status1_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_STATUS1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_STATUS1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10008 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_status2
+ *
+ * CGX SPU Status 2 Registers
+ */
+union cavm_cgxx_spux_status2 {
+	u64 u;
+	struct cavm_cgxx_spux_status2_s {
+		u64 tengb_r                          : 1;
+		u64 tengb_x                          : 1;
+		u64 tengb_w                          : 1;
+		u64 tengb_t                          : 1;
+		u64 usxgmii_r                        : 1;
+		u64 twentyfivegb_r                   : 1;
+		u64 fortygb_r                        : 1;
+		u64 fiftygb_r                        : 1;
+		u64 hundredgb_r                      : 1;
+		u64 reserved_9                       : 1;
+		u64 rcvflt                           : 1;
+		u64 xmtflt                           : 1;
+		u64 reserved_12_13                   : 2;
+		u64 dev                              : 2;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_status2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_STATUS2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_STATUS2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10020 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_tx_lpi_timing
+ *
+ * INTERNAL: CGX SPU TX EEE LPI Timing Parameters Registers
+ *
+ * Reserved.
+ * Internal:
+ * Transmit LPI timing parameters Tsl, Tql and Tul
+ */
+union cavm_cgxx_spux_tx_lpi_timing {
+	u64 u;
+	struct cavm_cgxx_spux_tx_lpi_timing_s {
+		u64 tql                              : 19;
+		u64 reserved_19_31                   : 13;
+		u64 tul                              : 12;
+		u64 reserved_44_47                   : 4;
+		u64 tsl                              : 12;
+		u64 reserved_60                      : 1;
+		u64 tx_lpi_ignore_twl                : 1;
+		u64 tx_lpi_fw                        : 1;
+		u64 tx_lpi_en                        : 1;
+	} s;
+	/* struct cavm_cgxx_spux_tx_lpi_timing_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_TX_LPI_TIMING(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_TX_LPI_TIMING(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10400 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_tx_lpi_timing2
+ *
+ * INTERNAL: CGX SPU TX EEE LPI Timing2 Parameters Registers
+ *
+ * Reserved.
+ * Internal:
+ * This register specifies transmit LPI timer parameters.
+ */
+union cavm_cgxx_spux_tx_lpi_timing2 {
+	u64 u;
+	struct cavm_cgxx_spux_tx_lpi_timing2_s {
+		u64 t1u                              : 8;
+		u64 reserved_8_11                    : 4;
+		u64 twl                              : 12;
+		u64 reserved_24_31                   : 8;
+		u64 twl2                             : 12;
+		u64 reserved_44_47                   : 4;
+		u64 tbyp                             : 12;
+		u64 reserved_60_63                   : 4;
+	} s;
+	/* struct cavm_cgxx_spux_tx_lpi_timing2_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_TX_LPI_TIMING2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_TX_LPI_TIMING2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10440 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_tx_mrk_cnt
+ *
+ * CGX SPU Transmitter Marker Interval Count Control Registers
+ */
+union cavm_cgxx_spux_tx_mrk_cnt {
+	u64 u;
+	struct cavm_cgxx_spux_tx_mrk_cnt_s {
+		u64 mrk_cnt                          : 20;
+		u64 reserved_20_43                   : 24;
+		u64 by_mrk_100g                      : 1;
+		u64 reserved_45_47                   : 3;
+		u64 ram_mrk_cnt                      : 8;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_cgxx_spux_tx_mrk_cnt_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_TX_MRK_CNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_TX_MRK_CNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10380 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_adv
+ *
+ * CGX SPU USXGMII Autonegotiation Advertisement Registers
+ * Software programs this register with the contents of the AN-link code word base page to be
+ * transmitted during autonegotiation. Any write operations to this register prior to completion
+ * of autonegotiation should be followed by a renegotiation in order for the new values to take
+ * effect. Once autonegotiation has completed, software can examine this register along with
+ * CGX()_SPU()_USX_AN_ADV to determine the highest common denominator technology.
+ * The format for this register is from USXGMII Multiport specification section 1.1.2 Table 2.
+ */
+union cavm_cgxx_spux_usx_an_adv {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_adv_s {
+		u64 set                              : 1;
+		u64 reserved_1_6                     : 6;
+		u64 eee_clk_stop_abil                : 1;
+		u64 eee_abil                         : 1;
+		u64 spd                              : 3;
+		u64 dplx                             : 1;
+		u64 reserved_13_14                   : 2;
+		u64 lnk_st                           : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_adv_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_ADV(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_ADV(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101d0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_control
+ *
+ * CGX SPU USXGMII Autonegotiation Control Register
+ */
+union cavm_cgxx_spux_usx_an_control {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_control_s {
+		u64 reserved_0_8                     : 9;
+		u64 rst_an                           : 1;
+		u64 reserved_10_11                   : 2;
+		u64 an_en                            : 1;
+		u64 reserved_13_14                   : 2;
+		u64 an_reset                         : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_CONTROL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_CONTROL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101c0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_expansion
+ *
+ * CGX SPU USXGMII Autonegotiation Expansion Register
+ * This register is only used to signal page reception.
+ */
+union cavm_cgxx_spux_usx_an_expansion {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_expansion_s {
+		u64 reserved_0                       : 1;
+		u64 an_page_received                 : 1;
+		u64 next_page_able                   : 1;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_expansion_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_EXPANSION(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_EXPANSION(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101e0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_flow_ctrl
+ *
+ * CGX SPU USXGMII Flow Control Registers
+ * This register is used by software to affect USXGMII AN hardware behavior.
+ */
+union cavm_cgxx_spux_usx_an_flow_ctrl {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_flow_ctrl_s {
+		u64 start_idle_detect                : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_flow_ctrl_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_FLOW_CTRL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_FLOW_CTRL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101e8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_link_timer
+ *
+ * CGX SPU USXGMII Link Timer Registers
+ * This is the link timer register.
+ */
+union cavm_cgxx_spux_usx_an_link_timer {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_link_timer_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_link_timer_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_LINK_TIMER(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_LINK_TIMER(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101f0 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_lp_abil
+ *
+ * CGX SPU USXGMII Autonegotiation Link-Partner Advertisement Registers
+ * This register captures the contents of the latest AN link code word base page received from
+ * the link partner during autonegotiation. This is register 5 per IEEE 802.3, Clause 37.
+ * CGX()_SPU()_USX_AN_EXPANSION[AN_PAGE_RECEIVED] is set when this register is updated by hardware.
+ */
+union cavm_cgxx_spux_usx_an_lp_abil {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_lp_abil_s {
+		u64 set                              : 1;
+		u64 reserved_1_6                     : 6;
+		u64 eee_clk_stop_abil                : 1;
+		u64 eee_abil                         : 1;
+		u64 spd                              : 3;
+		u64 dplx                             : 1;
+		u64 reserved_13_14                   : 2;
+		u64 lnk_st                           : 1;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_lp_abil_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_LP_ABIL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_LP_ABIL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101d8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu#_usx_an_status
+ *
+ * CGX SPU USXGMII Autonegotiation Status Register
+ */
+union cavm_cgxx_spux_usx_an_status {
+	u64 u;
+	struct cavm_cgxx_spux_usx_an_status_s {
+		u64 extnd                            : 1;
+		u64 reserved_1                       : 1;
+		u64 lnk_st                           : 1;
+		u64 an_abil                          : 1;
+		u64 rmt_flt                          : 1;
+		u64 an_cpt                           : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_spux_usx_an_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPUX_USX_AN_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPUX_USX_AN_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x101c8 + 0x40000 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu_dbg_control
+ *
+ * CGX SPU Debug Control Registers
+ */
+union cavm_cgxx_spu_dbg_control {
+	u64 u;
+	struct cavm_cgxx_spu_dbg_control_s {
+		u64 marker_rxp                       : 15;
+		u64 reserved_15                      : 1;
+		u64 scramble_dis                     : 1;
+		u64 reserved_17_18                   : 2;
+		u64 br_pmd_train_soft_en             : 1;
+		u64 reserved_20_27                   : 8;
+		u64 timestamp_norm_dis               : 1;
+		u64 an_nonce_match_dis               : 1;
+		u64 br_ber_mon_dis                   : 1;
+		u64 reserved_31                      : 1;
+		u64 us_clk_period                    : 12;
+		u64 ms_clk_period                    : 12;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_cgxx_spu_dbg_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPU_DBG_CONTROL
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPU_DBG_CONTROL
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10300;
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu_sds#_skew_status
+ *
+ * CGX SPU SerDes Lane Skew Status Registers
+ * This register provides SerDes lane skew status. One register per physical SerDes lane.
+ */
+union cavm_cgxx_spu_sdsx_skew_status {
+	u64 u;
+	struct cavm_cgxx_spu_sdsx_skew_status_s {
+		u64 skew_status                      : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_cgxx_spu_sdsx_skew_status_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPU_SDSX_SKEW_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPU_SDSX_SKEW_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10340 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu_sds#_states
+ *
+ * CGX SPU SerDes States Registers
+ * This register provides SerDes lane states. One register per physical SerDes lane.
+ */
+union cavm_cgxx_spu_sdsx_states {
+	u64 u;
+	struct cavm_cgxx_spu_sdsx_states_s {
+		u64 bx_sync_sm                       : 4;
+		u64 br_sh_cnt                        : 11;
+		u64 br_block_lock                    : 1;
+		u64 br_sh_invld_cnt                  : 7;
+		u64 reserved_23                      : 1;
+		u64 fec_sync_cnt                     : 4;
+		u64 fec_block_sync                   : 1;
+		u64 reserved_29                      : 1;
+		u64 an_rx_sm                         : 2;
+		u64 an_arb_sm                        : 3;
+		u64 reserved_35                      : 1;
+		u64 train_lock_bad_markers           : 3;
+		u64 train_lock_found_1st_marker      : 1;
+		u64 train_frame_lock                 : 1;
+		u64 train_code_viol                  : 1;
+		u64 train_sm                         : 3;
+		u64 reserved_45_47                   : 3;
+		u64 am_lock_sm                       : 2;
+		u64 am_lock_invld_cnt                : 2;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_cgxx_spu_sdsx_states_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPU_SDSX_STATES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPU_SDSX_STATES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0x10360 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RSL) cgx#_spu_usxgmii_control
+ *
+ * CGX SPU Common USXGMII Control Register
+ * This register is the common control register that enables USXGMII Mode. The fields
+ * in this register are preserved across any LMAC soft-resets. For an LMAC in soft-
+ * reset state in USXGMII mode, the CGX will transmit Remote Fault BASE-R blocks.
+ */
+union cavm_cgxx_spu_usxgmii_control {
+	u64 u;
+	struct cavm_cgxx_spu_usxgmii_control_s {
+		u64 enable                           : 1;
+		u64 usxgmii_type                     : 3;
+		u64 sds_id                           : 2;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_cgxx_spu_usxgmii_control_s cn; */
+};
+
+static inline u64 CAVM_CGXX_SPU_USXGMII_CONTROL
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_CGXX_SPU_USXGMII_CONTROL
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10920;
+	return -1;
+}
+
+#endif /* __CAVM_CSRS_CGX_H__ */
diff --git a/drivers/net/cavium/octeontx2/cavm-csrs-lmt.h b/drivers/net/cavium/octeontx2/cavm-csrs-lmt.h
new file mode 100644
index 0000000000..a9260dfdc9
--- /dev/null
+++ b/drivers/net/cavium/octeontx2/cavm-csrs-lmt.h
@@ -0,0 +1,98 @@
+#ifndef __CAVM_CSRS_LMT_H__
+#define __CAVM_CSRS_LMT_H__
+/* This file is auto-generated. Do not edit */
+
+/***********************license start***************
+ * Copyright (c) 2003-2018  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM  NETWORKS MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Configuration and status register (CSR) address and type definitions for
+ * Cavium LMT.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ */
+
+/**
+ * Register (RVU_PFVF_BAR2) lmt_lf_lmtcancel
+ *
+ * RVU VF LMT Cancel Register
+ */
+union cavm_lmt_lf_lmtcancel {
+	u64 u;
+	struct cavm_lmt_lf_lmtcancel_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_lmt_lf_lmtcancel_s cn; */
+};
+
+static inline u64 CAVM_LMT_LF_LMTCANCEL
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_LMT_LF_LMTCANCEL
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x400;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) lmt_lf_lmtline#
+ *
+ * RVU VF LMT Line Registers
+ */
+union cavm_lmt_lf_lmtlinex {
+	u64 u;
+	struct cavm_lmt_lf_lmtlinex_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_lmt_lf_lmtlinex_s cn; */
+};
+
+static inline u64 CAVM_LMT_LF_LMTLINEX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_LMT_LF_LMTLINEX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0 + 8 * ((a) & 0xf);
+	return -1;
+}
+
+#endif /* __CAVM_CSRS_LMT_H__ */
diff --git a/drivers/net/cavium/octeontx2/cavm-csrs-nix.h b/drivers/net/cavium/octeontx2/cavm-csrs-nix.h
new file mode 100644
index 0000000000..46e9e35313
--- /dev/null
+++ b/drivers/net/cavium/octeontx2/cavm-csrs-nix.h
@@ -0,0 +1,10177 @@
+#ifndef __CAVM_CSRS_NIX_H__
+#define __CAVM_CSRS_NIX_H__
+/* This file is auto-generated. Do not edit */
+
+/***********************license start***************
+ * Copyright (c) 2003-2018  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM  NETWORKS MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Configuration and status register (CSR) address and type definitions for
+ * Cavium NIX.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ */
+
+/**
+ * Enumeration nix_af_int_vec_e
+ *
+ * NIX Admin Function Interrupt Vector Enumeration
+ * Enumerates the NIX AF MSI-X interrupt vectors.
+ */
+#define CAVM_NIX_AF_INT_VEC_E_AF_ERR (3)
+#define CAVM_NIX_AF_INT_VEC_E_AQ_DONE (2)
+#define CAVM_NIX_AF_INT_VEC_E_GEN (1)
+#define CAVM_NIX_AF_INT_VEC_E_POISON (4)
+#define CAVM_NIX_AF_INT_VEC_E_RVU (0)
+
+/**
+ * Enumeration nix_aq_comp_e
+ *
+ * NIX Completion Enumeration
+ * Enumerates the values of NIX_AQ_RES_S[COMPCODE].
+ */
+#define CAVM_NIX_AQ_COMP_E_CTX_FAULT (4)
+#define CAVM_NIX_AQ_COMP_E_CTX_POISON (3)
+#define CAVM_NIX_AQ_COMP_E_GOOD (1)
+#define CAVM_NIX_AQ_COMP_E_LOCKERR (5)
+#define CAVM_NIX_AQ_COMP_E_NOTDONE (0)
+#define CAVM_NIX_AQ_COMP_E_SQB_ALLOC_FAIL (6)
+#define CAVM_NIX_AQ_COMP_E_SWERR (2)
+
+/**
+ * Enumeration nix_aq_ctype_e
+ *
+ * NIX Context Type Enumeration
+ * Enumerates NIX_AQ_INST_S[CTYPE] values.
+ */
+#define CAVM_NIX_AQ_CTYPE_E_CQ (2)
+#define CAVM_NIX_AQ_CTYPE_E_DYNO (5)
+#define CAVM_NIX_AQ_CTYPE_E_MCE (3)
+#define CAVM_NIX_AQ_CTYPE_E_RQ (0)
+#define CAVM_NIX_AQ_CTYPE_E_RSS (4)
+#define CAVM_NIX_AQ_CTYPE_E_SQ (1)
+
+/**
+ * Enumeration nix_aq_instop_e
+ *
+ * NIX Admin Queue Opcode Enumeration
+ * Enumerates NIX_AQ_INST_S[OP] values.
+ */
+#define CAVM_NIX_AQ_INSTOP_E_INIT (1)
+#define CAVM_NIX_AQ_INSTOP_E_LOCK (4)
+#define CAVM_NIX_AQ_INSTOP_E_NOP (0)
+#define CAVM_NIX_AQ_INSTOP_E_READ (3)
+#define CAVM_NIX_AQ_INSTOP_E_UNLOCK (5)
+#define CAVM_NIX_AQ_INSTOP_E_WRITE (2)
+
+/**
+ * Enumeration nix_chan_e
+ *
+ * NIX Channel Number Enumeration
+ * Enumerates the receive and transmit channels, and values of
+ * NIX_RX_PARSE_S[CHAN], NIX_SQ_CTX_S[DEFAULT_CHAN]. CNXXXX implements a subset of
+ * these channels. Specifically, only channels for links enumerated by NIX_LINK_E
+ * are implemented.
+ *
+ * Internal:
+ * P2X/X2P channel enumeration for t9x.
+ */
+#define CAVM_NIX_CHAN_E_CGXX_LMACX_CHX(a, b, c)	\
+	(0x800 + 0x100 * (a) + 0x10 * (b) + (c))
+#define CAVM_NIX_CHAN_E_LBKX_CHX(a, b) (0 + 0x100 * (a) + (b))
+#define CAVM_NIX_CHAN_E_RX(a) (0 + 0x100 * (a))
+#define CAVM_NIX_CHAN_E_SDP_CHX(a) (0x700 + (a))
+
+/**
+ * Enumeration nix_colorresult_e
+ *
+ * NIX Color Result Enumeration
+ * Enumerates the values of NIX_MEM_RESULT_S[COLOR], NIX_AF_TL1()_MD_DEBUG1[COLOR]
+ * and NIX_AF_TL1()_MD_DEBUG1[COLOR].
+ */
+#define CAVM_NIX_COLORRESULT_E_GREEN (0)
+#define CAVM_NIX_COLORRESULT_E_RED_DROP (3)
+#define CAVM_NIX_COLORRESULT_E_RED_SEND (2)
+#define CAVM_NIX_COLORRESULT_E_YELLOW (1)
+
+/**
+ * Enumeration nix_cqerrint_e
+ *
+ * NIX Completion Queue Interrupt Enumeration
+ * Enumerates the bit index of NIX_CQ_CTX_S[CQ_ERR_INT,CQ_ERR_INT_ENA].
+ */
+#define CAVM_NIX_CQERRINT_E_CQE_FAULT (2)
+#define CAVM_NIX_CQERRINT_E_DOOR_ERR (0)
+#define CAVM_NIX_CQERRINT_E_WR_FULL (1)
+
+/**
+ * Enumeration nix_intf_e
+ *
+ * NIX Interface Number Enumeration
+ * Enumerates the bit index of NIX_AF_STATUS[CALIBRATE_STATUS].
+ */
+#define CAVM_NIX_INTF_E_CGXX(a) (0 + (a))
+#define CAVM_NIX_INTF_E_LBKX(a) (3 + (a))
+#define CAVM_NIX_INTF_E_SDP (4)
+
+/**
+ * Enumeration nix_lf_int_vec_e
+ *
+ * NIX Local Function Interrupt Vector Enumeration
+ * Enumerates the NIX MSI-X interrupt vectors per LF.
+ */
+#define CAVM_NIX_LF_INT_VEC_E_CINTX(a) (0x40 + (a))
+#define CAVM_NIX_LF_INT_VEC_E_ERR_INT (0x81)
+#define CAVM_NIX_LF_INT_VEC_E_GINT (0x80)
+#define CAVM_NIX_LF_INT_VEC_E_POISON (0x82)
+#define CAVM_NIX_LF_INT_VEC_E_QINTX(a) (0 + (a))
+
+/**
+ * Enumeration nix_link_e
+ *
+ * NIX Link Number Enumeration
+ * Enumerates the receive and transmit links, and LINK index of
+ * NIX_AF_RX_LINK()_CFG, NIX_AF_RX_LINK()_WRR_CFG,
+ * NIX_AF_TX_LINK()_NORM_CREDIT, NIX_AF_TX_LINK()_EXPR_CREDIT,
+ * NIX_AF_TX_LINK()_HW_XOFF and NIX_AF_TX_LINK()_SW_XOFF,
+ * NIX_AF_TL3_TL2()_LINK()_CFG.
+ */
+#define CAVM_NIX_LINK_E_CGXX_LMACX(a, b) (0 + 4 * (a) + (b))
+#define CAVM_NIX_LINK_E_LBKX(a) (0xc + (a))
+#define CAVM_NIX_LINK_E_MC (0xe)
+#define CAVM_NIX_LINK_E_SDP (0xd)
+
+/**
+ * Enumeration nix_lsoalg_e
+ *
+ * NIX Large Send Offload Algorithm Enumeration
+ * Enumerates NIX_AF_LSO_FORMAT()_FIELD()[ALG] values. Specifies algorithm for
+ * modifying the associated LSO packet field.
+ */
+#define CAVM_NIX_LSOALG_E_ADD_OFFSET (3)
+#define CAVM_NIX_LSOALG_E_ADD_PAYLEN (2)
+#define CAVM_NIX_LSOALG_E_ADD_SEGNUM (1)
+#define CAVM_NIX_LSOALG_E_NOP (0)
+#define CAVM_NIX_LSOALG_E_TCP_FLAGS (4)
+
+/**
+ * Enumeration nix_maxsqesz_e
+ *
+ * NIX Maximum SQE Size Enumeration
+ * Enumerates the values of NIX_SQ_CTX_S[MAX_SQE_SIZE].
+ */
+#define CAVM_NIX_MAXSQESZ_E_W16 (0)
+#define CAVM_NIX_MAXSQESZ_E_W8 (1)
+
+/**
+ * Enumeration nix_mdtype_e
+ *
+ * NIX Meta Descriptor Type Enumeration
+ * Enumerates values of NIX_AF_MDQ()_MD_DEBUG[MD_TYPE].
+ */
+#define CAVM_NIX_MDTYPE_E_FLUSH (1)
+#define CAVM_NIX_MDTYPE_E_PMD (2)
+#define CAVM_NIX_MDTYPE_E_RSVD (0)
+
+/**
+ * Enumeration nix_mnqerr_e
+ *
+ * NIX Meta-Descriptor Enqueue Error Enumeration
+ * Enumerates NIX_LF_MNQ_ERR_DBG[ERRCODE] values.
+ */
+#define CAVM_NIX_MNQERR_E_CQ_QUERY_ERR (6)
+#define CAVM_NIX_MNQERR_E_LSO_ERR (5)
+#define CAVM_NIX_MNQERR_E_MAXLEN_ERR (8)
+#define CAVM_NIX_MNQERR_E_MAX_SQE_SIZE_ERR (7)
+#define CAVM_NIX_MNQERR_E_SQB_FAULT (2)
+#define CAVM_NIX_MNQERR_E_SQB_POISON (3)
+#define CAVM_NIX_MNQERR_E_SQ_CTX_FAULT (0)
+#define CAVM_NIX_MNQERR_E_SQ_CTX_POISON (1)
+#define CAVM_NIX_MNQERR_E_TOTAL_ERR (4)
+
+/**
+ * Enumeration nix_ndc_rx_port_e
+ *
+ * NIX Receive NDC Port Enumeration
+ * Enumerates NIX receive NDC (NDC_IDX_E::NIX()_RX) ports and the PORT index of
+ * NDC_AF_PORT()_RT()_RW()_REQ_PC and NDC_AF_PORT()_RT()_RW()_LAT_PC.
+ */
+#define CAVM_NIX_NDC_RX_PORT_E_AQ (0)
+#define CAVM_NIX_NDC_RX_PORT_E_CINT (2)
+#define CAVM_NIX_NDC_RX_PORT_E_CQ (1)
+#define CAVM_NIX_NDC_RX_PORT_E_MC (3)
+#define CAVM_NIX_NDC_RX_PORT_E_PKT (4)
+#define CAVM_NIX_NDC_RX_PORT_E_RQ (5)
+
+/**
+ * Enumeration nix_ndc_tx_port_e
+ *
+ * NIX Transmit NDC Port Enumeration
+ * Enumerates NIX transmit NDC (NDC_IDX_E::NIX()_TX) ports and the PORT index of
+ * NDC_AF_PORT()_RT()_RW()_REQ_PC and NDC_AF_PORT()_RT()_RW()_LAT_PC.
+ */
+#define CAVM_NIX_NDC_TX_PORT_E_DEQ (3)
+#define CAVM_NIX_NDC_TX_PORT_E_DMA (4)
+#define CAVM_NIX_NDC_TX_PORT_E_ENQ (1)
+#define CAVM_NIX_NDC_TX_PORT_E_LMT (0)
+#define CAVM_NIX_NDC_TX_PORT_E_MNQ (2)
+#define CAVM_NIX_NDC_TX_PORT_E_XQE (5)
+
+/**
+ * Enumeration nix_re_opcode_e
+ *
+ * NIX Receive Error Opcode Enumeration
+ * Enumerates NIX_RX_PARSE_S[ERRCODE] values when NIX_RX_PARSE_S[ERRLEV] =
+ * NPC_ERRLEV_E::RE.
+ */
+#define CAVM_NIX_RE_OPCODE_E_OL2_LENMISM (0x12)
+#define CAVM_NIX_RE_OPCODE_E_OVERSIZE (0x11)
+#define CAVM_NIX_RE_OPCODE_E_RE_DMAPKT (0xf)
+#define CAVM_NIX_RE_OPCODE_E_RE_FCS (7)
+#define CAVM_NIX_RE_OPCODE_E_RE_FCS_RCV (8)
+#define CAVM_NIX_RE_OPCODE_E_RE_JABBER (2)
+#define CAVM_NIX_RE_OPCODE_E_RE_NONE (0)
+#define CAVM_NIX_RE_OPCODE_E_RE_PARTIAL (1)
+#define CAVM_NIX_RE_OPCODE_E_RE_RX_CTL (0xb)
+#define CAVM_NIX_RE_OPCODE_E_RE_SKIP (0xc)
+#define CAVM_NIX_RE_OPCODE_E_RE_TERMINATE (9)
+#define CAVM_NIX_RE_OPCODE_E_UNDERSIZE (0x10)
+
+/**
+ * Enumeration nix_redalg_e
+ *
+ * NIX Red Algorithm Enumeration
+ * Enumerates the different algorithms of NIX_SEND_EXT_S[SHP_RA].
+ */
+#define CAVM_NIX_REDALG_E_DISCARD (3)
+#define CAVM_NIX_REDALG_E_SEND (1)
+#define CAVM_NIX_REDALG_E_STALL (2)
+#define CAVM_NIX_REDALG_E_STD (0)
+
+/**
+ * Enumeration nix_rqint_e
+ *
+ * NIX Receive Queue Interrupt Enumeration
+ * Enumerates the bit index of NIX_RQ_CTX_S[RQ_INT,RQ_INT_ENA].
+ */
+#define CAVM_NIX_RQINT_E_DROP (0)
+#define CAVM_NIX_RQINT_E_RX(a) (0 + (a))
+#define CAVM_NIX_RQINT_E_RED (1)
+
+/**
+ * Enumeration nix_rx_actionop_e
+ *
+ * NIX Receive Action Opcode Enumeration
+ * Enumerates the values of NIX_RX_ACTION_S[OP].
+ */
+#define CAVM_NIX_RX_ACTIONOP_E_DROP (0)
+#define CAVM_NIX_RX_ACTIONOP_E_MCAST (3)
+#define CAVM_NIX_RX_ACTIONOP_E_MIRROR (6)
+#define CAVM_NIX_RX_ACTIONOP_E_PF_FUNC_DROP (5)
+#define CAVM_NIX_RX_ACTIONOP_E_RSS (4)
+#define CAVM_NIX_RX_ACTIONOP_E_UCAST (1)
+#define CAVM_NIX_RX_ACTIONOP_E_UCAST_IPSEC (2)
+
+/**
+ * Enumeration nix_rx_mcop_e
+ *
+ * NIX Receive Multicast/Mirror Opcode Enumeration
+ * Enumerates the values of NIX_RX_MCE_S[OP].
+ */
+#define CAVM_NIX_RX_MCOP_E_RQ (0)
+#define CAVM_NIX_RX_MCOP_E_RSS (1)
+
+/**
+ * Enumeration nix_rx_perrcode_e
+ *
+ * NIX Receive Protocol Error Code Enumeration
+ * Enumerates NIX_RX_PARSE_S[ERRCODE] values when NIX_RX_PARSE_S[ERRLEV] =
+ * NPC_ERRLEV_E::NIX.
+ */
+#define CAVM_NIX_RX_PERRCODE_E_BUFS_OFLOW (0xa)
+#define CAVM_NIX_RX_PERRCODE_E_DATA_FAULT (8)
+#define CAVM_NIX_RX_PERRCODE_E_IL3_LEN (0x20)
+#define CAVM_NIX_RX_PERRCODE_E_IL4_CHK (0x22)
+#define CAVM_NIX_RX_PERRCODE_E_IL4_LEN (0x21)
+#define CAVM_NIX_RX_PERRCODE_E_IL4_PORT (0x23)
+#define CAVM_NIX_RX_PERRCODE_E_MCAST_FAULT (4)
+#define CAVM_NIX_RX_PERRCODE_E_MCAST_POISON (6)
+#define CAVM_NIX_RX_PERRCODE_E_MEMOUT (9)
+#define CAVM_NIX_RX_PERRCODE_E_MIRROR_FAULT (5)
+#define CAVM_NIX_RX_PERRCODE_E_MIRROR_POISON (7)
+#define CAVM_NIX_RX_PERRCODE_E_NPC_RESULT_ERR (2)
+#define CAVM_NIX_RX_PERRCODE_E_OL3_LEN (0x10)
+#define CAVM_NIX_RX_PERRCODE_E_OL4_CHK (0x12)
+#define CAVM_NIX_RX_PERRCODE_E_OL4_LEN (0x11)
+#define CAVM_NIX_RX_PERRCODE_E_OL4_PORT (0x13)
+
+/**
+ * Enumeration nix_send_status_e
+ *
+ * NIX Send Completion Status Enumeration
+ * Enumerates values of NIX_SEND_COMP_S[STATUS] and NIX_LF_SEND_ERR_DBG[ERRCODE].
+ * Internal:
+ * TODO.
+ */
+#define CAVM_NIX_SEND_STATUS_E_DATA_FAULT (0x16)
+#define CAVM_NIX_SEND_STATUS_E_DATA_POISON (0x17)
+#define CAVM_NIX_SEND_STATUS_E_GOOD (0)
+#define CAVM_NIX_SEND_STATUS_E_INVALID_SUBDC (0x14)
+#define CAVM_NIX_SEND_STATUS_E_JUMP_FAULT (7)
+#define CAVM_NIX_SEND_STATUS_E_JUMP_POISON (8)
+#define CAVM_NIX_SEND_STATUS_E_LOCK_VIOL (0x21)
+#define CAVM_NIX_SEND_STATUS_E_NPC_DROP_ACTION (0x20)
+#define CAVM_NIX_SEND_STATUS_E_NPC_MCAST_ABORT (0x24)
+#define CAVM_NIX_SEND_STATUS_E_NPC_MCAST_CHAN_ERR (0x23)
+#define CAVM_NIX_SEND_STATUS_E_NPC_UCAST_CHAN_ERR (0x22)
+#define CAVM_NIX_SEND_STATUS_E_NPC_VTAG_PTR_ERR (0x25)
+#define CAVM_NIX_SEND_STATUS_E_NPC_VTAG_SIZE_ERR (0x26)
+#define CAVM_NIX_SEND_STATUS_E_SEND_CRC_ERR (0x10)
+#define CAVM_NIX_SEND_STATUS_E_SEND_EXT_ERR (6)
+#define CAVM_NIX_SEND_STATUS_E_SEND_HDR_ERR (5)
+#define CAVM_NIX_SEND_STATUS_E_SEND_IMM_ERR (0x11)
+#define CAVM_NIX_SEND_STATUS_E_SEND_MEM_ERR (0x13)
+#define CAVM_NIX_SEND_STATUS_E_SEND_MEM_FAULT (0x27)
+#define CAVM_NIX_SEND_STATUS_E_SEND_SG_ERR (0x12)
+#define CAVM_NIX_SEND_STATUS_E_SQB_FAULT (3)
+#define CAVM_NIX_SEND_STATUS_E_SQB_POISON (4)
+#define CAVM_NIX_SEND_STATUS_E_SQ_CTX_FAULT (1)
+#define CAVM_NIX_SEND_STATUS_E_SQ_CTX_POISON (2)
+#define CAVM_NIX_SEND_STATUS_E_SUBDC_ORDER_ERR (0x15)
+
+/**
+ * Enumeration nix_sendcrcalg_e
+ *
+ * NIX Send CRC Algorithm Enumeration
+ * Enumerates the CRC algorithm used, see NIX_SEND_CRC_S[ALG].
+ */
+#define CAVM_NIX_SENDCRCALG_E_CRC32 (0)
+#define CAVM_NIX_SENDCRCALG_E_CRC32C (1)
+#define CAVM_NIX_SENDCRCALG_E_ONES16 (2)
+
+/**
+ * Enumeration nix_sendl3type_e
+ *
+ * NIX Send Layer 3 Header Type Enumeration
+ * Enumerates values of NIX_SEND_HDR_S[OL3TYPE], NIX_SEND_HDR_S[IL3TYPE].
+ * Internal:
+ * Encoding matches DPDK TX IP types:
+ * \<pre\>
+ * PKT_TX_IP_CKSUM      (1ULL \<\< 54)
+ * PKT_TX_IPV4          (1ULL \<\< 55)
+ * PKT_TX_IPV6          (1ULL \<\< 56)
+ *
+ * PKT_TX_OUTER_IP_CKSUM(1ULL \<\< 58)
+ * PKT_TX_OUTER_IPV4    (1ULL \<\< 59)
+ * PKT_TX_OUTER_IPV6    (1ULL \<\< 60)
+ * \</pre\>
+ */
+#define CAVM_NIX_SENDL3TYPE_E_IP4 (2)
+#define CAVM_NIX_SENDL3TYPE_E_IP4_CKSUM (3)
+#define CAVM_NIX_SENDL3TYPE_E_IP6 (4)
+#define CAVM_NIX_SENDL3TYPE_E_NONE (0)
+
+/**
+ * Enumeration nix_sendl4type_e
+ *
+ * NIX Send Layer 4 Header Type Enumeration
+ * Enumerates values of NIX_SEND_HDR_S[OL4TYPE], NIX_SEND_HDR_S[IL4TYPE].
+ * Internal:
+ * Encoding matches DPDK TX L4 types.
+ * \<pre\>
+ * PKT_TX_L4_NO_CKSUM   (0ULL \<\< 52)  // Disable L4 cksum of TX pkt.
+ * PKT_TX_TCP_CKSUM     (1ULL \<\< 52)  // TCP cksum of TX pkt. computed by nic.
+ * PKT_TX_SCTP_CKSUM    (2ULL \<\< 52)  // SCTP cksum of TX pkt. computed by nic.
+ * PKT_TX_UDP_CKSUM     (3ULL \<\< 52)  // UDP cksum of TX pkt. computed by nic.
+ * \</pre\>
+ */
+#define CAVM_NIX_SENDL4TYPE_E_NONE (0)
+#define CAVM_NIX_SENDL4TYPE_E_SCTP_CKSUM (2)
+#define CAVM_NIX_SENDL4TYPE_E_TCP_CKSUM (1)
+#define CAVM_NIX_SENDL4TYPE_E_UDP_CKSUM (3)
+
+/**
+ * Enumeration nix_sendldtype_e
+ *
+ * NIX Send Load Type Enumeration
+ * Enumerates the load transaction types for reading segment bytes specified by
+ * NIX_SEND_SG_S[LD_TYPE] and NIX_SEND_JUMP_S[LD_TYPE].
+ *
+ * Internal:
+ * The hardware implementation treats undefined encodings as LDD load type.
+ */
+#define CAVM_NIX_SENDLDTYPE_E_LDD (0)
+#define CAVM_NIX_SENDLDTYPE_E_LDT (1)
+#define CAVM_NIX_SENDLDTYPE_E_LDWB (2)
+
+/**
+ * Enumeration nix_sendmemalg_e
+ *
+ * NIX Memory Modify Algorithm Enumeration
+ * Enumerates the different algorithms for modifying memory; see
+ * NIX_SEND_MEM_S[ALG]. mbufs_freed is the number of gather buffers freed to NPA
+ * for the send descriptor. See NIX_SEND_HDR_S[DF] and NIX_SEND_SG_S[I*].
+ */
+#define CAVM_NIX_SENDMEMALG_E_ADD (8)
+#define CAVM_NIX_SENDMEMALG_E_ADDLEN (0xa)
+#define CAVM_NIX_SENDMEMALG_E_ADDMBUF (0xc)
+#define CAVM_NIX_SENDMEMALG_E_SET (0)
+#define CAVM_NIX_SENDMEMALG_E_SETRSLT (2)
+#define CAVM_NIX_SENDMEMALG_E_SETTSTMP (1)
+#define CAVM_NIX_SENDMEMALG_E_SUB (9)
+#define CAVM_NIX_SENDMEMALG_E_SUBLEN (0xb)
+#define CAVM_NIX_SENDMEMALG_E_SUBMBUF (0xd)
+
+/**
+ * Enumeration nix_sendmemdsz_e
+ *
+ * NIX Memory Data Size Enumeration
+ * Enumerates the datum size for modifying memory; see NIX_SEND_MEM_S[DSZ].
+ */
+#define CAVM_NIX_SENDMEMDSZ_E_B16 (2)
+#define CAVM_NIX_SENDMEMDSZ_E_B32 (1)
+#define CAVM_NIX_SENDMEMDSZ_E_B64 (0)
+#define CAVM_NIX_SENDMEMDSZ_E_B8 (3)
+
+/**
+ * Enumeration nix_sqint_e
+ *
+ * NIX Send Queue Interrupt Enumeration
+ * Enumerates the bit index of NIX_SQ_CTX_S[SQ_INT,SQ_INT_ENA].
+ */
+#define CAVM_NIX_SQINT_E_LMT_ERR (0)
+#define CAVM_NIX_SQINT_E_MNQ_ERR (1)
+#define CAVM_NIX_SQINT_E_SEND_ERR (2)
+#define CAVM_NIX_SQINT_E_SQB_ALLOC_FAIL (3)
+
+/**
+ * Enumeration nix_sqoperr_e
+ *
+ * NIX SQ Operation Error Enumeration
+ * Enumerates NIX_LF_SQ_OP_ERR_DBG[ERRCODE] values.
+ */
+#define CAVM_NIX_SQOPERR_E_MAX_SQE_SIZE_ERR (4)
+#define CAVM_NIX_SQOPERR_E_SQB_FAULT (7)
+#define CAVM_NIX_SQOPERR_E_SQB_NULL (6)
+#define CAVM_NIX_SQOPERR_E_SQE_OFLOW (5)
+#define CAVM_NIX_SQOPERR_E_SQ_CTX_FAULT (1)
+#define CAVM_NIX_SQOPERR_E_SQ_CTX_POISON (2)
+#define CAVM_NIX_SQOPERR_E_SQ_DISABLED (3)
+#define CAVM_NIX_SQOPERR_E_SQ_OOR (0)
+
+/**
+ * Enumeration nix_stat_lf_rx_e
+ *
+ * NIX Local Function Receive Statistics Enumeration
+ * Enumerates the last index of NIX_AF_LF()_RX_STAT() and NIX_LF_RX_STAT().
+ */
+#define CAVM_NIX_STAT_LF_RX_E_RX_BCAST (2)
+#define CAVM_NIX_STAT_LF_RX_E_RX_DROP (4)
+#define CAVM_NIX_STAT_LF_RX_E_RX_DROP_OCTS (5)
+#define CAVM_NIX_STAT_LF_RX_E_RX_DRP_BCAST (8)
+#define CAVM_NIX_STAT_LF_RX_E_RX_DRP_L3BCAST (0xa)
+#define CAVM_NIX_STAT_LF_RX_E_RX_DRP_L3MCAST (0xb)
+#define CAVM_NIX_STAT_LF_RX_E_RX_DRP_MCAST (9)
+#define CAVM_NIX_STAT_LF_RX_E_RX_ERR (7)
+#define CAVM_NIX_STAT_LF_RX_E_RX_FCS (6)
+#define CAVM_NIX_STAT_LF_RX_E_RX_MCAST (3)
+#define CAVM_NIX_STAT_LF_RX_E_RX_OCTS (0)
+#define CAVM_NIX_STAT_LF_RX_E_RX_UCAST (1)
+
+/**
+ * Enumeration nix_stat_lf_tx_e
+ *
+ * NIX Local Function Transmit Statistics Enumeration
+ * Enumerates the index of NIX_AF_LF()_TX_STAT() and NIX_LF_TX_STAT().
+ * These statistics do not account for packet replication due to
+ * NIX_TX_ACTION_S[OP] = NIX_TX_ACTIONOP_E::MCAST.
+ */
+#define CAVM_NIX_STAT_LF_TX_E_TX_BCAST (1)
+#define CAVM_NIX_STAT_LF_TX_E_TX_DROP (3)
+#define CAVM_NIX_STAT_LF_TX_E_TX_MCAST (2)
+#define CAVM_NIX_STAT_LF_TX_E_TX_OCTS (4)
+#define CAVM_NIX_STAT_LF_TX_E_TX_UCAST (0)
+
+/**
+ * Enumeration nix_stype_e
+ *
+ * NIX SQB Caching Type Enumeration
+ * Enumerates the values of NIX_SQ_CTX_S[SQE_STYPE].
+ */
+#define CAVM_NIX_STYPE_E_STF (0)
+#define CAVM_NIX_STYPE_E_STP (2)
+#define CAVM_NIX_STYPE_E_STT (1)
+
+/**
+ * Enumeration nix_subdc_e
+ *
+ * NIX Subdescriptor Operation Enumeration
+ * Enumerates send and receive subdescriptor codes. The codes differentiate
+ * subdescriptors within a NIX send or receive descriptor, excluding NIX_SEND_HDR_S for
+ * send and NIX_CQE_HDR_S/NIX_WQE_HDR_S for receive, which are determined by their
+ * position as the first subdescriptor, and NIX_RX_PARSE_S, which is determined by its
+ * position as the second subdescriptor.
+ */
+#define CAVM_NIX_SUBDC_E_CRC (2)
+#define CAVM_NIX_SUBDC_E_EXT (1)
+#define CAVM_NIX_SUBDC_E_IMM (3)
+#define CAVM_NIX_SUBDC_E_JUMP (6)
+#define CAVM_NIX_SUBDC_E_MEM (5)
+#define CAVM_NIX_SUBDC_E_NOP (0)
+#define CAVM_NIX_SUBDC_E_SG (4)
+#define CAVM_NIX_SUBDC_E_SOD (0xf)
+#define CAVM_NIX_SUBDC_E_WORK (7)
+
+/**
+ * Enumeration nix_tx_actionop_e
+ *
+ * NIX Transmit Action Opcode Enumeration
+ * Enumerates the values of NIX_TX_ACTION_S[OP].
+ */
+#define CAVM_NIX_TX_ACTIONOP_E_DROP (0)
+#define CAVM_NIX_TX_ACTIONOP_E_DROP_VIOL (5)
+#define CAVM_NIX_TX_ACTIONOP_E_MCAST (3)
+#define CAVM_NIX_TX_ACTIONOP_E_UCAST_CHAN (2)
+#define CAVM_NIX_TX_ACTIONOP_E_UCAST_DEFAULT (1)
+
+/**
+ * Enumeration nix_tx_vtagop_e
+ *
+ * NIX Transmit Vtag Opcode Enumeration
+ * Enumerates the values of NIX_TX_VTAG_ACTION_S[VTAG0_OP,VTAG1_OP].
+ */
+#define CAVM_NIX_TX_VTAGOP_E_INSERT (1)
+#define CAVM_NIX_TX_VTAGOP_E_NOP (0)
+#define CAVM_NIX_TX_VTAGOP_E_REPLACE (2)
+
+/**
+ * Enumeration nix_txlayer_e
+ *
+ * NIX Transmit Layer Enumeration
+ * Enumerates the values of NIX_AF_LSO_FORMAT()_FIELD()[LAYER].
+ */
+#define CAVM_NIX_TXLAYER_E_IL3 (2)
+#define CAVM_NIX_TXLAYER_E_IL4 (3)
+#define CAVM_NIX_TXLAYER_E_OL3 (0)
+#define CAVM_NIX_TXLAYER_E_OL4 (1)
+
+/**
+ * Enumeration nix_vtagsize_e
+ *
+ * NIX Vtag Size Enumeration
+ * Enumerates the values of NIX_AF_TX_VTAG_DEF()_CTL[SIZE] and NIX_AF_LF()_RX_VTAG_TYPE()[SIZE].
+ */
+#define CAVM_NIX_VTAGSIZE_E_T4 (0)
+#define CAVM_NIX_VTAGSIZE_E_T8 (1)
+
+/**
+ * Enumeration nix_xqe_type_e
+ *
+ * NIX WQE/CQE Type Enumeration
+ * Enumerates the values of NIX_WQE_HDR_S[WQE_TYPE], NIX_CQE_HDR_S[CQE_TYPE].
+ */
+#define CAVM_NIX_XQE_TYPE_E_INVALID (0)
+#define CAVM_NIX_XQE_TYPE_E_RX (1)
+#define CAVM_NIX_XQE_TYPE_E_RX_IPSECD (4)
+#define CAVM_NIX_XQE_TYPE_E_RX_IPSECH (3)
+#define CAVM_NIX_XQE_TYPE_E_RX_IPSECS (2)
+#define CAVM_NIX_XQE_TYPE_E_SEND (8)
+
+/**
+ * Enumeration nix_xqesz_e
+ *
+ * NIX WQE/CQE Size Enumeration
+ * Enumerates the values of NIX_AF_LF()_CFG[XQE_SIZE].
+ */
+#define CAVM_NIX_XQESZ_E_W16 (1)
+#define CAVM_NIX_XQESZ_E_W64 (0)
+
+/**
+ * Structure nix_aq_inst_s
+ *
+ * NIX Admin Queue Instruction Structure
+ * This structure specifies the AQ instruction.
+ * Instructions and associated software structures are stored in memory as
+ * little-endian unless NIX_AF_CFG[AF_BE] is set.
+ *
+ * Hardware reads of NIX_AQ_INST_S do not allocate into LLC.
+ *
+ * Hardware reads and writes of the context structure selected by [CTYPE], [LF]
+ * and [CINDEX] use the NDC and LLC caching style configured for that context. For
+ * example:
+ * * When [CTYPE] = NIX_AQ_CTYPE_E::RQ: use NIX_AF_LF()_RSS_CFG[CACHING] and
+ * NIX_AF_LF()_RSS_CFG[WAY_MASK].
+ * * When [CTYPE] = NIX_AQ_CTYPE_E::MCE: use NIX_AF_RX_MCAST_CFG[CACHING] and
+ * NIX_AF_RX_MCAST_CFG[WAY_MASK].
+ */
+union cavm_nix_aq_inst_s {
+	u64 u[2];
+	struct cavm_nix_aq_inst_s_s {
+		u64 op                               : 4;
+		u64 ctype                            : 4;
+		u64 lf                               : 7;
+		u64 reserved_15_23                   : 9;
+		u64 cindex                           : 20;
+		u64 reserved_44_62                   : 19;
+		u64 doneint                          : 1;
+		u64 res_addr                         : 64;
+	} s;
+	/* struct cavm_nix_aq_inst_s_s cn; */
+};
+
+/**
+ * Structure nix_aq_res_s
+ *
+ * NIX Admin Queue Result Structure
+ * NIX writes this structure after it completes the NIX_AQ_INST_S instruction.
+ * The result structure is exactly 16 bytes, and each instruction completion produces
+ * exactly one result structure.
+ *
+ * Results and associated software structures are stored in memory as
+ * little-endian unless NIX_AF_CFG[AF_BE] is set.
+ *
+ * When [OP] = NIX_AQ_INSTOP_E::INIT, WRITE or READ, this structure is
+ * immediately followed by context read or write data. See NIX_AQ_INSTOP_E.
+ *
+ * Hardware writes of NIX_AQ_RES_S and context data always allocate into LLC.
+ * Hardware reads of context data do not allocate into LLC.
+ */
+union cavm_nix_aq_res_s {
+	u64 u[2];
+	struct cavm_nix_aq_res_s_s {
+		u64 op                               : 4;
+		u64 ctype                            : 4;
+		u64 compcode                         : 8;
+		u64 doneint                          : 1;
+		u64 reserved_17_63                   : 47;
+		u64 reserved_64_127                  : 64;
+	} s;
+	/* struct cavm_nix_aq_res_s_s cn; */
+};
+
+/**
+ * Structure nix_cint_hw_s
+ *
+ * NIX Completion Interrupt Context Hardware Structure
+ * This structure contains context state maintained by hardware for each
+ * completion interrupt (CINT) in NDC/LLC/DRAM. Software accesses this structure
+ * with the NIX_LF_CINT()* registers.
+ * Hardware maintains a table of NIX_AF_CONST2[CINTS] contiguous NIX_CINT_HW_S
+ * structures per LF starting at AF IOVA NIX_AF_LF()_CINTS_BASE.
+ * Always stored in byte invariant little-endian format (LE8).
+ */
+union cavm_nix_cint_hw_s {
+	u64 u[2];
+	struct cavm_nix_cint_hw_s_s {
+		u64 ecount                           : 32;
+		u64 qcount                           : 16;
+		u64 intr                             : 1;
+		u64 ena                              : 1;
+		u64 timer_idx                        : 8;
+		u64 reserved_58_63                   : 6;
+		u64 ecount_wait                      : 32;
+		u64 qcount_wait                      : 16;
+		u64 time_wait                        : 8;
+		u64 reserved_120_127                 : 8;
+	} s;
+	/* struct cavm_nix_cint_hw_s_s cn; */
+};
+
+/**
+ * Structure nix_cq_ctx_s
+ *
+ * NIX Completion Queue Context Structure
+ * This structure contains context state maintained by hardware for each CQ in
+ * NDC/LLC/DRAM.
+ * Software uses the same structure format to read and write an CQ context with
+ * the NIX admin queue.
+ */
+union cavm_nix_cq_ctx_s {
+	u64 u[4];
+	struct cavm_nix_cq_ctx_s_s {
+		u64 base                             : 64;
+		u64 reserved_64_67                   : 4;
+		u64 bp_ena                           : 1;
+		u64 reserved_69_71                   : 3;
+		u64 bpid                             : 9;
+		u64 reserved_81_83                   : 3;
+		u64 qint_idx                         : 7;
+		u64 cq_err                           : 1;
+		u64 cint_idx                         : 7;
+		u64 avg_con                          : 9;
+		u64 wrptr                            : 20;
+		u64 tail                             : 20;
+		u64 head                             : 20;
+		u64 avg_level                        : 8;
+		u64 update_time                      : 16;
+		u64 bp                               : 8;
+		u64 drop                             : 8;
+		u64 drop_ena                         : 1;
+		u64 ena                              : 1;
+		u64 reserved_210_211                 : 2;
+		u64 substream                        : 20;
+		u64 caching                          : 1;
+		u64 reserved_233_235                 : 3;
+		u64 qsize                            : 4;
+		u64 cq_err_int                       : 8;
+		u64 cq_err_int_ena                   : 8;
+	} s;
+	/* struct cavm_nix_cq_ctx_s_s cn; */
+};
+
+/**
+ * Structure nix_cqe_hdr_s
+ *
+ * NIX Completion Queue Entry Header Structure
+ * This 64-bit structure defines the first word of every CQE. It is immediately
+ * followed by NIX_RX_PARSE_S in a receive CQE, and by NIX_SEND_COMP_S in a send
+ * completion CQE.
+ * Stored in memory as little-endian unless NIX_AF_LF()_CFG[BE] is set.
+ */
+union cavm_nix_cqe_hdr_s {
+	u64 u;
+	struct cavm_nix_cqe_hdr_s_s {
+		u64 tag                              : 32;
+		u64 q                                : 20;
+		u64 reserved_52_57                   : 6;
+		u64 node                             : 2;
+		u64 cqe_type                         : 4;
+	} s;
+	/* struct cavm_nix_cqe_hdr_s_s cn; */
+};
+
+/**
+ * Structure nix_inst_hdr_s
+ *
+ * NIX Instruction Header Structure
+ * This structure defines the instruction header that precedes the packet header
+ * supplied to NPC for packets to be transmitted by NIX.
+ */
+union cavm_nix_inst_hdr_s {
+	u64 u;
+	struct cavm_nix_inst_hdr_s_s {
+		u64 pf_func                          : 16;
+		u64 sq                               : 20;
+		u64 reserved_36_63                   : 28;
+	} s;
+	/* struct cavm_nix_inst_hdr_s_s cn; */
+};
+
+/**
+ * Structure nix_iova_s
+ *
+ * NIX I/O Virtual Address Structure
+ */
+union cavm_nix_iova_s {
+	u64 u;
+	struct cavm_nix_iova_s_s {
+		u64 addr                             : 64;
+	} s;
+	/* struct cavm_nix_iova_s_s cn; */
+};
+
+/**
+ * Structure nix_ipsec_dyno_s
+ *
+ * NIX IPSEC Dynamic Ordering Counter Structure
+ * This structure specifies the format of an IPSEC dynamic ordering (DYNO) counter
+ * in NDC/LLC/DRAM. See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA].
+ * Software uses the same structure format to read and write a DYNO counter with
+ * the NIX admin queue.
+ */
+union cavm_nix_ipsec_dyno_s {
+	u32 u;
+	struct cavm_nix_ipsec_dyno_s_s {
+		u32 count                            : 32;
+	} s;
+	/* struct cavm_nix_ipsec_dyno_s_s cn; */
+};
+
+/**
+ * Structure nix_mem_result_s
+ *
+ * NIX Memory Value Structure
+ * When NIX_SEND_MEM_S[ALG]=NIX_SENDMEMALG_E::SETRSLT, the value written to memory is formed with
+ * this structure.
+ */
+union cavm_nix_mem_result_s {
+	u64 u;
+	struct cavm_nix_mem_result_s_s {
+		u64 v                                : 1;
+		u64 color                            : 2;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_nix_mem_result_s_s cn; */
+};
+
+/**
+ * Structure nix_op_q_wdata_s
+ *
+ * NIX Statistics Operation Write Data Structure
+ * This structure specifies the write data format of an atomic 64-bit load-and-add
+ * of some NIX_LF_RQ_OP_*, NIX_LF_SQ_OP* and NIX_LF_CQ_OP* registers.
+ */
+union cavm_nix_op_q_wdata_s {
+	u64 u;
+	struct cavm_nix_op_q_wdata_s_s {
+		u64 reserved_0_31                    : 32;
+		u64 q                                : 20;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_nix_op_q_wdata_s_s cn; */
+};
+
+/**
+ * Structure nix_qint_hw_s
+ *
+ * NIX Queue Interrupt Context Hardware Structure
+ * This structure contains context state maintained by hardware for each queue
+ * interrupt (QINT) in NDC/LLC/DRAM. Software accesses this structure with the
+ * NIX_LF_QINT()* registers.
+ * Hardware maintains a table of NIX_AF_CONST2[QINTS] contiguous NIX_QINT_HW_S
+ * structures per LF starting at IOVA NIX_AF_LF()_QINTS_BASE.
+ * Always stored in byte invariant little-endian format (LE8).
+ */
+union cavm_nix_qint_hw_s {
+	u32 u;
+	struct cavm_nix_qint_hw_s_s {
+		u32 count                            : 22;
+		u32 reserved_22_30                   : 9;
+		u32 ena                              : 1;
+	} s;
+	/* struct cavm_nix_qint_hw_s_s cn; */
+};
+
+/**
+ * Structure nix_rq_ctx_hw_s
+ *
+ * NIX Receive Queue Context Structure
+ * This structure contains context state maintained by hardware for each RQ in
+ * NDC/LLC/DRAM. Software uses the equivalent NIX_RQ_CTX_S structure format to
+ * read and write an RQ context with the NIX admin queue.
+ * Always stored in byte invariant little-endian format (LE8).
+ */
+union cavm_nix_rq_ctx_hw_s {
+	u64 u[16];
+	struct cavm_nix_rq_ctx_hw_s_s {
+		u64 ena                              : 1;
+		u64 sso_ena                          : 1;
+		u64 ipsech_ena                       : 1;
+		u64 ena_wqwd                         : 1;
+		u64 cq                               : 20;
+		u64 substream                        : 20;
+		u64 wqe_aura                         : 20;
+		u64 spb_aura                         : 20;
+		u64 lpb_aura                         : 20;
+		u64 sso_grp                          : 10;
+		u64 sso_tt                           : 2;
+		u64 pb_caching                       : 2;
+		u64 wqe_caching                      : 1;
+		u64 xqe_drop_ena                     : 1;
+		u64 spb_drop_ena                     : 1;
+		u64 lpb_drop_ena                     : 1;
+		u64 wqe_skip                         : 2;
+		u64 reserved_124_127                 : 4;
+		u64 reserved_128_139                 : 12;
+		u64 spb_sizem1                       : 6;
+		u64 reserved_146_150                 : 5;
+		u64 spb_ena                          : 1;
+		u64 lpb_sizem1                       : 12;
+		u64 first_skip                       : 7;
+		u64 reserved_171                     : 1;
+		u64 later_skip                       : 6;
+		u64 xqe_imm_size                     : 6;
+		u64 reserved_184_189                 : 6;
+		u64 xqe_imm_copy                     : 1;
+		u64 xqe_hdr_split                    : 1;
+		u64 xqe_drop                         : 8;
+		u64 xqe_pass                         : 8;
+		u64 wqe_pool_drop                    : 8;
+		u64 wqe_pool_pass                    : 8;
+		u64 spb_aura_drop                    : 8;
+		u64 spb_aura_pass                    : 8;
+		u64 spb_pool_drop                    : 8;
+		u64 spb_pool_pass                    : 8;
+		u64 lpb_aura_drop                    : 8;
+		u64 lpb_aura_pass                    : 8;
+		u64 lpb_pool_drop                    : 8;
+		u64 lpb_pool_pass                    : 8;
+		u64 reserved_288_319                 : 32;
+		u64 ltag                             : 24;
+		u64 good_utag                        : 8;
+		u64 bad_utag                         : 8;
+		u64 flow_tagw                        : 6;
+		u64 reserved_366_383                 : 18;
+		u64 octs                             : 48;
+		u64 reserved_432_447                 : 16;
+		u64 pkts                             : 48;
+		u64 reserved_496_511                 : 16;
+		u64 drop_octs                        : 48;
+		u64 reserved_560_575                 : 16;
+		u64 drop_pkts                        : 48;
+		u64 reserved_624_639                 : 16;
+		u64 re_pkts                          : 48;
+		u64 reserved_688_702                 : 15;
+		u64 ena_copy                         : 1;
+		u64 reserved_704_739                 : 36;
+		u64 rq_int                           : 8;
+		u64 rq_int_ena                       : 8;
+		u64 qint_idx                         : 7;
+		u64 reserved_763_767                 : 5;
+		u64 reserved_768_831                 : 64;
+		u64 reserved_832_895                 : 64;
+		u64 reserved_896_959                 : 64;
+		u64 reserved_960_1023                : 64;
+	} s;
+	/* struct cavm_nix_rq_ctx_hw_s_s cn; */
+};
+
+/**
+ * Structure nix_rq_ctx_s
+ *
+ * NIX Receive Queue Context Structure
+ * This structure specifies the format used by software to read and write an RQ context with
+ * the NIX admin queue.
+ */
+union cavm_nix_rq_ctx_s {
+	u64 u[16];
+	struct cavm_nix_rq_ctx_s_s {
+		u64 ena                              : 1;
+		u64 sso_ena                          : 1;
+		u64 ipsech_ena                       : 1;
+		u64 ena_wqwd                         : 1;
+		u64 cq                               : 20;
+		u64 substream                        : 20;
+		u64 wqe_aura                         : 20;
+		u64 spb_aura                         : 20;
+		u64 lpb_aura                         : 20;
+		u64 sso_grp                          : 10;
+		u64 sso_tt                           : 2;
+		u64 pb_caching                       : 2;
+		u64 wqe_caching                      : 1;
+		u64 xqe_drop_ena                     : 1;
+		u64 spb_drop_ena                     : 1;
+		u64 lpb_drop_ena                     : 1;
+		u64 reserved_122_127                 : 6;
+		u64 reserved_128_139                 : 12;
+		u64 spb_sizem1                       : 6;
+		u64 wqe_skip                         : 2;
+		u64 reserved_148_150                 : 3;
+		u64 spb_ena                          : 1;
+		u64 lpb_sizem1                       : 12;
+		u64 first_skip                       : 7;
+		u64 reserved_171                     : 1;
+		u64 later_skip                       : 6;
+		u64 xqe_imm_size                     : 6;
+		u64 reserved_184_189                 : 6;
+		u64 xqe_imm_copy                     : 1;
+		u64 xqe_hdr_split                    : 1;
+		u64 xqe_drop                         : 8;
+		u64 xqe_pass                         : 8;
+		u64 wqe_pool_drop                    : 8;
+		u64 wqe_pool_pass                    : 8;
+		u64 spb_aura_drop                    : 8;
+		u64 spb_aura_pass                    : 8;
+		u64 spb_pool_drop                    : 8;
+		u64 spb_pool_pass                    : 8;
+		u64 lpb_aura_drop                    : 8;
+		u64 lpb_aura_pass                    : 8;
+		u64 lpb_pool_drop                    : 8;
+		u64 lpb_pool_pass                    : 8;
+		u64 reserved_288_291                 : 4;
+		u64 rq_int                           : 8;
+		u64 rq_int_ena                       : 8;
+		u64 qint_idx                         : 7;
+		u64 reserved_315_319                 : 5;
+		u64 ltag                             : 24;
+		u64 good_utag                        : 8;
+		u64 bad_utag                         : 8;
+		u64 flow_tagw                        : 6;
+		u64 reserved_366_383                 : 18;
+		u64 octs                             : 48;
+		u64 reserved_432_447                 : 16;
+		u64 pkts                             : 48;
+		u64 reserved_496_511                 : 16;
+		u64 drop_octs                        : 48;
+		u64 reserved_560_575                 : 16;
+		u64 drop_pkts                        : 48;
+		u64 reserved_624_639                 : 16;
+		u64 re_pkts                          : 48;
+		u64 reserved_688_703                 : 16;
+		u64 reserved_704_767                 : 64;
+		u64 reserved_768_831                 : 64;
+		u64 reserved_832_895                 : 64;
+		u64 reserved_896_959                 : 64;
+		u64 reserved_960_1023                : 64;
+	} s;
+	/* struct cavm_nix_rq_ctx_s_s cn; */
+};
+
+/**
+ * Structure nix_rsse_s
+ *
+ * NIX Receive Side Scaling Entry Structure
+ * This structure specifies the format of each hardware entry in the NIX RSS
+ * tables in NDC/LLC/DRAM. See NIX_AF_LF()_RSS_BASE and NIX_AF_LF()_RSS_GRP().
+ * Software uses the same structure format to read and write an RSS table entry
+ * with the NIX admin queue.
+ */
+union cavm_nix_rsse_s {
+	u32 u;
+	struct cavm_nix_rsse_s_s {
+		u32 rq                               : 20;
+		u32 reserved_20_31                   : 12;
+	} s;
+	/* struct cavm_nix_rsse_s_s cn; */
+};
+
+/**
+ * Structure nix_rx_action_s
+ *
+ * NIX Receive Action Structure
+ * This structure defines the format of NPC_RESULT_S[ACTION] for a receive packet.
+ */
+union cavm_nix_rx_action_s {
+	u64 u;
+	struct cavm_nix_rx_action_s_s {
+		u64 op                               : 4;
+		u64 pf_func                          : 16;
+		u64 index                            : 20;
+		u64 match_id                         : 16;
+		u64 flow_key_alg                     : 5;
+		u64 reserved_61_63                   : 3;
+	} s;
+	/* struct cavm_nix_rx_action_s_s cn; */
+};
+
+/**
+ * Structure nix_rx_imm_s
+ *
+ * NIX Receive Immediate Subdescriptor Structure
+ * The receive immediate subdescriptor indicates that bytes immediately following this
+ * NIX_RX_IMM_S (after skipping [APAD] bytes) were saved from the received packet. The
+ * next subdescriptor following this NIX_RX_IMM_S (when one exists) will follow the
+ * immediate bytes, after rounding up the address to a multiple of 16 bytes.
+ */
+union cavm_nix_rx_imm_s {
+	u64 u;
+	struct cavm_nix_rx_imm_s_s {
+		u64 size                             : 16;
+		u64 apad                             : 3;
+		u64 reserved_19_59                   : 41;
+		u64 subdc                            : 4;
+	} s;
+	/* struct cavm_nix_rx_imm_s_s cn; */
+};
+
+/**
+ * Structure nix_rx_mce_s
+ *
+ * NIX Receive Multicast/Mirror Entry Structure
+ * This structure specifies the format of entries in the NIX receive
+ * multicast/mirror table maintained by hardware in NDC/LLC/DRAM. See
+ * NIX_AF_RX_MCAST_BASE and NIX_AF_RX_MCAST_CFG. Note the table may contain both
+ * multicast and mirror replication lists.
+ * Software uses the same structure format to read and write a multicast/mirror
+ * table entry with the NIX admin queue.
+ */
+union cavm_nix_rx_mce_s {
+	u64 u;
+	struct cavm_nix_rx_mce_s_s {
+		u64 op                               : 2;
+		u64 reserved_2                       : 1;
+		u64 eol                              : 1;
+		u64 index                            : 20;
+		u64 reserved_24_31                   : 8;
+		u64 pf_func                          : 16;
+		u64 next                             : 16;
+	} s;
+	/* struct cavm_nix_rx_mce_s_s cn; */
+};
+
+/**
+ * Structure nix_rx_parse_s
+ *
+ * NIX Receive Parse Structure
+ * This structure contains the receive packet parse result. It immediately follows
+ * NIX_CQE_HDR_S in a receive CQE, or NIX_WQE_HDR_S in a receive WQE.
+ * Stored in memory as little-endian unless NIX_AF_LF()_CFG[BE] is set.
+ *
+ * Header layers are always 2-byte aligned, so all header pointers in this
+ * structure ([EOH_PTR], [LAPTR] through [LHPTR], [VTAG*_PTR]) are even.
+ */
+union cavm_nix_rx_parse_s {
+	u64 u[7];
+	struct cavm_nix_rx_parse_s_s {
+		u64 chan                             : 12;
+		u64 desc_sizem1                      : 5;
+		u64 imm_copy                         : 1;
+		u64 express                          : 1;
+		u64 wqwd                             : 1;
+		u64 errlev                           : 4;
+		u64 errcode                          : 8;
+		u64 latype                           : 4;
+		u64 lbtype                           : 4;
+		u64 lctype                           : 4;
+		u64 ldtype                           : 4;
+		u64 letype                           : 4;
+		u64 lftype                           : 4;
+		u64 lgtype                           : 4;
+		u64 lhtype                           : 4;
+		u64 pkt_lenm1                        : 16;
+		u64 l2m                              : 1;
+		u64 l2b                              : 1;
+		u64 l3m                              : 1;
+		u64 l3b                              : 1;
+		u64 vtag0_valid                      : 1;
+		u64 vtag0_gone                       : 1;
+		u64 vtag1_valid                      : 1;
+		u64 vtag1_gone                       : 1;
+		u64 pkind                            : 6;
+		u64 reserved_94_95                   : 2;
+		u64 vtag0_tci                        : 16;
+		u64 vtag1_tci                        : 16;
+		u64 laflags                          : 8;
+		u64 lbflags                          : 8;
+		u64 lcflags                          : 8;
+		u64 ldflags                          : 8;
+		u64 leflags                          : 8;
+		u64 lfflags                          : 8;
+		u64 lgflags                          : 8;
+		u64 lhflags                          : 8;
+		u64 eoh_ptr                          : 8;
+		u64 wqe_aura                         : 20;
+		u64 pb_aura                          : 20;
+		u64 match_id                         : 16;
+		u64 laptr                            : 8;
+		u64 lbptr                            : 8;
+		u64 lcptr                            : 8;
+		u64 ldptr                            : 8;
+		u64 leptr                            : 8;
+		u64 lfptr                            : 8;
+		u64 lgptr                            : 8;
+		u64 lhptr                            : 8;
+		u64 vtag0_ptr                        : 8;
+		u64 vtag1_ptr                        : 8;
+		u64 flow_key_alg                     : 5;
+		u64 reserved_341_383                 : 43;
+		u64 reserved_384_447                 : 64;
+	} s;
+	/* struct cavm_nix_rx_parse_s_s cn; */
+};
+
+/**
+ * Structure nix_rx_sg_s
+ *
+ * NIX Receive Scatter/Gather Subdescriptor Structure
+ * The receive scatter/gather subdescriptor specifies one to three segments of packet data bytes.
+ * There may be multiple NIX_RX_SG_Ss in each NIX receive descriptor.
+ *
+ * NIX_RX_SG_S is immediately followed by one NIX_IOVA_S word when [SEGS] = 1,
+ * three NIX_IOVA_S words when [SEGS] \>= 2. Each NIX_IOVA_S word specifies the
+ * LF IOVA of first packet data byte in the corresponding segment; first NIX_IOVA_S
+ * word for segment 1, second word for segment 2, third word for segment 3. Note
+ * the third word is present when [SEGS] \>= 2 but only valid when [SEGS] = 3.
+ */
+union cavm_nix_rx_sg_s {
+	u64 u;
+	struct cavm_nix_rx_sg_s_s {
+		u64 seg1_size                        : 16;
+		u64 seg2_size                        : 16;
+		u64 seg3_size                        : 16;
+		u64 segs                             : 2;
+		u64 reserved_50_59                   : 10;
+		u64 subdc                            : 4;
+	} s;
+	/* struct cavm_nix_rx_sg_s_s cn; */
+};
+
+/**
+ * Structure nix_rx_vtag_action_s
+ *
+ * NIX Receive Vtag Action Structure
+ * This structure defines the format of NPC_RESULT_S[VTAG_ACTION] for a receive packet.
+ * It specifies up to two Vtags (e.g. C-VLAN/S-VLAN tags, 802.1BR E-TAG) for optional
+ * capture and/or stripping.
+ */
+union cavm_nix_rx_vtag_action_s {
+	u64 u;
+	struct cavm_nix_rx_vtag_action_s_s {
+		u64 vtag0_relptr                     : 8;
+		u64 vtag0_lid                        : 3;
+		u64 reserved_11                      : 1;
+		u64 vtag0_type                       : 3;
+		u64 vtag0_valid                      : 1;
+		u64 reserved_16_31                   : 16;
+		u64 vtag1_relptr                     : 8;
+		u64 vtag1_lid                        : 3;
+		u64 reserved_43                      : 1;
+		u64 vtag1_type                       : 3;
+		u64 vtag1_valid                      : 1;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nix_rx_vtag_action_s_s cn; */
+};
+
+/**
+ * Structure nix_send_comp_s
+ *
+ * NIX Send Completion Structure
+ * This structure immediately follows NIX_CQE_HDR_S in a send completion CQE.
+ */
+union cavm_nix_send_comp_s {
+	u64 u;
+	struct cavm_nix_send_comp_s_s {
+		u64 status                           : 8;
+		u64 sqe_id                           : 16;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nix_send_comp_s_s cn; */
+};
+
+/**
+ * Structure nix_send_crc_s
+ *
+ * NIX Send CRC Subdescriptor Structure
+ * The send CRC subdescriptor specifies a CRC calculation be performed during
+ * transmission.
+ * Ignored when present in a send descriptor with NIX_SEND_EXT_S[LSO] set.
+ * There may be up to two NIX_SEND_CRC_Ss per send descriptor.
+ *
+ * NIX_SEND_CRC_S constraints:
+ * * When present, NIX_SEND_CRC_S subdescriptors must precede all NIX_SEND_SG_S,
+ * NIX_SEND_IMM_S and NIX_SEND_MEM_S subdescriptors in the send descriptor.
+ * * NIX_SEND_CRC_S subdescriptors must follow the same order as their checksum
+ * and insert regions in the packet, i.e. the checksum and insert regions of a
+ * NIX_SEND_CRC_S must come after the checksum and insert regions of a preceding
+ * NIX_SEND_CRC_S. There must be no overlap between any NIX_SEND_CRC_S checksum
+ * and insert regions.
+ * * If either NIX_SEND_HDR_S[OL4TYPE,IL4TYPE] = NIX_SENDL4TYPE_E::SCTP_CKSUM, the
+ * SCTP checksum region and NIX_SEND_CRC_S insert region must not overlap, and
+ * likewise the NIX_SEND_CRC_S checksum region and SCTP insert region must not
+ * overlap.
+ * * If either NIX_SEND_HDR_S[OL3TYPE,IL3TYPE] = NIX_SENDL3TYPE_E::IP4_CKSUM, the
+ * IPv4 header checksum region and NIX_SEND_CRC_S insert region must not overlap.
+ * * Any checksums inserted by NIX_SEND_HDR_S[OL3TYPE,OL4TYPE,IL3TYPE,IL4TYPE]
+ * must be outside of the NIX_SEND_CRC_S checksum and insert regions.
+ *
+ * Hardware adjusts [START], [SIZE] and [INSERT] as needed to account for any VLAN
+ * inserted by NIX_SEND_EXT_S[VLAN*] or Vtag inserted by NIX_TX_VTAG_ACTION_S.
+ */
+union cavm_nix_send_crc_s {
+	u64 u[2];
+	struct cavm_nix_send_crc_s_s {
+		u64 size                             : 16;
+		u64 start                            : 16;
+		u64 insert                           : 16;
+		u64 reserved_48_57                   : 10;
+		u64 alg                              : 2;
+		u64 subdc                            : 4;
+		u64 iv                               : 32;
+		u64 reserved_96_127                  : 32;
+	} s;
+	/* struct cavm_nix_send_crc_s_s cn; */
+};
+
+/**
+ * Structure nix_send_ext_s
+ *
+ * NIX Send Extended Header Subdescriptor Structure
+ * The send extended header specifies LSO, VLAN insertion, timestamp and/or
+ * scheduling services on the packet. If present, it must immediately follow
+ * NIX_SEND_HDR_S. All fields are assumed to be zero when this subdescriptor is not
+ * present.
+ */
+union cavm_nix_send_ext_s {
+	u64 u[2];
+	struct cavm_nix_send_ext_s_s {
+		u64 lso_mps                          : 14;
+		u64 lso                              : 1;
+		u64 tstmp                            : 1;
+		u64 lso_sb                           : 8;
+		u64 lso_format                       : 5;
+		u64 reserved_29_31                   : 3;
+		u64 shp_chg                          : 9;
+		u64 shp_dis                          : 1;
+		u64 shp_ra                           : 2;
+		u64 markptr                          : 8;
+		u64 markform                         : 7;
+		u64 mark_en                          : 1;
+		u64 subdc                            : 4;
+		u64 vlan0_ins_ptr                    : 8;
+		u64 vlan0_ins_tci                    : 16;
+		u64 vlan1_ins_ptr                    : 8;
+		u64 vlan1_ins_tci                    : 16;
+		u64 vlan0_ins_ena                    : 1;
+		u64 vlan1_ins_ena                    : 1;
+		u64 reserved_114_127                 : 14;
+	} s;
+	/* struct cavm_nix_send_ext_s_s cn; */
+};
+
+/**
+ * Structure nix_send_hdr_s
+ *
+ * NIX Send Header Subdescriptor Structure
+ * The send header is the first subdescriptor of every send descriptor.
+ */
+union cavm_nix_send_hdr_s {
+	u64 u[2];
+	struct cavm_nix_send_hdr_s_s {
+		u64 total                            : 18;
+		u64 reserved_18                      : 1;
+		u64 df                               : 1;
+		u64 aura                             : 20;
+		u64 sizem1                           : 3;
+		u64 pnc                              : 1;
+		u64 sq                               : 20;
+		u64 ol3ptr                           : 8;
+		u64 ol4ptr                           : 8;
+		u64 il3ptr                           : 8;
+		u64 il4ptr                           : 8;
+		u64 ol3type                          : 4;
+		u64 ol4type                          : 4;
+		u64 il3type                          : 4;
+		u64 il4type                          : 4;
+		u64 sqe_id                           : 16;
+	} s;
+	/* struct cavm_nix_send_hdr_s_s cn; */
+};
+
+/**
+ * Structure nix_send_imm_s
+ *
+ * NIX Send Immediate Subdescriptor Structure
+ * The send immediate subdescriptor requests that bytes immediately following this
+ * NIX_SEND_IMM_S (after skipping [APAD] bytes) are to be included in the packet data.
+ * The next subdescriptor following this NIX_SEND_IMM_S (when one exists) will
+ * follow the immediate bytes, after rounding up the address to a multiple of 16 bytes.
+ *
+ * There may be multiple NIX_SEND_IMM_S in one NIX send descriptor. A
+ * NIX_SEND_IMM_S is ignored in a NIX send descriptor if the sum of all prior
+ * NIX_SEND_SG_S[SEG*_SIZE]s and NIX_SEND_IMM_S[SIZE]s meets or exceeds
+ * NIX_SEND_HDR_S[TOTAL].
+ *
+ * When NIX_SEND_EXT_S[LSO] is set in the descriptor, all NIX_SEND_IMM_S
+ * bytes must be included in the first NIX_SEND_EXT_S[LSO_SB] bytes of the
+ * source packet.
+ */
+union cavm_nix_send_imm_s {
+	u64 u;
+	struct cavm_nix_send_imm_s_s {
+		u64 size                             : 16;
+		u64 apad                             : 3;
+		u64 reserved_19_59                   : 41;
+		u64 subdc                            : 4;
+	} s;
+	/* struct cavm_nix_send_imm_s_s cn; */
+};
+
+/**
+ * Structure nix_send_jump_s
+ *
+ * NIX Send Jump Subdescriptor Structure
+ * The send jump subdescriptor selects a new address for fetching the remaining
+ * subdescriptors of a send descriptor. This allows software to create a send
+ * descriptor longer than SQE size selected by NIX_SQ_CTX_S[MAX_SQE_SIZE].
+ *
+ * There can be only one NIX_SEND_JUMP_S subdescriptor in a send descriptor. If
+ * present, it must immediately follow NIX_SEND_HDR_S if NIX_SEND_EXT_S is not
+ * present, else it must immediately follow NIX_SEND_EXT_S. In either case, it
+ * must terminate the SQE enqueued by software.
+ */
+union cavm_nix_send_jump_s {
+	u64 u[2];
+	struct cavm_nix_send_jump_s_s {
+		u64 sizem1                           : 7;
+		u64 reserved_7_13                    : 7;
+		u64 ld_type                          : 2;
+		u64 aura                             : 20;
+		u64 reserved_36_58                   : 23;
+		u64 f                                : 1;
+		u64 subdc                            : 4;
+		u64 addr                             : 64;
+	} s;
+	/* struct cavm_nix_send_jump_s_s cn; */
+};
+
+/**
+ * Structure nix_send_mem_s
+ *
+ * NIX Send Memory Subdescriptor Structure
+ * The send memory subdescriptor atomically sets, increments or decrements a memory location.
+ *
+ * NIX_SEND_MEM_S subdescriptors must follow all NIX_SEND_SG_S and NIX_SEND_IMM_S
+ * subdescriptors in the NIX send descriptor. NIX will not initiate the memory
+ * update for this subdescriptor until after it has completed all LLC/DRAM fetches
+ * that service all prior NIX_SEND_SG_S subdescriptors. The memory update is
+ * executed once, even if the packet is replicated due to NIX_TX_ACTION_S[OP] =
+ * NIX_TX_ACTIONOP_E::MCAST.
+ *
+ * Performance is best if a memory decrement by one is used rather than any other memory
+ * set/increment/decrement. (Less internal bus bandwidth is used with memory decrements by one.)
+ *
+ * When NIX_SEND_EXT_S[LSO] is set in the descriptor, NIX executes the
+ * memory update only while processing the last LSO segment, after
+ * processing prior segments.
+ */
+union cavm_nix_send_mem_s {
+	u64 u[2];
+	struct cavm_nix_send_mem_s_s {
+		u64 offset                           : 16;
+		u64 reserved_16_52                   : 37;
+		u64 wmem                             : 1;
+		u64 dsz                              : 2;
+		u64 alg                              : 4;
+		u64 subdc                            : 4;
+		u64 addr                             : 64;
+	} s;
+	/* struct cavm_nix_send_mem_s_s cn; */
+};
+
+/**
+ * Structure nix_send_sg_s
+ *
+ * NIX Send Scatter/Gather Subdescriptor Structure
+ * The send scatter/gather subdescriptor requests one to three segments of packet
+ * data bytes to be transmitted. There may be multiple NIX_SEND_SG_Ss in each NIX send descriptor.
+ *
+ * NIX_SEND_SG_S is immediately followed by one NIX_IOVA_S word when [SEGS] = 1,
+ * three NIX_IOVA_S words when [SEGS] \>= 2. Each NIX_IOVA_S word specifies the
+ * LF IOVA of first packet data byte in the corresponding segment; first NIX_IOVA_S
+ * word for segment 1, second word for segment 2, third word for segment 3. Note
+ * the third word is present when [SEGS] \>= 2 but only valid when [SEGS] = 3.
+ *
+ * If the sum of all prior NIX_SEND_SG_S[SEG*_SIZE]s and NIX_SEND_IMM_S[SIZE]s
+ * meets or exceeds NIX_SEND_HDR_S[TOTAL], this subdescriptor will not contribute
+ * any packet data but may free buffers to NPA (see [I1]).
+ */
+union cavm_nix_send_sg_s {
+	u64 u;
+	struct cavm_nix_send_sg_s_s {
+		u64 seg1_size                        : 16;
+		u64 seg2_size                        : 16;
+		u64 seg3_size                        : 16;
+		u64 segs                             : 2;
+		u64 reserved_50_54                   : 5;
+		u64 i1                               : 1;
+		u64 i2                               : 1;
+		u64 i3                               : 1;
+		u64 ld_type                          : 2;
+		u64 subdc                            : 4;
+	} s;
+	/* struct cavm_nix_send_sg_s_s cn; */
+};
+
+/**
+ * Structure nix_send_work_s
+ *
+ * NIX Send Work Subdescriptor Structure
+ * This subdescriptor adds work to the SSO. At most one NIX_SEND_WORK_S subdescriptor
+ * can exist in the NIX send descriptor. If a NIX_SEND_WORK_S exists in the
+ * descriptor, it must be the last subdescriptor. NIX will not initiate the work add
+ * for this subdescriptor until after (1) it has completed all LLC/DRAM fetches that
+ * service all prior NIX_SEND_SG_S subdescriptors, (2) it has
+ * fetched all subdescriptors in the descriptor, and (3) all NIX_SEND_MEM_S[WMEM]=1
+ * LLC/DRAM updates have completed.
+ *
+ * Provided the path of descriptors from the SQ through NIX to an output FIFO is
+ * unmodified between the descriptors (as should normally be the case, but it is
+ * possible for software to change the path), NIX also (1) will submit
+ * the SSO add works from all descriptors in the SQ in order, and
+ * (2) will not submit an SSO work add until after all prior descriptors
+ * in the SQ have completed their NIX_SEND_SG_S
+ * processing, and (3) will not submit an SSO work add until after
+ * it has fetched all subdescriptors from prior descriptors in the SQ.
+ *
+ * When NIX_SEND_EXT_S[LSO] is set in the descriptor, NIX executes the
+ * NIX_SEND_WORK_S work add only while processing the last LSO segment, after
+ * processing prior segments.
+ *
+ * Hardware ignores NIX_SEND_WORK_S when NIX_SQ_CTX_S[SSO_ENA] is clear.
+ */
+union cavm_nix_send_work_s {
+	u64 u[2];
+	struct cavm_nix_send_work_s_s {
+		u64 tag                              : 32;
+		u64 tt                               : 2;
+		u64 grp                              : 10;
+		u64 reserved_44_59                   : 16;
+		u64 subdc                            : 4;
+		u64 addr                             : 64;
+	} s;
+	/* struct cavm_nix_send_work_s_s cn; */
+};
+
+/**
+ * Structure nix_sq_ctx_hw_s
+ *
+ * NIX SQ Context Hardware Structure
+ * This structure contains context state maintained by hardware for each SQ in
+ * NDC/LLC/DRAM.
+ * Software uses the equivalent NIX_SQ_CTX_S structure format to read and write an
+ * SQ context with the NIX admin queue.
+ * Always stored in byte invariant little-endian format (LE8).
+ */
+union cavm_nix_sq_ctx_hw_s {
+	u64 u[16];
+	struct cavm_nix_sq_ctx_hw_s_s {
+		u64 gbl_ena                          : 1;
+		u64 gbl_substream                    : 20;
+		u64 gbl_max_sqe_size                 : 2;
+		u64 gbl_sqe_way_mask                 : 16;
+		u64 gbl_sqb_aura                     : 20;
+		u64 gbl_rsvd1                        : 5;
+		u64 gbl_cq_id                        : 20;
+		u64 gbl_cq_ena                       : 1;
+		u64 qint_idx                         : 7;
+		u64 sq_int                           : 8;
+		u64 sq_int_ena                       : 8;
+		u64 xoff                             : 1;
+		u64 send_lso_segnum                  : 8;
+		u64 gbl_rsvd                         : 11;
+		u64 sqb_enqueue_count                : 16;
+		u64 sqe_stype                        : 2;
+		u64 tail_offset                      : 6;
+		u64 lmt_dis                          : 1;
+		u64 smq_rr_quantum                   : 24;
+		u64 dnq_rsvd1                        : 15;
+		u64 tail_sqb                         : 64;
+		u64 next_sqb                         : 64;
+		u64 mnq_dis                          : 1;
+		u64 smq                              : 10;
+		u64 smq_pend                         : 1;
+		u64 smq_next_sq                      : 20;
+		u64 smq_next_sq_vld                  : 1;
+		u64 scm1_rsvd2                       : 31;
+		u64 smenq_sqb                        : 64;
+		u64 smenq_offset                     : 6;
+		u64 cq_limit                         : 8;
+		u64 smq_rr_count                     : 25;
+		u64 scm_lso_rem                      : 18;
+		u64 scm_dq_rsvd0                     : 7;
+		u64 smq_lso_segnum                   : 8;
+		u64 vfi_lso_total                    : 18;
+		u64 vfi_lso_sizem1                   : 3;
+		u64 vfi_lso_sb                       : 8;
+		u64 vfi_lso_mps                      : 14;
+		u64 vfi_lso_vlan0_ins_ena            : 1;
+		u64 vfi_lso_vlan1_ins_ena            : 1;
+		u64 vfi_lso_vld                      : 1;
+		u64 smenq_next_sqb_vld               : 1;
+		u64 scm_dq_rsvd1                     : 9;
+		u64 smenq_next_sqb                   : 64;
+		u64 head_sqb                         : 64;
+		u64 head_offset                      : 6;
+		u64 sqb_dequeue_count                : 16;
+		u64 default_chan                     : 12;
+		u64 sdp_mcast                        : 1;
+		u64 sso_ena                          : 1;
+		u64 dse_rsvd1                        : 28;
+		u64 seb_rsvd1                        : 64;
+		u64 drop_pkts                        : 48;
+		u64 drop_octs_lsw                    : 16;
+		u64 drop_octs_msw                    : 32;
+		u64 pkts_lsw                         : 32;
+		u64 pkts_msw                         : 16;
+		u64 octs                             : 48;
+	} s;
+	/* struct cavm_nix_sq_ctx_hw_s_s cn; */
+};
+
+/**
+ * Structure nix_sq_ctx_s
+ *
+ * NIX Send Queue Context Structure
+ * This structure specifies the format used by software with the NIX admin queue
+ * to read and write a send queue's NIX_SQ_CTX_HW_S structure maintained by
+ * hardware in NDC/LLC/DRAM.
+ *
+ * The SQ statistics ([OCTS], [PKTS], [DROP_OCTS], [DROP_PKTS]) do not account for
+ * packet replication due to NIX_TX_ACTION_S[OP] = NIX_TX_ACTIONOP_E::MCAST.
+ */
+union cavm_nix_sq_ctx_s {
+	u64 u[16];
+	struct cavm_nix_sq_ctx_s_s {
+		u64 ena                              : 1;
+		u64 cq_ena                           : 1;
+		u64 max_sqe_size                     : 2;
+		u64 substream                        : 20;
+		u64 sdp_mcast                        : 1;
+		u64 lmt_dis                          : 1;
+		u64 mnq_dis                          : 1;
+		u64 reserved_27_35                   : 9;
+		u64 cq                               : 20;
+		u64 cq_limit                         : 8;
+		u64 smq                              : 10;
+		u64 xoff                             : 1;
+		u64 sso_ena                          : 1;
+		u64 smq_rr_quantum                   : 24;
+		u64 default_chan                     : 12;
+		u64 sqb_count                        : 16;
+		u64 sqe_way_mask                     : 16;
+		u64 reserved_144_171                 : 28;
+		u64 sq_int                           : 8;
+		u64 sq_int_ena                       : 8;
+		u64 sqe_stype                        : 2;
+		u64 smq_pend                         : 1;
+		u64 reserved_191                     : 1;
+		u64 reserved_192_223                 : 32;
+		u64 send_lso_segnum                  : 8;
+		u64 smq_lso_segnum                   : 8;
+		u64 qint_idx                         : 7;
+		u64 reserved_247_255                 : 9;
+		u64 next_sqb                         : 64;
+		u64 tail_sqb                         : 64;
+		u64 smenq_sqb                        : 64;
+		u64 head_sqb                         : 64;
+		u64 tail_offset                      : 6;
+		u64 smenq_offset                     : 6;
+		u64 head_offset                      : 6;
+		u64 reserved_530_539                 : 10;
+		u64 sqb_aura                         : 20;
+		u64 reserved_560_575                 : 16;
+		u64 smq_rr_count                     : 25;
+		u64 smenq_next_sqb_vld               : 1;
+		u64 smq_next_sq                      : 20;
+		u64 reserved_622_639                 : 18;
+		u64 octs                             : 48;
+		u64 reserved_688_703                 : 16;
+		u64 pkts                             : 48;
+		u64 reserved_752_767                 : 16;
+		u64 reserved_768_831                 : 64;
+		u64 smenq_next_sqb                   : 64;
+		u64 drop_octs                        : 48;
+		u64 reserved_944_959                 : 16;
+		u64 drop_pkts                        : 48;
+		u64 reserved_1008_1023               : 16;
+	} s;
+	struct cavm_nix_sq_ctx_s_cn {
+		u64 ena                              : 1;
+		u64 cq_ena                           : 1;
+		u64 max_sqe_size                     : 2;
+		u64 substream                        : 20;
+		u64 sdp_mcast                        : 1;
+		u64 lmt_dis                          : 1;
+		u64 mnq_dis                          : 1;
+		u64 reserved_27                      : 1;
+		u64 reserved_28_35                   : 8;
+		u64 cq                               : 20;
+		u64 cq_limit                         : 8;
+		u64 smq                              : 10;
+		u64 xoff                             : 1;
+		u64 sso_ena                          : 1;
+		u64 smq_rr_quantum                   : 24;
+		u64 default_chan                     : 12;
+		u64 sqb_count                        : 16;
+		u64 sqe_way_mask                     : 16;
+		u64 reserved_144_171                 : 28;
+		u64 sq_int                           : 8;
+		u64 sq_int_ena                       : 8;
+		u64 sqe_stype                        : 2;
+		u64 smq_pend                         : 1;
+		u64 reserved_191                     : 1;
+		u64 reserved_192_223                 : 32;
+		u64 send_lso_segnum                  : 8;
+		u64 smq_lso_segnum                   : 8;
+		u64 qint_idx                         : 7;
+		u64 reserved_247_255                 : 9;
+		u64 next_sqb                         : 64;
+		u64 tail_sqb                         : 64;
+		u64 smenq_sqb                        : 64;
+		u64 head_sqb                         : 64;
+		u64 tail_offset                      : 6;
+		u64 smenq_offset                     : 6;
+		u64 head_offset                      : 6;
+		u64 reserved_530_539                 : 10;
+		u64 sqb_aura                         : 20;
+		u64 reserved_560_575                 : 16;
+		u64 smq_rr_count                     : 25;
+		u64 smenq_next_sqb_vld               : 1;
+		u64 smq_next_sq                      : 20;
+		u64 reserved_622_639                 : 18;
+		u64 octs                             : 48;
+		u64 reserved_688_703                 : 16;
+		u64 pkts                             : 48;
+		u64 reserved_752_767                 : 16;
+		u64 reserved_768_831                 : 64;
+		u64 smenq_next_sqb                   : 64;
+		u64 drop_octs                        : 48;
+		u64 reserved_944_959                 : 16;
+		u64 drop_pkts                        : 48;
+		u64 reserved_1008_1023               : 16;
+	} cn;
+};
+
+/**
+ * Structure nix_tx_action_s
+ *
+ * NIX Transmit Action Structure
+ * This structure defines the format of NPC_RESULT_S[ACTION] for a transmit packet.
+ */
+union cavm_nix_tx_action_s {
+	u64 u;
+	struct cavm_nix_tx_action_s_s {
+		u64 op                               : 4;
+		u64 reserved_4_11                    : 8;
+		u64 index                            : 20;
+		u64 match_id                         : 16;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nix_tx_action_s_s cn; */
+};
+
+/**
+ * Structure nix_tx_vtag_action_s
+ *
+ * NIX Transmit Vtag Action Structure
+ * This structure defines the format of NPC_RESULT_S[VTAG_ACTION] for a transmit
+ * packet. It specifies the optional insertion or replacement of up to two Vtags
+ * (e.g. C-VLAN/S-VLAN tags, 802.1BR E-TAG).
+ *
+ * If two Vtags are specified:
+ * * The Vtag 0 byte offset from packet start (see [VTAG0_RELPTR]) must be less
+ * than or equal to the Vtag 1 byte offset.
+ * * Hardware executes the Vtag 0 action first, Vtag 1 action second.
+ * * If Vtag 0 is inserted, hardware adjusts the Vtag 1 byte offset accordingly.
+ * Thus, if the two offsets are equal in the structure, hardware inserts Vtag 1
+ * immediately after Vtag 0 in the packet.
+ *
+ * A Vtag must not be inserted or replaced within an outer or inner L3/L4 header,
+ * but may be inserted or replaced within an outer L4 payload.
+ */
+union cavm_nix_tx_vtag_action_s {
+	u64 u;
+	struct cavm_nix_tx_vtag_action_s_s {
+		u64 vtag0_relptr                     : 8;
+		u64 vtag0_lid                        : 3;
+		u64 reserved_11                      : 1;
+		u64 vtag0_op                         : 2;
+		u64 reserved_14_15                   : 2;
+		u64 vtag0_def                        : 10;
+		u64 reserved_26_31                   : 6;
+		u64 vtag1_relptr                     : 8;
+		u64 vtag1_lid                        : 3;
+		u64 reserved_43                      : 1;
+		u64 vtag1_op                         : 2;
+		u64 reserved_46_47                   : 2;
+		u64 vtag1_def                        : 10;
+		u64 reserved_58_63                   : 6;
+	} s;
+	/* struct cavm_nix_tx_vtag_action_s_s cn; */
+};
+
+/**
+ * Structure nix_wqe_hdr_s
+ *
+ * NIX Work Queue Entry Header Structure
+ * This 64-bit structure defines the first word of every receive WQE generated by
+ * NIX. It is immediately followed by NIX_RX_PARSE_S.
+ * Stored in memory as little-endian unless NIX_AF_LF()_CFG[BE] is set.
+ */
+union cavm_nix_wqe_hdr_s {
+	u64 u;
+	struct cavm_nix_wqe_hdr_s_s {
+		u64 tag                              : 32;
+		u64 tt                               : 2;
+		u64 grp                              : 10;
+		u64 node                             : 2;
+		u64 q                                : 14;
+		u64 wqe_type                         : 4;
+	} s;
+	/* struct cavm_nix_wqe_hdr_s_s cn; */
+};
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_base
+ *
+ * NIX AF Admin Queue Base Address Register
+ */
+union cavm_nixx_af_aq_base {
+	u64 u;
+	struct cavm_nixx_af_aq_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 base_addr                        : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_aq_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_BASE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_BASE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x410;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_cfg
+ *
+ * NIX AF Admin Queue Configuration Register
+ */
+union cavm_nixx_af_aq_cfg {
+	u64 u;
+	struct cavm_nixx_af_aq_cfg_s {
+		u64 qsize                            : 4;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_aq_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x400;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done
+ *
+ * NIX AF Admin Queue Done Count Register
+ */
+union cavm_nixx_af_aq_done {
+	u64 u;
+	struct cavm_nixx_af_aq_done_s {
+		u64 done                             : 20;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_nixx_af_aq_done_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x450;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_ack
+ *
+ * NIX AF Admin Queue Done Count Ack Register
+ * This register is written by software to acknowledge interrupts.
+ */
+union cavm_nixx_af_aq_done_ack {
+	u64 u;
+	struct cavm_nixx_af_aq_done_ack_s {
+		u64 done_ack                         : 20;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_nixx_af_aq_done_ack_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_ACK
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_ACK
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x460;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_ena_w1c
+ *
+ * NIX AF Admin Queue Done Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_af_aq_done_ena_w1c {
+	u64 u;
+	struct cavm_nixx_af_aq_done_ena_w1c_s {
+		u64 done                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_aq_done_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x498;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_ena_w1s
+ *
+ * NIX AF Admin Queue Done Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_af_aq_done_ena_w1s {
+	u64 u;
+	struct cavm_nixx_af_aq_done_ena_w1s_s {
+		u64 done                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_aq_done_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x490;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_int
+ *
+ * INTERNAL: NIX AF Admin Queue Done Interrupt Register
+ */
+union cavm_nixx_af_aq_done_int {
+	u64 u;
+	struct cavm_nixx_af_aq_done_int_s {
+		u64 done                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_aq_done_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x480;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_int_w1s
+ *
+ * INTERNAL: NIX AF Admin Queue Done Interrupt Set Register
+ */
+union cavm_nixx_af_aq_done_int_w1s {
+	u64 u;
+	struct cavm_nixx_af_aq_done_int_w1s_s {
+		u64 done                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_aq_done_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x488;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_timer
+ *
+ * NIX AF Admin Queue Done Interrupt Timer Register
+ */
+union cavm_nixx_af_aq_done_timer {
+	u64 u;
+	struct cavm_nixx_af_aq_done_timer_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_aq_done_timer_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_TIMER
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_TIMER
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x470;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_done_wait
+ *
+ * NIX AF Admin Queue Done Interrupt Coalescing Wait Register
+ * Specifies the queue interrupt coalescing settings.
+ */
+union cavm_nixx_af_aq_done_wait {
+	u64 u;
+	struct cavm_nixx_af_aq_done_wait_s {
+		u64 num_wait                         : 20;
+		u64 reserved_20_31                   : 12;
+		u64 time_wait                        : 16;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_aq_done_wait_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DONE_WAIT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DONE_WAIT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x440;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_door
+ *
+ * NIX AF Admin Queue Doorbell Register
+ * Software writes to this register to enqueue entries to AQ.
+ */
+union cavm_nixx_af_aq_door {
+	u64 u;
+	struct cavm_nixx_af_aq_door_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_aq_door_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_DOOR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_DOOR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x430;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_aq_status
+ *
+ * NIX AF Admin Queue Status Register
+ */
+union cavm_nixx_af_aq_status {
+	u64 u;
+	struct cavm_nixx_af_aq_status_s {
+		u64 reserved_0_3                     : 4;
+		u64 head_ptr                         : 20;
+		u64 reserved_24_35                   : 12;
+		u64 tail_ptr                         : 20;
+		u64 reserved_56_61                   : 6;
+		u64 aq_busy                          : 1;
+		u64 aq_err                           : 1;
+	} s;
+	struct cavm_nixx_af_aq_status_cn {
+		u64 reserved_0_3                     : 4;
+		u64 head_ptr                         : 20;
+		u64 reserved_24_31                   : 8;
+		u64 reserved_32_35                   : 4;
+		u64 tail_ptr                         : 20;
+		u64 reserved_56_61                   : 6;
+		u64 aq_busy                          : 1;
+		u64 aq_err                           : 1;
+	} cn;
+};
+
+static inline u64 CAVM_NIXX_AF_AQ_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AQ_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x420;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_avg_delay
+ *
+ * NIX AF Queue Average Delay Register
+ */
+union cavm_nixx_af_avg_delay {
+	u64 u;
+	struct cavm_nixx_af_avg_delay_s {
+		u64 avg_dly                          : 19;
+		u64 reserved_19_23                   : 5;
+		u64 avg_timer                        : 16;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_avg_delay_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_AVG_DELAY
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_AVG_DELAY
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xe0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_bar2_alias#
+ *
+ * INTERNAL: NIX Admin Function  BAR2 Alias Registers
+ *
+ * These registers alias to the NIX BAR2 registers for the PF and function
+ * selected by NIX_AF_BAR2_SEL[PF_FUNC].
+ *
+ * Internal:
+ * Not implemented. Placeholder for bug33464.
+ */
+union cavm_nixx_af_bar2_aliasx {
+	u64 u;
+	struct cavm_nixx_af_bar2_aliasx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_nixx_af_bar2_aliasx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_BAR2_ALIASX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_BAR2_ALIASX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 131071))
+		return 0x9100000 + 8 * ((a) & 0x1ffff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_bar2_sel
+ *
+ * INTERNAL: NIX Admin Function BAR2 Select Register
+ *
+ * This register configures BAR2 accesses from the NIX_AF_BAR2_ALIAS() registers in BAR0.
+ * Internal:
+ * Not implemented. Placeholder for bug33464.
+ */
+union cavm_nixx_af_bar2_sel {
+	u64 u;
+	struct cavm_nixx_af_bar2_sel_s {
+		u64 alias_pf_func                    : 16;
+		u64 alias_ena                        : 1;
+		u64 reserved_17_63                   : 47;
+	} s;
+	/* struct cavm_nixx_af_bar2_sel_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_BAR2_SEL
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_BAR2_SEL
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x9000000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_blk_rst
+ *
+ * NIX AF Block Reset Register
+ */
+union cavm_nixx_af_blk_rst {
+	u64 u;
+	struct cavm_nixx_af_blk_rst_s {
+		u64 rst                              : 1;
+		u64 reserved_1_62                    : 62;
+		u64 busy                             : 1;
+	} s;
+	/* struct cavm_nixx_af_blk_rst_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_BLK_RST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_BLK_RST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xb0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_cfg
+ *
+ * NIX AF General Configuration Register
+ */
+union cavm_nixx_af_cfg {
+	u64 u;
+	struct cavm_nixx_af_cfg_s {
+		u64 force_cond_clk_en                : 1;
+		u64 af_be                            : 1;
+		u64 calibrate_x2p                    : 1;
+		u64 force_intf_clk_en                : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_cint_delay
+ *
+ * NIX AF Completion Interrupt Delay Register
+ */
+union cavm_nixx_af_cint_delay {
+	u64 u;
+	struct cavm_nixx_af_cint_delay_s {
+		u64 cint_dly                         : 10;
+		u64 reserved_10_15                   : 6;
+		u64 cint_timer                       : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_cint_delay_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CINT_DELAY
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CINT_DELAY
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xf0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_cint_timer#
+ *
+ * NIX AF Completion Interrupt Timer Registers
+ */
+union cavm_nixx_af_cint_timerx {
+	u64 u;
+	struct cavm_nixx_af_cint_timerx_s {
+		u64 expir_time                       : 16;
+		u64 cint                             : 7;
+		u64 reserved_23                      : 1;
+		u64 lf                               : 8;
+		u64 active                           : 1;
+		u64 reserved_33_63                   : 31;
+	} s;
+	/* struct cavm_nixx_af_cint_timerx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CINT_TIMERX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CINT_TIMERX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1a40 + 0x40000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_const
+ *
+ * NIX AF Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_const {
+	u64 u;
+	struct cavm_nixx_af_const_s {
+		u64 cgx_lmac_channels                : 8;
+		u64 cgx_lmacs                        : 4;
+		u64 num_cgx                          : 4;
+		u64 lbk_channels                     : 8;
+		u64 lbk_to_nix0                      : 6;
+		u64 lbk_to_nix1                      : 6;
+		u64 lbk_from_nix0                    : 6;
+		u64 lbk_from_nix1                    : 6;
+		u64 links                            : 8;
+		u64 intfs                            : 4;
+		u64 reserved_60_63                   : 4;
+	} s;
+	/* struct cavm_nixx_af_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x20;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_const1
+ *
+ * NIX AF Constants 1 Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_const1 {
+	u64 u;
+	struct cavm_nixx_af_const1_s {
+		u64 sdp_channels                     : 12;
+		u64 rx_bpids                         : 12;
+		u64 lf_tx_stats                      : 8;
+		u64 lf_rx_stats                      : 8;
+		u64 lso_format_fields                : 8;
+		u64 lso_formats                      : 8;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_nixx_af_const1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CONST1
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CONST1
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_const2
+ *
+ * NIX AF Constants 2 Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_const2 {
+	u64 u;
+	struct cavm_nixx_af_const2_s {
+		u64 lfs                              : 12;
+		u64 qints                            : 12;
+		u64 cints                            : 12;
+		u64 reserved_36_63                   : 28;
+	} s;
+	/* struct cavm_nixx_af_const2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CONST2
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CONST2
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x30;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_const3
+ *
+ * NIX AF Constants 2 Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_const3 {
+	u64 u;
+	struct cavm_nixx_af_const3_s {
+		u64 sq_ctx_log2bytes                 : 4;
+		u64 rq_ctx_log2bytes                 : 4;
+		u64 cq_ctx_log2bytes                 : 4;
+		u64 rsse_log2bytes                   : 4;
+		u64 mce_log2bytes                    : 4;
+		u64 qint_log2bytes                   : 4;
+		u64 cint_log2bytes                   : 4;
+		u64 dyno_log2bytes                   : 4;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_const3_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CONST3
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CONST3
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x38;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_cq_const
+ *
+ * NIX AF CQ Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_cq_const {
+	u64 u;
+	struct cavm_nixx_af_cq_const_s {
+		u64 queues_per_lf                    : 24;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_cq_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_CQ_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_CQ_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x48;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_err_int
+ *
+ * NIX Admin Function Error Interrupt Register
+ */
+union cavm_nixx_af_err_int {
+	u64 u;
+	struct cavm_nixx_af_err_int_s {
+		u64 rx_mcast_data_fault              : 1;
+		u64 rx_mirror_data_fault             : 1;
+		u64 rx_mcast_wqe_fault               : 1;
+		u64 rx_mirror_wqe_fault              : 1;
+		u64 rx_mce_fault                     : 1;
+		u64 rx_mce_list_err                  : 1;
+		u64 reserved_6_11                    : 6;
+		u64 aq_door_err                      : 1;
+		u64 aq_res_fault                     : 1;
+		u64 aq_inst_fault                    : 1;
+		u64 reserved_15_63                   : 49;
+	} s;
+	/* struct cavm_nixx_af_err_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_ERR_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_ERR_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x180;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_err_int_ena_w1c
+ *
+ * NIX Admin Function Error Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_af_err_int_ena_w1c {
+	u64 u;
+	struct cavm_nixx_af_err_int_ena_w1c_s {
+		u64 rx_mcast_data_fault              : 1;
+		u64 rx_mirror_data_fault             : 1;
+		u64 rx_mcast_wqe_fault               : 1;
+		u64 rx_mirror_wqe_fault              : 1;
+		u64 rx_mce_fault                     : 1;
+		u64 rx_mce_list_err                  : 1;
+		u64 reserved_6_11                    : 6;
+		u64 aq_door_err                      : 1;
+		u64 aq_res_fault                     : 1;
+		u64 aq_inst_fault                    : 1;
+		u64 reserved_15_63                   : 49;
+	} s;
+	/* struct cavm_nixx_af_err_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_ERR_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_ERR_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x198;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_err_int_ena_w1s
+ *
+ * NIX Admin Function Error Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_af_err_int_ena_w1s {
+	u64 u;
+	struct cavm_nixx_af_err_int_ena_w1s_s {
+		u64 rx_mcast_data_fault              : 1;
+		u64 rx_mirror_data_fault             : 1;
+		u64 rx_mcast_wqe_fault               : 1;
+		u64 rx_mirror_wqe_fault              : 1;
+		u64 rx_mce_fault                     : 1;
+		u64 rx_mce_list_err                  : 1;
+		u64 reserved_6_11                    : 6;
+		u64 aq_door_err                      : 1;
+		u64 aq_res_fault                     : 1;
+		u64 aq_inst_fault                    : 1;
+		u64 reserved_15_63                   : 49;
+	} s;
+	/* struct cavm_nixx_af_err_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_ERR_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_ERR_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x190;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_err_int_w1s
+ *
+ * NIX Admin Function Error Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_af_err_int_w1s {
+	u64 u;
+	struct cavm_nixx_af_err_int_w1s_s {
+		u64 rx_mcast_data_fault              : 1;
+		u64 rx_mirror_data_fault             : 1;
+		u64 rx_mcast_wqe_fault               : 1;
+		u64 rx_mirror_wqe_fault              : 1;
+		u64 rx_mce_fault                     : 1;
+		u64 rx_mce_list_err                  : 1;
+		u64 reserved_6_11                    : 6;
+		u64 aq_door_err                      : 1;
+		u64 aq_res_fault                     : 1;
+		u64 aq_inst_fault                    : 1;
+		u64 reserved_15_63                   : 49;
+	} s;
+	/* struct cavm_nixx_af_err_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_ERR_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_ERR_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x188;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_expr_tx_fifo_status
+ *
+ * NIX AF Express Transmit FIFO Status Register
+ * Status of FIFO which transmits express packets to CGX and LBK.
+ */
+union cavm_nixx_af_expr_tx_fifo_status {
+	u64 u;
+	struct cavm_nixx_af_expr_tx_fifo_status_s {
+		u64 count                            : 12;
+		u64 reserved_12_63                   : 52;
+	} s;
+	/* struct cavm_nixx_af_expr_tx_fifo_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_EXPR_TX_FIFO_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_EXPR_TX_FIFO_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x640;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_gen_int
+ *
+ * NIX AF General Interrupt Register
+ */
+union cavm_nixx_af_gen_int {
+	u64 u;
+	struct cavm_nixx_af_gen_int_s {
+		u64 rx_mcast_drop                    : 1;
+		u64 rx_mirror_drop                   : 1;
+		u64 reserved_2                       : 1;
+		u64 tl1_drain                        : 1;
+		u64 smq_flush_done                   : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_nixx_af_gen_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_GEN_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_GEN_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x160;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_gen_int_ena_w1c
+ *
+ * NIX AF General Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_af_gen_int_ena_w1c {
+	u64 u;
+	struct cavm_nixx_af_gen_int_ena_w1c_s {
+		u64 rx_mcast_drop                    : 1;
+		u64 rx_mirror_drop                   : 1;
+		u64 reserved_2                       : 1;
+		u64 tl1_drain                        : 1;
+		u64 smq_flush_done                   : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_nixx_af_gen_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_GEN_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_GEN_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x178;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_gen_int_ena_w1s
+ *
+ * NIX AF General Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_af_gen_int_ena_w1s {
+	u64 u;
+	struct cavm_nixx_af_gen_int_ena_w1s_s {
+		u64 rx_mcast_drop                    : 1;
+		u64 rx_mirror_drop                   : 1;
+		u64 reserved_2                       : 1;
+		u64 tl1_drain                        : 1;
+		u64 smq_flush_done                   : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_nixx_af_gen_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_GEN_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_GEN_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x170;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_gen_int_w1s
+ *
+ * NIX AF General Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_af_gen_int_w1s {
+	u64 u;
+	struct cavm_nixx_af_gen_int_w1s_s {
+		u64 rx_mcast_drop                    : 1;
+		u64 rx_mirror_drop                   : 1;
+		u64 reserved_2                       : 1;
+		u64 tl1_drain                        : 1;
+		u64 smq_flush_done                   : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_nixx_af_gen_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_GEN_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_GEN_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x168;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_cfg
+ *
+ * NIX AF Local Function Configuration Registers
+ */
+union cavm_nixx_af_lfx_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_cfg_s {
+		u64 npa_pf_func                      : 16;
+		u64 sso_pf_func                      : 16;
+		u64 be                               : 1;
+		u64 xqe_size                         : 2;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_lfx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4000 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_cints_base
+ *
+ * NIX AF Local Function Completion Interrupts Base Address Registers
+ * This register specifies the base AF IOVA of LF's completion interrupt
+ * context table in NDC/LLC/DRAM. The table consists of NIX_AF_CONST2[CINTS]
+ * contiguous NIX_CINT_HW_S structures.
+ */
+union cavm_nixx_af_lfx_cints_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_cints_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_cints_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_CINTS_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_CINTS_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4130 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_cints_cfg
+ *
+ * NIX AF Local Function Completion Interrupts Configuration Registers
+ * This register controls access to the LF's completion interrupt context table in
+ * NDC/LLC/DRAM. The table consists of NIX_AF_CONST2[CINTS] contiguous NIX_CINT_HW_S
+ * structures. The size of each structure is 1 \<\< NIX_AF_CONST3[CINT_LOG2BYTES].
+ */
+union cavm_nixx_af_lfx_cints_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_cints_cfg_s {
+		u64 reserved_0_19                    : 20;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_cints_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_CINTS_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_CINTS_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4120 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_cqs_base
+ *
+ * NIX AF Local Function Completion Queues Base Address Register
+ * This register specifies the base AF IOVA of the LF's CQ context table.
+ * The table consists of NIX_AF_LF()_CQS_CFG[MAX_QUEUESM1]+1 contiguous
+ * NIX_CQ_CTX_S structures.
+ */
+union cavm_nixx_af_lfx_cqs_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_cqs_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_cqs_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_CQS_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_CQS_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4070 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_cqs_cfg
+ *
+ * NIX AF Local Function Completion Queues Configuration Register
+ * This register configures completion queues in the LF.
+ */
+union cavm_nixx_af_lfx_cqs_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_cqs_cfg_s {
+		u64 max_queuesm1                     : 20;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_cqs_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_CQS_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_CQS_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4060 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_lock#
+ *
+ * NIX AF Local Function Lockdown Registers
+ * Internal:
+ * The NIX lockdown depth of 32 bytes is shallow compared to 96 bytes for NIC and meant for outer
+ * MAC and/or VLAN (optionally preceded by a small number of skip bytes). NPC's MCAM can be used
+ * for deeper protocol-aware lockdown.
+ */
+union cavm_nixx_af_lfx_lockx {
+	u64 u;
+	struct cavm_nixx_af_lfx_lockx_s {
+		u64 data                             : 32;
+		u64 bit_ena                          : 32;
+	} s;
+	/* struct cavm_nixx_af_lfx_lockx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_LOCKX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_LOCKX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 127) && (b <= 7)))
+		return 0x4300 + 0x20000 * ((a) & 0x7f) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_qints_base
+ *
+ * NIX AF Local Function Queue Interrupts Base Address Registers
+ * This register specifies the base AF IOVA of LF's queue interrupt context
+ * table in NDC/LLC/DRAM. The table consists of NIX_AF_CONST2[QINTS] contiguous
+ * NIX_QINT_HW_S structures.
+ */
+union cavm_nixx_af_lfx_qints_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_qints_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_qints_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_QINTS_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_QINTS_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4110 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_qints_cfg
+ *
+ * NIX AF Local Function Queue Interrupts Configuration Registers
+ * This register controls access to the LF's queue interrupt context table in
+ * NDC/LLC/DRAM. The table consists of NIX_AF_CONST2[QINTS] contiguous NIX_QINT_HW_S
+ * structures. The size of each structure is 1 \<\< NIX_AF_CONST3[QINT_LOG2BYTES].
+ */
+union cavm_nixx_af_lfx_qints_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_qints_cfg_s {
+		u64 reserved_0_19                    : 20;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_qints_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_QINTS_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_QINTS_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4100 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rqs_base
+ *
+ * NIX AF Local Function Receive Queues Base Address Register
+ * This register specifies the base AF IOVA of the LF's RQ context table.
+ * The table consists of NIX_AF_LF()_RQS_CFG[MAX_QUEUESM1]+1 contiguous
+ * NIX_RQ_CTX_S structures.
+ */
+union cavm_nixx_af_lfx_rqs_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_rqs_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_rqs_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RQS_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RQS_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4050 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rqs_cfg
+ *
+ * NIX AF Local Function Receive Queues Configuration Register
+ * This register configures receive queues in the LF.
+ */
+union cavm_nixx_af_lfx_rqs_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_rqs_cfg_s {
+		u64 max_queuesm1                     : 20;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_rqs_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RQS_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RQS_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4040 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rss_base
+ *
+ * NIX AF Local Function Receive Size Scaling Table Base Address Register
+ * This register specifies the base AF IOVA of the RSS table per LF. The
+ * table is present when NIX_AF_LF()_RSS_CFG[ENA] is set and consists of
+ * 2^(NIX_AF_LF()_RSS_CFG[SIZE]+8) contiguous NIX_RSSE_S structures, where the
+ * size of each structure is 1 \<\< NIX_AF_CONST3[RSSE_LOG2BYTES].
+ * See NIX_AF_LF()_RSS_GRP().
+ */
+union cavm_nixx_af_lfx_rss_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_rss_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_rss_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RSS_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RSS_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x40d0 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rss_cfg
+ *
+ * NIX AF Local Function Receive Size Scaling Table Configuration Register
+ * See NIX_AF_LF()_RSS_BASE and NIX_AF_LF()_RSS_GRP().
+ */
+union cavm_nixx_af_lfx_rss_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_rss_cfg_s {
+		u64 size                             : 4;
+		u64 ena                              : 1;
+		u64 reserved_5_19                    : 15;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_rss_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RSS_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RSS_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x40c0 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rss_grp#
+ *
+ * NIX AF Local Function Receive Side Scaling Group Registers
+ * A receive packet targets a LF's RSS group when its NIX_RX_ACTION_S[OP] =
+ * NIX_RX_ACTIONOP_E::RSS, or its target multicast list has an entry with
+ * NIX_RX_MCE_S[OP] = NIX_RX_MCOP_E::RSS. The RSS group index (this register's last
+ * index) is NIX_RX_ACTION_S[INDEX] or NIX_RX_MCE_S[INDEX].
+ *
+ * The RSS computation is as follows:
+ * * The packet's flow_tag (see NIX_LF_RX_SECRET()) and RSS group are used to
+ * select a NIX_RSSE_S entry in the LF's RSS table (see [SIZEM1]).
+ * * NIX_RSSE_S selects the packet's destination RQ.
+ */
+union cavm_nixx_af_lfx_rss_grpx {
+	u64 u;
+	struct cavm_nixx_af_lfx_rss_grpx_s {
+		u64 offset                           : 11;
+		u64 reserved_11_15                   : 5;
+		u64 sizem1                           : 3;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_lfx_rss_grpx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RSS_GRPX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RSS_GRPX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 127) && (b <= 7)))
+		return 0x4600 + 0x20000 * ((a) & 0x7f) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_cfg
+ *
+ * NIX AF Local Function Receive Configuration Register
+ */
+union cavm_nixx_af_lfx_rx_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_cfg_s {
+		u64 reserved_0_32                    : 33;
+		u64 lenerr_en                        : 1;
+		u64 ip6_udp_opt                      : 1;
+		u64 dis_apad                         : 1;
+		u64 csum_il4                         : 1;
+		u64 csum_ol4                         : 1;
+		u64 len_il4                          : 1;
+		u64 len_il3                          : 1;
+		u64 len_ol4                          : 1;
+		u64 len_ol3                          : 1;
+		u64 reserved_42_63                   : 22;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x40a0 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_cfg0
+ *
+ * NIX AF LF Receive IPSEC Configuration Registers
+ */
+union cavm_nixx_af_lfx_rx_ipsec_cfg0 {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_ipsec_cfg0_s {
+		u64 lenm1_max                        : 14;
+		u64 reserved_14_15                   : 2;
+		u64 sa_pow2_size                     : 4;
+		u64 tag_const                        : 24;
+		u64 tt                               : 2;
+		u64 defcpt                           : 1;
+		u64 hshcpt                           : 1;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_ipsec_cfg0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_CFG0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_CFG0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4140 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_cfg1
+ *
+ * NIX AF LF Receive IPSEC Security Association Configuration Register
+ */
+union cavm_nixx_af_lfx_rx_ipsec_cfg1 {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_ipsec_cfg1_s {
+		u64 sa_idx_max                       : 32;
+		u64 sa_idx_w                         : 5;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_ipsec_cfg1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_CFG1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_CFG1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4148 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_dyno_base
+ *
+ * NIX AF LF Receive IPSEC Dynamic Ordering Base Address Registers
+ * This register specifies the base AF IOVA of the LF's IPSEC dynamic ordering
+ * counter table in NDC/LLC/DRAM. The table consists of 1 \<\<
+ * (NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_IDX_W]) NIX_IPSEC_DYNO_S structures.
+ * See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA].
+ */
+union cavm_nixx_af_lfx_rx_ipsec_dyno_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_ipsec_dyno_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_ipsec_dyno_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4158 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_dyno_cfg
+ *
+ * NIX AF LF Receive IPSEC Dynamic Ordering Base Address Registers
+ */
+union cavm_nixx_af_lfx_rx_ipsec_dyno_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_ipsec_dyno_cfg_s {
+		u64 dyno_idx_w                       : 4;
+		u64 dyno_ena                         : 1;
+		u64 reserved_5_19                    : 15;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_ipsec_dyno_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4150 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_sa_base
+ *
+ * NIX AF LF Receive IPSEC Security Association Base Address Register
+ * This register specifies the base IOVA of CPT's IPSEC SA table in LLC/DRAM.
+ */
+union cavm_nixx_af_lfx_rx_ipsec_sa_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_ipsec_sa_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_ipsec_sa_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_SA_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_IPSEC_SA_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4170 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_stat#
+ *
+ * NIX AF Local Function Receive Statistics Registers
+ * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_RX_E.
+ */
+union cavm_nixx_af_lfx_rx_statx {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_statx_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_statx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_STATX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_STATX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 127) && (b <= 11)))
+		return 0x4500 + 0x20000 * ((a) & 0x7f) + 8 * ((b) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_rx_vtag_type#
+ *
+ * NIX AF Local Function Receive Vtag Type Registers
+ * These registers specify optional Vtag (e.g. VLAN, E-TAG) actions for received
+ * packets. Indexed by NIX_RX_VTAG_ACTION_S[VTAG*_TYPE].
+ */
+union cavm_nixx_af_lfx_rx_vtag_typex {
+	u64 u;
+	struct cavm_nixx_af_lfx_rx_vtag_typex_s {
+		u64 size                             : 3;
+		u64 reserved_3                       : 1;
+		u64 strip                            : 1;
+		u64 capture                          : 1;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_nixx_af_lfx_rx_vtag_typex_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_RX_VTAG_TYPEX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_RX_VTAG_TYPEX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 127) && (b <= 7)))
+		return 0x4200 + 0x20000 * ((a) & 0x7f) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_sqs_base
+ *
+ * NIX AF Local Function Send Queues Base Address Register
+ * This register specifies the base AF IOVA of the LF's SQ context table.
+ * The table consists of NIX_AF_LF()_SQS_CFG[MAX_QUEUESM1]+1 contiguous
+ * NIX_SQ_CTX_HW_S structures.
+ */
+union cavm_nixx_af_lfx_sqs_base {
+	u64 u;
+	struct cavm_nixx_af_lfx_sqs_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_lfx_sqs_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_SQS_BASE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_SQS_BASE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4030 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_sqs_cfg
+ *
+ * NIX AF Local Function Send Queues Configuration Register
+ * This register configures send queues in the LF.
+ */
+union cavm_nixx_af_lfx_sqs_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_sqs_cfg_s {
+		u64 max_queuesm1                     : 20;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_lfx_sqs_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_SQS_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_SQS_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4020 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_tx_cfg
+ *
+ * NIX AF Local Function Transmit Configuration Register
+ */
+union cavm_nixx_af_lfx_tx_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_tx_cfg_s {
+		u64 vlan0_ins_etype                  : 16;
+		u64 vlan1_ins_etype                  : 16;
+		u64 send_tstmp_ena                   : 1;
+		u64 lock_viol_cqe_ena                : 1;
+		u64 lock_ena                         : 1;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_lfx_tx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_TX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_TX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4080 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_tx_cfg2
+ *
+ * NIX AF Local Function Transmit Configuration Register
+ */
+union cavm_nixx_af_lfx_tx_cfg2 {
+	u64 u;
+	struct cavm_nixx_af_lfx_tx_cfg2_s {
+		u64 lmt_ena                          : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_lfx_tx_cfg2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_TX_CFG2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_TX_CFG2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4028 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_tx_parse_cfg
+ *
+ * NIX AF Local Function Transmit Parse Configuration Register
+ */
+union cavm_nixx_af_lfx_tx_parse_cfg {
+	u64 u;
+	struct cavm_nixx_af_lfx_tx_parse_cfg_s {
+		u64 pkind                            : 6;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_nixx_af_lfx_tx_parse_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_TX_PARSE_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_TX_PARSE_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4090 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_tx_stat#
+ *
+ * NIX AF Local Function Transmit Statistics Registers
+ * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_TX_E.
+ */
+union cavm_nixx_af_lfx_tx_statx {
+	u64 u;
+	struct cavm_nixx_af_lfx_tx_statx_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_lfx_tx_statx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_TX_STATX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_TX_STATX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 127) && (b <= 4)))
+		return 0x4400 + 0x20000 * ((a) & 0x7f) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf#_tx_status
+ *
+ * NIX AF LF Transmit Status Register
+ */
+union cavm_nixx_af_lfx_tx_status {
+	u64 u;
+	struct cavm_nixx_af_lfx_tx_status_s {
+		u64 sq_ctx_err                       : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_lfx_tx_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LFX_TX_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LFX_TX_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x4180 + 0x20000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lf_rst
+ *
+ * NIX Admin Function LF Reset Register
+ */
+union cavm_nixx_af_lf_rst {
+	u64 u;
+	struct cavm_nixx_af_lf_rst_s {
+		u64 lf                               : 8;
+		u64 reserved_8_11                    : 4;
+		u64 exec                             : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_nixx_af_lf_rst_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LF_RST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LF_RST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x150;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lso_cfg
+ *
+ * NIX AF Large Send Offload Configuration Register
+ */
+union cavm_nixx_af_lso_cfg {
+	u64 u;
+	struct cavm_nixx_af_lso_cfg_s {
+		u64 tcp_lsf                          : 16;
+		u64 tcp_msf                          : 16;
+		u64 tcp_fsf                          : 16;
+		u64 reserved_48_62                   : 15;
+		u64 enable                           : 1;
+	} s;
+	/* struct cavm_nixx_af_lso_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LSO_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LSO_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_lso_format#_field#
+ *
+ * NIX AF Large Send Offload Format Field Registers
+ * These registers specify LSO packet modification formats. Each format may modify
+ * up to eight packet fields with the following constraints:
+ * * If fewer than eight fields are modified, [ALG] must be NIX_LSOALG_E::NOP in the
+ * unused field registers.
+ * * Modified fields must be specified in contiguous field registers starting with
+ * NIX_AF_LSO_FORMAT()_FIELD(0).
+ * * Modified fields cannot overlap.
+ * * Multiple fields with the same [LAYER] value must be specified in
+ * ascending [OFFSET] order.
+ * * Fields in different layers must be specified in ascending [LAYER] order.
+ */
+union cavm_nixx_af_lso_formatx_fieldx {
+	u64 u;
+	struct cavm_nixx_af_lso_formatx_fieldx_s {
+		u64 offset                           : 8;
+		u64 layer                            : 2;
+		u64 reserved_10_11                   : 2;
+		u64 sizem1                           : 2;
+		u64 reserved_14_15                   : 2;
+		u64 alg                              : 3;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_lso_formatx_fieldx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_LSO_FORMATX_FIELDX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_LSO_FORMATX_FIELDX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 31) && (b <= 7)))
+		return 0x1b00 + 0x10000 * ((a) & 0x1f) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mark_format#_ctl
+ *
+ * NIX AF Packet Marking Format Registers
+ * Describes packet marking calculations for YELLOW and for
+ * NIX_COLORRESULT_E::RED_SEND packets. NIX_SEND_EXT_S[MARKFORM] selects the CSR
+ * used for the packet descriptor.
+ *
+ * All the packet marking offset calculations assume big-endian bits within a byte.
+ *
+ * For example, if NIX_SEND_EXT_S[MARKPTR] is 3 and [OFFSET] is 5 and the packet is YELLOW,
+ * the NIX marking hardware would do this:
+ *
+ * _  byte[3]\<2:0\> |=   [Y_VAL]\<3:1\>
+ * _  byte[3]\<2:0\> &= ~[Y_MASK]\<3:1\>
+ * _  byte[4]\<7\>   |=   [Y_VAL]\<0\>
+ * _  byte[4]\<7\>   &= ~[Y_MASK]\<0\>
+ *
+ * where byte[3] is the third byte in the packet, and byte[4] the fourth.
+ *
+ * For another example, if NIX_SEND_EXT_S[MARKPTR] is 3 and [OFFSET] is 0 and the
+ * packet is NIX_COLORRESULT_E::RED_SEND,
+ *
+ * _   byte[3]\<7:4\> |=   [R_VAL]\<3:0\>
+ * _   byte[3]\<7:4\> &= ~[R_MASK]\<3:0\>
+ */
+union cavm_nixx_af_mark_formatx_ctl {
+	u64 u;
+	struct cavm_nixx_af_mark_formatx_ctl_s {
+		u64 r_val                            : 4;
+		u64 r_mask                           : 4;
+		u64 y_val                            : 4;
+		u64 y_mask                           : 4;
+		u64 offset                           : 3;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_mark_formatx_ctl_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MARK_FORMATX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MARK_FORMATX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x900 + 0x40000 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mc_mirror_const
+ *
+ * NIX AF Multicast/Mirror Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_mc_mirror_const {
+	u64 u;
+	struct cavm_nixx_af_mc_mirror_const_s {
+		u64 buf_size                         : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_mc_mirror_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MC_MIRROR_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MC_MIRROR_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x98;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_cir
+ *
+ * NIX AF Meta Descriptor Queue Committed Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_mdqx_cir {
+	u64 u;
+	struct cavm_nixx_af_mdqx_cir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_mdqx_cir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_CIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_CIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1420 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_md_debug
+ *
+ * NIX AF Meta Descriptor Queue Meta Descriptor State Debug Registers
+ * This register provides access to the meta descriptor at the front of the MDQ. An MDQ can
+ * hold up to 8 packet meta descriptors (PMD) and one flush meta descriptor (FMD).
+ */
+union cavm_nixx_af_mdqx_md_debug {
+	u64 u;
+	struct cavm_nixx_af_mdqx_md_debug_s {
+		u64 pkt_len                          : 16;
+		u64 red_algo_override                : 2;
+		u64 shp_dis                          : 1;
+		u64 reserved_19                      : 1;
+		u64 shp_chg                          : 9;
+		u64 reserved_29_31                   : 3;
+		u64 sqm_pkt_id                       : 13;
+		u64 reserved_45_60                   : 16;
+		u64 md_type                          : 2;
+		u64 reserved_63                      : 1;
+	} s;
+	/* struct cavm_nixx_af_mdqx_md_debug_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_MD_DEBUG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_MD_DEBUG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x14c0 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_parent
+ *
+ * NIX AF Meta Descriptor Queue Topology Registers
+ */
+union cavm_nixx_af_mdqx_parent {
+	u64 u;
+	struct cavm_nixx_af_mdqx_parent_s {
+		u64 reserved_0_15                    : 16;
+		u64 parent                           : 9;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_mdqx_parent_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_PARENT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_PARENT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1480 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_pir
+ *
+ * NIX AF Meta Descriptor Queue Peak Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_mdqx_pir {
+	u64 u;
+	struct cavm_nixx_af_mdqx_pir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_mdqx_pir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_PIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_PIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1430 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_pointers
+ *
+ * INTERNAL: NIX AF Meta Descriptor 4 Linked List Pointers Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL4()_POINTERS.
+ */
+union cavm_nixx_af_mdqx_pointers {
+	u64 u;
+	struct cavm_nixx_af_mdqx_pointers_s {
+		u64 next                             : 9;
+		u64 reserved_9_15                    : 7;
+		u64 prev                             : 9;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_mdqx_pointers_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_POINTERS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_POINTERS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1460 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_ptr_fifo
+ *
+ * INTERNAL: NIX Meta Descriptor Queue Pointer FIFO State Debug Registers
+ */
+union cavm_nixx_af_mdqx_ptr_fifo {
+	u64 u;
+	struct cavm_nixx_af_mdqx_ptr_fifo_s {
+		u64 tail                             : 4;
+		u64 head                             : 4;
+		u64 p_con                            : 1;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_nixx_af_mdqx_ptr_fifo_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_PTR_FIFO(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_PTR_FIFO(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x14d0 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_sched_state
+ *
+ * NIX AF Meta Descriptor Queue Scheduling Control State Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SCHED_STATE.
+ */
+union cavm_nixx_af_mdqx_sched_state {
+	u64 u;
+	struct cavm_nixx_af_mdqx_sched_state_s {
+		u64 rr_count                         : 25;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_mdqx_sched_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_SCHED_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_SCHED_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1440 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_schedule
+ *
+ * NIX AF Meta Descriptor Queue Scheduling Control Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SCHEDULE.
+ */
+union cavm_nixx_af_mdqx_schedule {
+	u64 u;
+	struct cavm_nixx_af_mdqx_schedule_s {
+		u64 rr_quantum                       : 24;
+		u64 prio                             : 4;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_nixx_af_mdqx_schedule_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_SCHEDULE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_SCHEDULE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1400 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_shape
+ *
+ * NIX AF Meta Descriptor Queue Shaping Control Registers
+ * This register has the same bit fields as NIX_AF_TL3()_SHAPE.
+ */
+union cavm_nixx_af_mdqx_shape {
+	u64 u;
+	struct cavm_nixx_af_mdqx_shape_s {
+		u64 adjust                           : 9;
+		u64 red_algo                         : 2;
+		u64 red_disable                      : 1;
+		u64 yellow_disable                   : 1;
+		u64 reserved_13_23                   : 11;
+		u64 length_disable                   : 1;
+		u64 schedule_list                    : 2;
+		u64 reserved_27_63                   : 37;
+	} s;
+	/* struct cavm_nixx_af_mdqx_shape_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_SHAPE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_SHAPE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1410 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_shape_state
+ *
+ * NIX AF Meta Descriptor Queue Shaping State Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SHAPE_STATE.
+ * This register must not be written during normal operation.
+ */
+union cavm_nixx_af_mdqx_shape_state {
+	u64 u;
+	struct cavm_nixx_af_mdqx_shape_state_s {
+		u64 cir_accum                        : 26;
+		u64 pir_accum                        : 26;
+		u64 color                            : 2;
+		u64 reserved_54_63                   : 10;
+	} s;
+	/* struct cavm_nixx_af_mdqx_shape_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_SHAPE_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_SHAPE_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1450 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq#_sw_xoff
+ *
+ * NIX AF Meta Descriptor Controlled XOFF Registers
+ * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF
+ */
+union cavm_nixx_af_mdqx_sw_xoff {
+	u64 u;
+	struct cavm_nixx_af_mdqx_sw_xoff_s {
+		u64 xoff                             : 1;
+		u64 drain                            : 1;
+		u64 reserved_2                       : 1;
+		u64 drain_irq                        : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_mdqx_sw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQX_SW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQX_SW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1470 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_mdq_const
+ *
+ * NIX AF Meta Descriptor Queue Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_mdq_const {
+	u64 u;
+	struct cavm_nixx_af_mdq_const_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_mdq_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_MDQ_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_MDQ_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x90;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ndc_cfg
+ *
+ * NIX AF General Configuration Register
+ */
+union cavm_nixx_af_ndc_cfg {
+	u64 u;
+	struct cavm_nixx_af_ndc_cfg_s {
+		u64 ndc_ign_pois                     : 1;
+		u64 byp_sq                           : 1;
+		u64 byp_sqb                          : 1;
+		u64 byp_cqs                          : 1;
+		u64 byp_cints                        : 1;
+		u64 byp_dyno                         : 1;
+		u64 byp_mce                          : 1;
+		u64 byp_rqc                          : 1;
+		u64 byp_rsse                         : 1;
+		u64 byp_mc_data                      : 1;
+		u64 byp_mc_wqe                       : 1;
+		u64 byp_mr_data                      : 1;
+		u64 byp_mr_wqe                       : 1;
+		u64 byp_qints                        : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_nixx_af_ndc_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_NDC_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_NDC_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x18;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ndc_rx_sync
+ *
+ * NIX AF Receive NDC Sync Register
+ * Used to synchronize the NIX receive NDC (NDC_IDX_E::NIX()_RX).
+ */
+union cavm_nixx_af_ndc_rx_sync {
+	u64 u;
+	struct cavm_nixx_af_ndc_rx_sync_s {
+		u64 lf                               : 8;
+		u64 reserved_8_11                    : 4;
+		u64 exec                             : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_nixx_af_ndc_rx_sync_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_NDC_RX_SYNC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_NDC_RX_SYNC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3e0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ndc_tx_sync
+ *
+ * NIX AF NDC_TX Sync Register
+ * Used to synchronize the NIX transmit NDC (NDC_IDX_E::NIX()_TX).
+ */
+union cavm_nixx_af_ndc_tx_sync {
+	u64 u;
+	struct cavm_nixx_af_ndc_tx_sync_s {
+		u64 lf                               : 8;
+		u64 reserved_8_11                    : 4;
+		u64 exec                             : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_nixx_af_ndc_tx_sync_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_NDC_TX_SYNC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_NDC_TX_SYNC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3f0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_norm_tx_fifo_status
+ *
+ * NIX AF Normal Transmit FIFO Status Register
+ * Status of FIFO which transmits normal (potentially preemptable) packets to CGX and LBK.
+ */
+union cavm_nixx_af_norm_tx_fifo_status {
+	u64 u;
+	struct cavm_nixx_af_norm_tx_fifo_status_s {
+		u64 count                            : 12;
+		u64 reserved_12_63                   : 52;
+	} s;
+	/* struct cavm_nixx_af_norm_tx_fifo_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_NORM_TX_FIFO_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_NORM_TX_FIFO_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x648;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_active_cycles_pc
+ *
+ * NIX AF Active Cycles Register
+ * These registers are indexed by the conditional clock domain number.
+ */
+union cavm_nixx_af_pse_active_cycles_pc {
+	u64 u;
+	struct cavm_nixx_af_pse_active_cycles_pc_s {
+		u64 act_cyc                          : 64;
+	} s;
+	/* struct cavm_nixx_af_pse_active_cycles_pc_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_ACTIVE_CYCLES_PC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_ACTIVE_CYCLES_PC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8c0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_bp_test0
+ *
+ * INTERNAL: NIX AF PSE Backpressure Test 0 Register
+ */
+union cavm_nixx_af_pse_bp_test0 {
+	u64 u;
+	struct cavm_nixx_af_pse_bp_test0_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_59                   : 36;
+		u64 enable                           : 4;
+	} s;
+	/* struct cavm_nixx_af_pse_bp_test0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST0
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST0
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x840;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_bp_test1
+ *
+ * INTERNAL: NIX AF PSE Backpressure Test 1 Register
+ */
+union cavm_nixx_af_pse_bp_test1 {
+	u64 u;
+	struct cavm_nixx_af_pse_bp_test1_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_59                   : 36;
+		u64 enable                           : 4;
+	} s;
+	/* struct cavm_nixx_af_pse_bp_test1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST1
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST1
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x850;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_bp_test2
+ *
+ * INTERNAL: NIX AF PSE Backpressure Test 2 Register
+ */
+union cavm_nixx_af_pse_bp_test2 {
+	u64 u;
+	struct cavm_nixx_af_pse_bp_test2_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_59                   : 36;
+		u64 enable                           : 4;
+	} s;
+	/* struct cavm_nixx_af_pse_bp_test2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST2
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST2
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x860;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_bp_test3
+ *
+ * INTERNAL: NIX AF PSE Backpressure Test 3 Register
+ */
+union cavm_nixx_af_pse_bp_test3 {
+	u64 u;
+	struct cavm_nixx_af_pse_bp_test3_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_59                   : 36;
+		u64 enable                           : 4;
+	} s;
+	/* struct cavm_nixx_af_pse_bp_test3_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST3
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_BP_TEST3
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x870;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_channel_level
+ *
+ * NIX AF PSE Channel Level Register
+ */
+union cavm_nixx_af_pse_channel_level {
+	u64 u;
+	struct cavm_nixx_af_pse_channel_level_s {
+		u64 bp_level                         : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_pse_channel_level_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_CHANNEL_LEVEL
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_CHANNEL_LEVEL
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x800;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_const
+ *
+ * NIX AF PSE Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_pse_const {
+	u64 u;
+	struct cavm_nixx_af_pse_const_s {
+		u64 levels                           : 4;
+		u64 reserved_4_7                     : 4;
+		u64 mark_formats                     : 8;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_pse_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x60;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_expr_bp_test
+ *
+ * INTERNAL: NIX AF PSE Express Backpressure Test Register
+ */
+union cavm_nixx_af_pse_expr_bp_test {
+	u64 u;
+	struct cavm_nixx_af_pse_expr_bp_test_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 32;
+		u64 enable                           : 16;
+	} s;
+	/* struct cavm_nixx_af_pse_expr_bp_test_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_EXPR_BP_TEST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_EXPR_BP_TEST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x890;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_norm_bp_test
+ *
+ * INTERNAL: NIX AF PSE Normal Backpressure Test Register
+ */
+union cavm_nixx_af_pse_norm_bp_test {
+	u64 u;
+	struct cavm_nixx_af_pse_norm_bp_test_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 32;
+		u64 enable                           : 16;
+	} s;
+	/* struct cavm_nixx_af_pse_norm_bp_test_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_NORM_BP_TEST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_NORM_BP_TEST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x880;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_pse_shaper_cfg
+ *
+ * NIX AF PSE Shaper Configuration Register
+ */
+union cavm_nixx_af_pse_shaper_cfg {
+	u64 u;
+	struct cavm_nixx_af_pse_shaper_cfg_s {
+		u64 red_send_as_yellow               : 1;
+		u64 color_aware                      : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_nixx_af_pse_shaper_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_PSE_SHAPER_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_PSE_SHAPER_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x810;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ras
+ *
+ * NIX AF RAS Interrupt Register
+ * This register is intended for delivery of RAS events to the SCP, so should be
+ * ignored by OS drivers.
+ */
+union cavm_nixx_af_ras {
+	u64 u;
+	struct cavm_nixx_af_ras_s {
+		u64 rx_mce_poison                    : 1;
+		u64 rx_mcast_wqe_poison              : 1;
+		u64 rx_mirror_wqe_poison             : 1;
+		u64 rx_mcast_data_poison             : 1;
+		u64 rx_mirror_data_poison            : 1;
+		u64 reserved_5_31                    : 27;
+		u64 aq_ctx_poison                    : 1;
+		u64 aq_res_poison                    : 1;
+		u64 aq_inst_poison                   : 1;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_ras_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RAS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RAS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1a0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ras_ena_w1c
+ *
+ * NIX AF RAS Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_af_ras_ena_w1c {
+	u64 u;
+	struct cavm_nixx_af_ras_ena_w1c_s {
+		u64 rx_mce_poison                    : 1;
+		u64 rx_mcast_wqe_poison              : 1;
+		u64 rx_mirror_wqe_poison             : 1;
+		u64 rx_mcast_data_poison             : 1;
+		u64 rx_mirror_data_poison            : 1;
+		u64 reserved_5_31                    : 27;
+		u64 aq_ctx_poison                    : 1;
+		u64 aq_res_poison                    : 1;
+		u64 aq_inst_poison                   : 1;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_ras_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RAS_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RAS_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1b8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ras_ena_w1s
+ *
+ * NIX AF RAS Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_af_ras_ena_w1s {
+	u64 u;
+	struct cavm_nixx_af_ras_ena_w1s_s {
+		u64 rx_mce_poison                    : 1;
+		u64 rx_mcast_wqe_poison              : 1;
+		u64 rx_mirror_wqe_poison             : 1;
+		u64 rx_mcast_data_poison             : 1;
+		u64 rx_mirror_data_poison            : 1;
+		u64 reserved_5_31                    : 27;
+		u64 aq_ctx_poison                    : 1;
+		u64 aq_res_poison                    : 1;
+		u64 aq_inst_poison                   : 1;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_ras_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RAS_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RAS_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1b0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_ras_w1s
+ *
+ * NIX AF RAS Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_af_ras_w1s {
+	u64 u;
+	struct cavm_nixx_af_ras_w1s_s {
+		u64 rx_mce_poison                    : 1;
+		u64 rx_mcast_wqe_poison              : 1;
+		u64 rx_mirror_wqe_poison             : 1;
+		u64 rx_mcast_data_poison             : 1;
+		u64 rx_mirror_data_poison            : 1;
+		u64 reserved_5_31                    : 27;
+		u64 aq_ctx_poison                    : 1;
+		u64 aq_res_poison                    : 1;
+		u64 aq_inst_poison                   : 1;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_ras_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RAS_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RAS_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1a8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_reb_bp_test#
+ *
+ * INTERNAL: NIX AF REB Backpressure Test Registers
+ */
+union cavm_nixx_af_reb_bp_testx {
+	u64 u;
+	struct cavm_nixx_af_reb_bp_testx_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_47                   : 24;
+		u64 enable                           : 4;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_nixx_af_reb_bp_testx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_REB_BP_TESTX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_REB_BP_TESTX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 28))
+		return 0x4840 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rq_const
+ *
+ * NIX AF RQ Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_rq_const {
+	u64 u;
+	struct cavm_nixx_af_rq_const_s {
+		u64 queues_per_lf                    : 24;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_rq_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RQ_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RQ_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x50;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rqm_bp_test
+ *
+ * INTERNAL: NIX AF REB Backpressure Test Registers
+ */
+union cavm_nixx_af_rqm_bp_test {
+	u64 u;
+	struct cavm_nixx_af_rqm_bp_test_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 16;
+		u64 reserved_32_47                   : 16;
+		u64 enable                           : 8;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_nixx_af_rqm_bp_test_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RQM_BP_TEST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RQM_BP_TEST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x4880;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rvu_int
+ *
+ * NIX AF RVU Interrupt Register
+ * This register contains RVU error interrupt summary bits.
+ */
+union cavm_nixx_af_rvu_int {
+	u64 u;
+	struct cavm_nixx_af_rvu_int_s {
+		u64 unmapped_slot                    : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_rvu_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RVU_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RVU_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1c0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rvu_int_ena_w1c
+ *
+ * NIX AF RVU Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_af_rvu_int_ena_w1c {
+	u64 u;
+	struct cavm_nixx_af_rvu_int_ena_w1c_s {
+		u64 unmapped_slot                    : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_rvu_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RVU_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RVU_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1d8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rvu_int_ena_w1s
+ *
+ * NIX AF RVU Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_af_rvu_int_ena_w1s {
+	u64 u;
+	struct cavm_nixx_af_rvu_int_ena_w1s_s {
+		u64 unmapped_slot                    : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_rvu_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RVU_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RVU_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1d0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rvu_int_w1s
+ *
+ * NIX AF RVU Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_af_rvu_int_w1s {
+	u64 u;
+	struct cavm_nixx_af_rvu_int_w1s_s {
+		u64 unmapped_slot                    : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_rvu_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RVU_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RVU_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1c8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rvu_lf_cfg_debug
+ *
+ * NIX Privileged LF Configuration Debug Register
+ * This debug register allows software to lookup the reverse mapping from VF/PF
+ * slot to LF. The forward mapping is programmed with NIX_PRIV_LF()_CFG.
+ */
+union cavm_nixx_af_rvu_lf_cfg_debug {
+	u64 u;
+	struct cavm_nixx_af_rvu_lf_cfg_debug_s {
+		u64 lf                               : 12;
+		u64 lf_valid                         : 1;
+		u64 exec                             : 1;
+		u64 reserved_14_15                   : 2;
+		u64 slot                             : 8;
+		u64 pf_func                          : 16;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_rvu_lf_cfg_debug_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RVU_LF_CFG_DEBUG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RVU_LF_CFG_DEBUG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8000030;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_active_cycles_pc#
+ *
+ * NIX AF Active Cycles Register
+ * These registers are indexed by the conditional clock domain number.
+ */
+union cavm_nixx_af_rx_active_cycles_pcx {
+	u64 u;
+	struct cavm_nixx_af_rx_active_cycles_pcx_s {
+		u64 act_cyc                          : 64;
+	} s;
+	/* struct cavm_nixx_af_rx_active_cycles_pcx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_ACTIVE_CYCLES_PCX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_ACTIVE_CYCLES_PCX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 30))
+		return 0x4800 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_bpid#_status
+ *
+ * NIX AF Receive Backpressure ID Status Registers
+ */
+union cavm_nixx_af_rx_bpidx_status {
+	u64 u;
+	struct cavm_nixx_af_rx_bpidx_status_s {
+		u64 aura_cnt                         : 32;
+		u64 cq_cnt                           : 32;
+	} s;
+	/* struct cavm_nixx_af_rx_bpidx_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_BPIDX_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_BPIDX_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1a20 + 0x20000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_cfg
+ *
+ * NIX AF Receive Configuration Register
+ */
+union cavm_nixx_af_rx_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_cfg_s {
+		u64 cbp_ena                          : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_rx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xd0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_chan#_cfg
+ *
+ * NIX AF Receive Channel Configuration Registers
+ */
+union cavm_nixx_af_rx_chanx_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_chanx_cfg_s {
+		u64 bpid                             : 9;
+		u64 reserved_9_15                    : 7;
+		u64 bp_ena                           : 1;
+		u64 sw_xoff                          : 1;
+		u64 imp                              : 1;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_rx_chanx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_CHANX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_CHANX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 4095))
+		return 0x1a30 + 0x8000 * ((a) & 0xfff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_cpt#_credit
+ *
+ * NIX AF Receive CPT Credit Register
+ */
+union cavm_nixx_af_rx_cptx_credit {
+	u64 u;
+	struct cavm_nixx_af_rx_cptx_credit_s {
+		u64 inst_cred_cnt                    : 22;
+		u64 reserved_22_63                   : 42;
+	} s;
+	/* struct cavm_nixx_af_rx_cptx_credit_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_CPTX_CREDIT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_CPTX_CREDIT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0x360 + 8 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_cpt#_inst_qsel
+ *
+ * NIX AF Receive CPT Instruction Queue Select Register
+ * Selects the CPT queue to which instructions (CPT_INST_S) are sent.
+ * Internal:
+ * NIX sends CPT_INST_S to the CPT_LF_NQ() physical address for [PF_FUNC] and [SLOT]:
+ * \<pre\>
+ * // CPT_LF_NQ() physical address:
+ * chip_pa_defs::io_rvu2a_t cpt_addr;
+ * cpt_addr = RVU_BAR_E::RVU_PF()_FUNC()_BAR2(pf, func);
+ * cpt_addr.block = RVU_BLOCK_ADDR_E::CPT()({a}); // {a} = CPT index
+ * cpt_addr.slot = [SLOT];
+ * cpt_addr.offset = `CPT_LF_NQX__BASE;
+ *
+ * // NDC/NCBI command:
+ * ncbi_cmd.paddr = 1; // Physical address
+ * ncbi_cmd.addr = cpt_addr;
+ * \</pre\>
+ */
+union cavm_nixx_af_rx_cptx_inst_qsel {
+	u64 u;
+	struct cavm_nixx_af_rx_cptx_inst_qsel_s {
+		u64 slot                             : 8;
+		u64 pf_func                          : 16;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_rx_cptx_inst_qsel_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_CPTX_INST_QSEL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_CPTX_INST_QSEL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0x320 + 8 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_iip4
+ *
+ * NIX AF Receive Inner IPv4 Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an inner IPv4 header.
+ * Typically the same as NPC_PCK_DEF_IIP4.
+ */
+union cavm_nixx_af_rx_def_iip4 {
+	u64 u;
+	struct cavm_nixx_af_rx_def_iip4_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_iip4_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_IIP4
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_IIP4
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x220;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_iip6
+ *
+ * NIX AF Receive Inner IPv6 Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an inner IPv6 header.
+ */
+union cavm_nixx_af_rx_def_iip6 {
+	u64 u;
+	struct cavm_nixx_af_rx_def_iip6_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_iip6_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_IIP6
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_IIP6
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x240;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_ipsec#
+ *
+ * NIX AF Receive IPSEC Header Definition Registers
+ * These two registers define layer information in NPC_RESULT_S to identify an
+ * IPSEC header for up to two IPSEC packet formats. The two formats are typically
+ * IPSEC ESP (RFC 4303) and UDP-encapsulated IPSEC ESP (RFC 3948).
+ */
+union cavm_nixx_af_rx_def_ipsecx {
+	u64 u;
+	struct cavm_nixx_af_rx_def_ipsecx_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11                      : 1;
+		u64 spi_offset                       : 4;
+		u64 spi_nz                           : 1;
+		u64 reserved_17_63                   : 47;
+	} s;
+	/* struct cavm_nixx_af_rx_def_ipsecx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_IPSECX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_IPSECX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x2b0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_isctp
+ *
+ * NIX AF Receive Inner SCTP Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an inner SCTP header.
+ */
+union cavm_nixx_af_rx_def_isctp {
+	u64 u;
+	struct cavm_nixx_af_rx_def_isctp_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_isctp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_ISCTP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_ISCTP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2a0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_itcp
+ *
+ * NIX AF Receive Inner TCP Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an inner TCP header.
+ */
+union cavm_nixx_af_rx_def_itcp {
+	u64 u;
+	struct cavm_nixx_af_rx_def_itcp_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_itcp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_ITCP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_ITCP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x260;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_iudp
+ *
+ * NIX AF Receive Inner UDP Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an inner UDP header.
+ */
+union cavm_nixx_af_rx_def_iudp {
+	u64 u;
+	struct cavm_nixx_af_rx_def_iudp_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_iudp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_IUDP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_IUDP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x280;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_oip4
+ *
+ * NIX AF Receive Outer IPv4 Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an outer IPv4 L3 header.
+ * Typically the same as NPC_PCK_DEF_OIP4.
+ */
+union cavm_nixx_af_rx_def_oip4 {
+	u64 u;
+	struct cavm_nixx_af_rx_def_oip4_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_oip4_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_OIP4
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_OIP4
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x210;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_oip6
+ *
+ * NIX AF Receive Outer IPv6 Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an outer IPv6 header.
+ * Typically the same as NPC_PCK_DEF_OIP6.
+ */
+union cavm_nixx_af_rx_def_oip6 {
+	u64 u;
+	struct cavm_nixx_af_rx_def_oip6_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_oip6_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_OIP6
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_OIP6
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x230;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_ol2
+ *
+ * NIX AF Receive Outer L2 Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an outer L2/Ethernet
+ * header. Typically the same as NPC_PCK_DEF_OL2.
+ */
+union cavm_nixx_af_rx_def_ol2 {
+	u64 u;
+	struct cavm_nixx_af_rx_def_ol2_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_ol2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_OL2
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_OL2
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x200;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_osctp
+ *
+ * NIX AF Receive Outer SCTP Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an outer SCTP header.
+ */
+union cavm_nixx_af_rx_def_osctp {
+	u64 u;
+	struct cavm_nixx_af_rx_def_osctp_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_osctp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_OSCTP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_OSCTP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x290;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_otcp
+ *
+ * NIX AF Receive Outer TCP Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an outer TCP header.
+ */
+union cavm_nixx_af_rx_def_otcp {
+	u64 u;
+	struct cavm_nixx_af_rx_def_otcp_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_otcp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_OTCP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_OTCP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x250;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_def_oudp
+ *
+ * NIX AF Receive Outer UDP Header Definition Register
+ * Defines layer information in NPC_RESULT_S to identify an outer UDP header.
+ */
+union cavm_nixx_af_rx_def_oudp {
+	u64 u;
+	struct cavm_nixx_af_rx_def_oudp_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_af_rx_def_oudp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_DEF_OUDP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_DEF_OUDP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x270;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_flow_key_alg#_field#
+ *
+ * NIX AF Receive Flow Key Algorithm Field Registers
+ * A flow key algorithm defines how the 40-byte FLOW_KEY is formed from the received
+ * packet header. FLOW_KEY is formed using up to five header fields (this register's
+ * last index) with up to 16 bytes per field.
+ *
+ * The algorithm (index {a} (ALG) of these registers) is selected by
+ * NIX_RX_ACTION_S[FLOW_KEY_ALG] from the packet's NPC_RESULT_S[ACTION].
+ *
+ * Internal:
+ * 40-byte FLOW_KEY is wide enough to support an IPv6 5-tuple that includes a
+ * VXLAN/GENEVE/NVGRE tunnel ID, e.g:
+ * _ Source IP: 16B.
+ * _ Dest IP: 16B.
+ * _ Source port: 2B.
+ * _ Dest port: 2B.
+ * _ Tunnel VNI/VSI: 3B.
+ * _ Total: 39B.
+ */
+union cavm_nixx_af_rx_flow_key_algx_fieldx {
+	u64 u;
+	struct cavm_nixx_af_rx_flow_key_algx_fieldx_s {
+		u64 key_offset                       : 6;
+		u64 ln_mask                          : 1;
+		u64 fn_mask                          : 1;
+		u64 hdr_offset                       : 8;
+		u64 bytesm1                          : 5;
+		u64 lid                              : 3;
+		u64 reserved_24                      : 1;
+		u64 ena                              : 1;
+		u64 sel_chan                         : 1;
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_nixx_af_rx_flow_key_algx_fieldx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 31) && (b <= 4)))
+		return 0x1800 + 0x40000 * ((a) & 0x1f) + 8 * ((b) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_ipsec_gen_cfg
+ *
+ * NIX AF Receive IPSEC General Configuration Register
+ * This register specifie the values of certain fields in CPT instructions
+ * (CPT_INST_S) generated by NIX for IPSEC hardware fast-path packets.
+ */
+union cavm_nixx_af_rx_ipsec_gen_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_ipsec_gen_cfg_s {
+		u64 param2                           : 16;
+		u64 param1                           : 16;
+		u64 opcode                           : 16;
+		u64 egrp                             : 3;
+		u64 reserved_51_63                   : 13;
+	} s;
+	/* struct cavm_nixx_af_rx_ipsec_gen_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_IPSEC_GEN_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_IPSEC_GEN_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x300;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_link#_cfg
+ *
+ * NIX AF Receive Link Configuration Registers
+ * Index enumerated by NIX_LINK_E.
+ */
+union cavm_nixx_af_rx_linkx_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_linkx_cfg_s {
+		u64 minlen                           : 16;
+		u64 maxlen                           : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_rx_linkx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_LINKX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_LINKX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 13))
+		return 0x540 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_link#_sl#_spkt_cnt
+ *
+ * INTERNAL: NIX Receive Software Sync Link Packet Count Registers
+ *
+ * For diagnostic use only for debug of NIX_AF_RX_SW_SYNC[ENA] function. LINK
+ * index is enumerated by NIX_LINK_E. SL index is zero for non-express packets,
+ * one for express packets. For the internal NIX_LINK_E::MC, SL index is zero for
+ * multicast replay, one for mirror replay.
+ */
+union cavm_nixx_af_rx_linkx_slx_spkt_cnt {
+	u64 u;
+	struct cavm_nixx_af_rx_linkx_slx_spkt_cnt_s {
+		u64 in_cnt                           : 20;
+		u64 reserved_20_31                   : 12;
+		u64 out_cnt                          : 20;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_nixx_af_rx_linkx_slx_spkt_cnt_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 14) && (b <= 1)))
+		return 0x500 + 0x10000 * ((a) & 0xf) + 8 * ((b) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_link#_wrr_cfg
+ *
+ * NIX AF Receive Link Weighted Round Robin Configuration Registers
+ * Index enumerated by NIX_LINK_E.
+ */
+union cavm_nixx_af_rx_linkx_wrr_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_linkx_wrr_cfg_s {
+		u64 weight                           : 8;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_nixx_af_rx_linkx_wrr_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_LINKX_WRR_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_LINKX_WRR_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 14))
+		return 0x560 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_mcast_base
+ *
+ * NIX AF Receive Multicast/Mirror Table Base Address Register
+ * This register specifies the base AF IOVA of the receive multicast/mirror
+ * table in NDC/LLC/DRAM. The table consists of 1 \<\< (NIX_AF_RX_MCAST_CFG[SIZE]+8)
+ * contiguous NIX_RX_MCE_S structures. The size of each structure is
+ * 1 \<\< NIX_AF_CONST3[MCE_LOG2BYTES].
+ *
+ * The table contains multicast/mirror replication lists. Each list consists of
+ * linked entries with NIX_RX_MCE_S[EOL] = 1 in the last entry. All lists
+ * must reside within the table size specified by NIX_AF_RX_MCAST_CFG[SIZE]. A
+ * mirror replication list will typically consist of two entries, but that is not
+ * checked or enforced by hardware.
+ *
+ * A receive packet is multicast when the action returned by NPC has
+ * NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::MCAST.
+ * A receive packet is mirrored when the action returned by NPC has
+ * NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::MIRROR.
+ * In both cases, NIX_RX_ACTION_S[INDEX] specifies the index of the replication
+ * list's first NIX_RX_MCE_S in the table, and a linked entry with
+ * NIX_RX_MCE_S[EOL] = 1 indicates the end of list.
+ *
+ * If a mirrored flow is part of a multicast replication list, software should
+ * include the two mirror entries in that list.
+ *
+ * Internal:
+ * A multicast list may have multiple entries for the same LF (e.g. for future
+ * RoCE/IB multicast).
+ */
+union cavm_nixx_af_rx_mcast_base {
+	u64 u;
+	struct cavm_nixx_af_rx_mcast_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_rx_mcast_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_MCAST_BASE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_MCAST_BASE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x100;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_mcast_buf_base
+ *
+ * NIX AF Receive Multicast Buffer Base Address Register
+ * This register specifies the base AF IOVA of the receive multicast
+ * buffers in NDC/LLC/DRAM. These buffers are used to temporarily store packets
+ * whose action returned by NPC has NIX_RX_ACTION_S[OP] =
+ * NIX_RX_ACTIONOP_E::MCAST. The number of buffers is configured by
+ * NIX_AF_RX_MCAST_BUF_CFG[SIZE].
+ *
+ * If the number of free buffers is insufficient for a received multicast packet,
+ * hardware tail drops the packet and sets NIX_AF_GEN_INT[RX_MCAST_DROP].
+ *
+ * Hardware prioritizes the processing of RX mirror packets over RX multicast
+ * packets.
+ */
+union cavm_nixx_af_rx_mcast_buf_base {
+	u64 u;
+	struct cavm_nixx_af_rx_mcast_buf_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_rx_mcast_buf_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_MCAST_BUF_BASE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_MCAST_BUF_BASE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x120;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_mcast_buf_cfg
+ *
+ * NIX AF Receive Multicast Buffer Configuration Register
+ * See NIX_AF_RX_MCAST_BUF_BASE.
+ */
+union cavm_nixx_af_rx_mcast_buf_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_mcast_buf_cfg_s {
+		u64 size                             : 4;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_21_23                   : 3;
+		u64 npc_replay_pkind                 : 6;
+		u64 reserved_30_31                   : 2;
+		u64 free_buf_level                   : 11;
+		u64 reserved_43_62                   : 20;
+		u64 ena                              : 1;
+	} s;
+	/* struct cavm_nixx_af_rx_mcast_buf_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_MCAST_BUF_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_MCAST_BUF_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x130;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_mcast_cfg
+ *
+ * NIX AF Receive Multicast/Mirror Table Configuration Register
+ * See NIX_AF_RX_MCAST_BASE.
+ */
+union cavm_nixx_af_rx_mcast_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_mcast_cfg_s {
+		u64 size                             : 4;
+		u64 max_list_lenm1                   : 8;
+		u64 reserved_12_19                   : 8;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_37_63                   : 27;
+	} s;
+	/* struct cavm_nixx_af_rx_mcast_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_MCAST_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_MCAST_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x110;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_mirror_buf_base
+ *
+ * NIX AF Receive Mirror Buffer Base Address Register
+ * This register specifies the base AF IOVA of the receive mirror buffers
+ * in NDC/LLC/DRAM. These buffers are used to temporarily store packets whose
+ * action returned by NPC has NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::MIRROR. The
+ * number of buffers is configured by NIX_AF_RX_MIRROR_BUF_CFG[SIZE].
+ *
+ * If the number of free buffers is insufficient for a received multicast packet,
+ * hardware tail drops the packet and sets NIX_AF_GEN_INT[RX_MIRROR_DROP].
+ *
+ * Hardware prioritizes the processing of RX mirror packets over RX multicast
+ * packets.
+ */
+union cavm_nixx_af_rx_mirror_buf_base {
+	u64 u;
+	struct cavm_nixx_af_rx_mirror_buf_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_rx_mirror_buf_base_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_MIRROR_BUF_BASE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_MIRROR_BUF_BASE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x140;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_mirror_buf_cfg
+ *
+ * NIX AF Receive Mirror Buffer Configuration Register
+ * See NIX_AF_RX_MIRROR_BUF_BASE.
+ */
+union cavm_nixx_af_rx_mirror_buf_cfg {
+	u64 u;
+	struct cavm_nixx_af_rx_mirror_buf_cfg_s {
+		u64 size                             : 4;
+		u64 way_mask                         : 16;
+		u64 caching                          : 1;
+		u64 reserved_21_23                   : 3;
+		u64 npc_replay_pkind                 : 6;
+		u64 reserved_30_31                   : 2;
+		u64 free_buf_level                   : 11;
+		u64 reserved_43_62                   : 20;
+		u64 ena                              : 1;
+	} s;
+	/* struct cavm_nixx_af_rx_mirror_buf_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_MIRROR_BUF_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_MIRROR_BUF_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x148;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_npc_mc_drop
+ *
+ * NIX AF Multicast Drop Statistics Register
+ * The counter increments for every dropped MC packet marked by the NPC.
+ */
+union cavm_nixx_af_rx_npc_mc_drop {
+	u64 u;
+	struct cavm_nixx_af_rx_npc_mc_drop_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_rx_npc_mc_drop_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_NPC_MC_DROP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_NPC_MC_DROP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x4710;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_npc_mc_rcv
+ *
+ * NIX AF Multicast Receive Statistics Register
+ * The counter increments for every recieved MC packet marked by the NPC.
+ */
+union cavm_nixx_af_rx_npc_mc_rcv {
+	u64 u;
+	struct cavm_nixx_af_rx_npc_mc_rcv_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_rx_npc_mc_rcv_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_NPC_MC_RCV
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_NPC_MC_RCV
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x4700;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_npc_mirror_drop
+ *
+ * NIX AF Mirror Drop Statistics Register
+ * The counter increments for every dropped MIRROR packet marked by the NPC.
+ */
+union cavm_nixx_af_rx_npc_mirror_drop {
+	u64 u;
+	struct cavm_nixx_af_rx_npc_mirror_drop_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_rx_npc_mirror_drop_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_NPC_MIRROR_DROP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_NPC_MIRROR_DROP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x4730;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_npc_mirror_rcv
+ *
+ * NIX AF Mirror Receive Statistics Register
+ * The counter increments for every recieved MIRROR packet marked by the NPC.
+ */
+union cavm_nixx_af_rx_npc_mirror_rcv {
+	u64 u;
+	struct cavm_nixx_af_rx_npc_mirror_rcv_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_rx_npc_mirror_rcv_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_NPC_MIRROR_RCV
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_NPC_MIRROR_RCV
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x4720;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_rx_sw_sync
+ *
+ * NIX AF Receive Software Sync Register
+ */
+union cavm_nixx_af_rx_sw_sync {
+	u64 u;
+	struct cavm_nixx_af_rx_sw_sync_s {
+		u64 ena                              : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_rx_sw_sync_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_RX_SW_SYNC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_RX_SW_SYNC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x550;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sdp_hw_xoff#
+ *
+ * NIX AF SDP Transmit Link Hardware Controlled XOFF Registers
+ * .
+ */
+union cavm_nixx_af_sdp_hw_xoffx {
+	u64 u;
+	struct cavm_nixx_af_sdp_hw_xoffx_s {
+		u64 chan_xoff                        : 64;
+	} s;
+	/* struct cavm_nixx_af_sdp_hw_xoffx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SDP_HW_XOFFX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SDP_HW_XOFFX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xac0 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sdp_link_credit
+ *
+ * NIX AF Transmit Link SDP Credit Register
+ * This register tracks SDP link credits.
+ */
+union cavm_nixx_af_sdp_link_credit {
+	u64 u;
+	struct cavm_nixx_af_sdp_link_credit_s {
+		u64 reserved_0                       : 1;
+		u64 cc_enable                        : 1;
+		u64 cc_packet_cnt                    : 10;
+		u64 cc_unit_cnt                      : 20;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_sdp_link_credit_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SDP_LINK_CREDIT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SDP_LINK_CREDIT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa40;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sdp_sw_xoff#
+ *
+ * NIX AF SDP Transmit Link Software Controlled XOFF Registers
+ */
+union cavm_nixx_af_sdp_sw_xoffx {
+	u64 u;
+	struct cavm_nixx_af_sdp_sw_xoffx_s {
+		u64 chan_xoff                        : 64;
+	} s;
+	/* struct cavm_nixx_af_sdp_sw_xoffx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SDP_SW_XOFFX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SDP_SW_XOFFX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 3))
+		return 0xa60 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sdp_tx_fifo_status
+ *
+ * NIX AF SDP Transmit FIFO Status Register
+ * Status of FIFO which transmits packets to SDP.
+ */
+union cavm_nixx_af_sdp_tx_fifo_status {
+	u64 u;
+	struct cavm_nixx_af_sdp_tx_fifo_status_s {
+		u64 count                            : 12;
+		u64 reserved_12_63                   : 52;
+	} s;
+	/* struct cavm_nixx_af_sdp_tx_fifo_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SDP_TX_FIFO_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SDP_TX_FIFO_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x650;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_seb_active_cycles_pc#
+ *
+ * NIX AF Active Cycles Register
+ * These registers are indexed by the conditional clock domain number.
+ */
+union cavm_nixx_af_seb_active_cycles_pcx {
+	u64 u;
+	struct cavm_nixx_af_seb_active_cycles_pcx_s {
+		u64 act_cyc                          : 64;
+	} s;
+	/* struct cavm_nixx_af_seb_active_cycles_pcx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SEB_ACTIVE_CYCLES_PCX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SEB_ACTIVE_CYCLES_PCX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 2))
+		return 0x6c0 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_seb_eco
+ *
+ * INTERNAL: SEB AF ECO Register
+ *
+ * Internal:
+ * TODO - When CSI ECO register is added, switch this one to inherit from it.
+ */
+union cavm_nixx_af_seb_eco {
+	u64 u;
+	struct cavm_nixx_af_seb_eco_s {
+		u64 eco_rw                           : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_seb_eco_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SEB_ECO
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SEB_ECO
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x600;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_seb_test_bp
+ *
+ * NIX AF SEB Backpressure Test Register
+ */
+union cavm_nixx_af_seb_test_bp {
+	u64 u;
+	struct cavm_nixx_af_seb_test_bp_s {
+		u64 introp                           : 1;
+		u64 ndx                              : 1;
+		u64 npx                              : 1;
+		u64 sso_lwa                          : 1;
+		u64 sqm_lf_err_dbg                   : 1;
+		u64 reserved_5_63                    : 59;
+	} s;
+	/* struct cavm_nixx_af_seb_test_bp_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SEB_TEST_BP
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SEB_TEST_BP
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x610;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_seb_test_bp_pipe#
+ *
+ * NIX AF SEB Pipe Backpressure Test Registers
+ */
+union cavm_nixx_af_seb_test_bp_pipex {
+	u64 u;
+	struct cavm_nixx_af_seb_test_bp_pipex_s {
+		u64 jump                             : 1;
+		u64 jump_ndx_req                     : 1;
+		u64 jump_npx_rtn                     : 1;
+		u64 sg_cp                            : 1;
+		u64 sg_dp                            : 1;
+		u64 sg_ndx_req                       : 1;
+		u64 sg_npx_rtn                       : 1;
+		u64 mod1_cp                          : 1;
+		u64 mod1_dp                          : 1;
+		u64 query_cp                         : 1;
+		u64 query_dp                         : 1;
+		u64 query_npc_hdr                    : 1;
+		u64 mod2_cp                          : 1;
+		u64 mod2_dp                          : 1;
+		u64 check_cp                         : 1;
+		u64 check_dp                         : 1;
+		u64 tail_introp                      : 1;
+		u64 tail_lf_err_dbg                  : 1;
+		u64 tail_linkcdt                     : 1;
+		u64 tail_ndx_wr                      : 1;
+		u64 tail_ssoif                       : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_nixx_af_seb_test_bp_pipex_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SEB_TEST_BP_PIPEX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SEB_TEST_BP_PIPEX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 2))
+		return 0x620 + 8 * ((a) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_smq#_cfg
+ *
+ * NIX AF SQM PSE Queue Configuration Registers
+ */
+union cavm_nixx_af_smqx_cfg {
+	u64 u;
+	struct cavm_nixx_af_smqx_cfg_s {
+		u64 minlen                           : 7;
+		u64 desc_shp_ctl_dis                 : 1;
+		u64 maxlen                           : 16;
+		u64 lf                               : 7;
+		u64 reserved_31_35                   : 5;
+		u64 max_vtag_ins                     : 3;
+		u64 rr_minlen                        : 9;
+		u64 express                          : 1;
+		u64 flush                            : 1;
+		u64 enq_xoff                         : 1;
+		u64 pri_thr                          : 6;
+		u64 reserved_57_63                   : 7;
+	} s;
+	/* struct cavm_nixx_af_smqx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SMQX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SMQX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x700 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_smq#_head
+ *
+ * NIX AF SQM SMQ Head Register
+ * These registers track the head of the SMQ linked list.
+ */
+union cavm_nixx_af_smqx_head {
+	u64 u;
+	struct cavm_nixx_af_smqx_head_s {
+		u64 sq_idx                           : 20;
+		u64 valid                            : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_nixx_af_smqx_head_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SMQX_HEAD(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SMQX_HEAD(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x710 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_smq#_nxt_head
+ *
+ * NIX AF SQM SMQ Next Head Register
+ * These registers track the next head of the SMQ linked list.
+ */
+union cavm_nixx_af_smqx_nxt_head {
+	u64 u;
+	struct cavm_nixx_af_smqx_nxt_head_s {
+		u64 sq_idx                           : 20;
+		u64 valid                            : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_nixx_af_smqx_nxt_head_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SMQX_NXT_HEAD(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SMQX_NXT_HEAD(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x740 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_smq#_status
+ *
+ * NIX AF SQM SMQ Status Register
+ * These registers track the status of the SMQ fifo.
+ */
+union cavm_nixx_af_smqx_status {
+	u64 u;
+	struct cavm_nixx_af_smqx_status_s {
+		u64 level                            : 7;
+		u64 reserved_7_63                    : 57;
+	} s;
+	/* struct cavm_nixx_af_smqx_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SMQX_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SMQX_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x730 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_smq#_tail
+ *
+ * NIX AF SQM SMQ Head Register
+ * These registers track the tail of SMQ linked list.
+ */
+union cavm_nixx_af_smqx_tail {
+	u64 u;
+	struct cavm_nixx_af_smqx_tail_s {
+		u64 sq_idx                           : 20;
+		u64 valid                            : 1;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_nixx_af_smqx_tail_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SMQX_TAIL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SMQX_TAIL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x720 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sq_const
+ *
+ * NIX AF SQ Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_sq_const {
+	u64 u;
+	struct cavm_nixx_af_sq_const_s {
+		u64 queues_per_lf                    : 24;
+		u64 smq_depth                        : 10;
+		u64 sqb_size                         : 16;
+		u64 reserved_50_63                   : 14;
+	} s;
+	/* struct cavm_nixx_af_sq_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SQ_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SQ_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x40;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sqm_active_cycles_pc
+ *
+ * NIX AF SQM Active Cycles Register
+ * These registers are indexed by the conditional clock domain number.
+ */
+union cavm_nixx_af_sqm_active_cycles_pc {
+	u64 u;
+	struct cavm_nixx_af_sqm_active_cycles_pc_s {
+		u64 act_cyc                          : 64;
+	} s;
+	/* struct cavm_nixx_af_sqm_active_cycles_pc_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SQM_ACTIVE_CYCLES_PC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SQM_ACTIVE_CYCLES_PC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x770;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sqm_bp_test0
+ *
+ * INTERNAL: NIX AF SQM Backpressure Test Register
+ */
+union cavm_nixx_af_sqm_bp_test0 {
+	u64 u;
+	struct cavm_nixx_af_sqm_bp_test0_s {
+		u64 lfsr_freq                        : 12;
+		u64 reserved_12_15                   : 4;
+		u64 bp_cfg                           : 8;
+		u64 reserved_24_59                   : 36;
+		u64 enable                           : 4;
+	} s;
+	/* struct cavm_nixx_af_sqm_bp_test0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SQM_BP_TEST0
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SQM_BP_TEST0
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x760;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_sqm_dbg_ctl_status
+ *
+ * INTERNAL: NIX AF SQM Debug Register
+ *
+ * This register is for SQM diagnostic use only.
+ */
+union cavm_nixx_af_sqm_dbg_ctl_status {
+	u64 u;
+	struct cavm_nixx_af_sqm_dbg_ctl_status_s {
+		u64 tm1                              : 8;
+		u64 tm2                              : 1;
+		u64 tm3                              : 4;
+		u64 tm4                              : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_nixx_af_sqm_dbg_ctl_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_SQM_DBG_CTL_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_SQM_DBG_CTL_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x750;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_status
+ *
+ * NIX AF General Status Register
+ */
+union cavm_nixx_af_status {
+	u64 u;
+	struct cavm_nixx_af_status_s {
+		u64 blk_busy                         : 10;
+		u64 calibrate_done                   : 1;
+		u64 reserved_11_15                   : 5;
+		u64 calibrate_status                 : 15;
+		u64 reserved_31_63                   : 33;
+	} s;
+	/* struct cavm_nixx_af_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tcp_timer
+ *
+ * NIX TCP Timer Register
+ */
+union cavm_nixx_af_tcp_timer {
+	u64 u;
+	struct cavm_nixx_af_tcp_timer_s {
+		u64 dur_counter                      : 16;
+		u64 lf_counter                       : 8;
+		u64 reserved_24_31                   : 8;
+		u64 duration                         : 16;
+		u64 reserved_48_62                   : 15;
+		u64 ena                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tcp_timer_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TCP_TIMER
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TCP_TIMER
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x1e0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_cir
+ *
+ * NIX AF Transmit Level 1 Committed Information Rate Register
+ */
+union cavm_nixx_af_tl1x_cir {
+	u64 u;
+	struct cavm_nixx_af_tl1x_cir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl1x_cir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_CIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_CIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc20 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_dropped_bytes
+ *
+ * NIX AF Transmit Level 1 Dropped Bytes Registers
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN_BYTES.
+ */
+union cavm_nixx_af_tl1x_dropped_bytes {
+	u64 u;
+	struct cavm_nixx_af_tl1x_dropped_bytes_s {
+		u64 count                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_tl1x_dropped_bytes_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_DROPPED_BYTES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_DROPPED_BYTES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd30 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_dropped_packets
+ *
+ * NIX AF Transmit Level 1 Dropped Packets Registers
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN_PACKETS.
+ */
+union cavm_nixx_af_tl1x_dropped_packets {
+	u64 u;
+	struct cavm_nixx_af_tl1x_dropped_packets_s {
+		u64 count                            : 40;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_tl1x_dropped_packets_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_DROPPED_PACKETS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_DROPPED_PACKETS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd20 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_green
+ *
+ * INTERNAL: NIX Transmit Level 1 Green State Debug Register
+ */
+union cavm_nixx_af_tl1x_green {
+	u64 u;
+	struct cavm_nixx_af_tl1x_green_s {
+		u64 tail                             : 8;
+		u64 reserved_8_9                     : 2;
+		u64 head                             : 8;
+		u64 reserved_18_19                   : 2;
+		u64 active_vec                       : 20;
+		u64 rr_active                        : 1;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl1x_green_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_GREEN(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_GREEN(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc90 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_green_bytes
+ *
+ * NIX AF Transmit Level 1 Green Sent Bytes Registers
+ */
+union cavm_nixx_af_tl1x_green_bytes {
+	u64 u;
+	struct cavm_nixx_af_tl1x_green_bytes_s {
+		u64 count                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_tl1x_green_bytes_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_GREEN_BYTES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_GREEN_BYTES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd90 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_green_packets
+ *
+ * NIX AF Transmit Level 1 Green Sent Packets Registers
+ */
+union cavm_nixx_af_tl1x_green_packets {
+	u64 u;
+	struct cavm_nixx_af_tl1x_green_packets_s {
+		u64 count                            : 40;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_tl1x_green_packets_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_GREEN_PACKETS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_GREEN_PACKETS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd80 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug0
+ *
+ * NIX AF Transmit Level 1 Meta Descriptor Debug 0 Registers
+ * NIX_AF_TL1()_MD_DEBUG0, NIX_AF_TL1()_MD_DEBUG1, NIX_AF_TL1()_MD_DEBUG2 and
+ * NIX_AF_TL1()_MD_DEBUG3 provide access to the TLn queue meta descriptor. A TLn
+ * queue can hold up to two packet meta descriptors (PMD) and one flush meta
+ * descriptor (FMD):
+ * * PMD0 state is accessed with [PMD0_VLD], [PMD0_LENGTH] and NIX_AF_TL1()_MD_DEBUG1.
+ * * PMD1 is accessed with [PMD1_VLD], [PMD1_LENGTH] and NIX_AF_TL1()_MD_DEBUG2.
+ * * FMD is accessed with NIX_AF_TL1()_MD_DEBUG3.
+ */
+union cavm_nixx_af_tl1x_md_debug0 {
+	u64 u;
+	struct cavm_nixx_af_tl1x_md_debug0_s {
+		u64 pmd0_length                      : 16;
+		u64 pmd1_length                      : 16;
+		u64 pmd0_vld                         : 1;
+		u64 pmd1_vld                         : 1;
+		u64 reserved_34_45                   : 12;
+		u64 drain_pri                        : 1;
+		u64 drain                            : 1;
+		u64 c_con                            : 1;
+		u64 p_con                            : 1;
+		u64 reserved_50_51                   : 2;
+		u64 child                            : 10;
+		u64 reserved_62                      : 1;
+		u64 pmd_count                        : 1;
+	} s;
+	/* struct cavm_nixx_af_tl1x_md_debug0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xcc0 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug1
+ *
+ * NIX AF Transmit Level 1 Meta Descriptor Debug 1 Registers
+ * Packet meta descriptor 0 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl1x_md_debug1 {
+	u64 u;
+	struct cavm_nixx_af_tl1x_md_debug1_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl1x_md_debug1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xcc8 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug2
+ *
+ * NIX AF Transmit Level 1 Meta Descriptor Debug 2 Registers
+ * Packet meta descriptor 1 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl1x_md_debug2 {
+	u64 u;
+	struct cavm_nixx_af_tl1x_md_debug2_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl1x_md_debug2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xcd0 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug3
+ *
+ * NIX AF Transmit Level 1 Meta Descriptor Debug 3 Registers
+ * Flush meta descriptor debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl1x_md_debug3 {
+	u64 u;
+	struct cavm_nixx_af_tl1x_md_debug3_s {
+		u64 reserved_0_36                    : 37;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl1x_md_debug3_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_MD_DEBUG3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xcd8 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_red
+ *
+ * INTERNAL: NIX Transmit Level 1 Red State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL1()_YELLOW.
+ */
+union cavm_nixx_af_tl1x_red {
+	u64 u;
+	struct cavm_nixx_af_tl1x_red_s {
+		u64 tail                             : 8;
+		u64 reserved_8_9                     : 2;
+		u64 head                             : 8;
+		u64 reserved_18_63                   : 46;
+	} s;
+	/* struct cavm_nixx_af_tl1x_red_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_RED(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_RED(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xcb0 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_red_bytes
+ *
+ * NIX AF Transmit Level 1 Red Sent Bytes Registers
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN_BYTES.
+ */
+union cavm_nixx_af_tl1x_red_bytes {
+	u64 u;
+	struct cavm_nixx_af_tl1x_red_bytes_s {
+		u64 count                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_tl1x_red_bytes_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_RED_BYTES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_RED_BYTES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd50 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_red_packets
+ *
+ * NIX AF Transmit Level 1 Red Sent Packets Registers
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN_PACKETS.
+ */
+union cavm_nixx_af_tl1x_red_packets {
+	u64 u;
+	struct cavm_nixx_af_tl1x_red_packets_s {
+		u64 count                            : 40;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_tl1x_red_packets_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_RED_PACKETS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_RED_PACKETS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd40 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_schedule
+ *
+ * NIX AF Transmit Level 1 Scheduling Control Register
+ */
+union cavm_nixx_af_tl1x_schedule {
+	u64 u;
+	struct cavm_nixx_af_tl1x_schedule_s {
+		u64 rr_quantum                       : 24;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_tl1x_schedule_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_SCHEDULE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_SCHEDULE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc00 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_shape
+ *
+ * NIX AF Transmit Level 1 Shaping Control Register
+ */
+union cavm_nixx_af_tl1x_shape {
+	u64 u;
+	struct cavm_nixx_af_tl1x_shape_s {
+		u64 adjust                           : 9;
+		u64 reserved_9_23                    : 15;
+		u64 length_disable                   : 1;
+		u64 reserved_25_63                   : 39;
+	} s;
+	struct cavm_nixx_af_tl1x_shape_cn {
+		u64 adjust                           : 9;
+		u64 reserved_9_17                    : 9;
+		u64 reserved_18_23                   : 6;
+		u64 length_disable                   : 1;
+		u64 reserved_25_63                   : 39;
+	} cn;
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_SHAPE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_SHAPE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc10 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_shape_state
+ *
+ * NIX AF Transmit Level 1 Shape State Register
+ * This register must not be written during normal operation.
+ */
+union cavm_nixx_af_tl1x_shape_state {
+	u64 u;
+	struct cavm_nixx_af_tl1x_shape_state_s {
+		u64 cir_accum                        : 26;
+		u64 reserved_26_51                   : 26;
+		u64 color                            : 1;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_nixx_af_tl1x_shape_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_SHAPE_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_SHAPE_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc50 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_sw_xoff
+ *
+ * NIX AF Transmit Level 1 Software Controlled XOFF Registers
+ */
+union cavm_nixx_af_tl1x_sw_xoff {
+	u64 u;
+	struct cavm_nixx_af_tl1x_sw_xoff_s {
+		u64 xoff                             : 1;
+		u64 drain                            : 1;
+		u64 reserved_2                       : 1;
+		u64 drain_irq                        : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_tl1x_sw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_SW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_SW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc70 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_topology
+ *
+ * NIX AF Transmit Level 1 Topology Registers
+ */
+union cavm_nixx_af_tl1x_topology {
+	u64 u;
+	struct cavm_nixx_af_tl1x_topology_s {
+		u64 reserved_0                       : 1;
+		u64 rr_prio                          : 4;
+		u64 reserved_5_31                    : 27;
+		u64 prio_anchor                      : 8;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_tl1x_topology_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_TOPOLOGY(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_TOPOLOGY(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xc80 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_yellow
+ *
+ * INTERNAL: NIX Transmit Level 1 Yellow State Debug Register
+ */
+union cavm_nixx_af_tl1x_yellow {
+	u64 u;
+	struct cavm_nixx_af_tl1x_yellow_s {
+		u64 tail                             : 8;
+		u64 reserved_8_9                     : 2;
+		u64 head                             : 8;
+		u64 reserved_18_63                   : 46;
+	} s;
+	/* struct cavm_nixx_af_tl1x_yellow_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_YELLOW(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_YELLOW(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xca0 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_yellow_bytes
+ *
+ * NIX AF Transmit Level 1 Yellow Sent Bytes Registers
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN_BYTES.
+ */
+union cavm_nixx_af_tl1x_yellow_bytes {
+	u64 u;
+	struct cavm_nixx_af_tl1x_yellow_bytes_s {
+		u64 count                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_af_tl1x_yellow_bytes_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_YELLOW_BYTES(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_YELLOW_BYTES(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd70 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1#_yellow_packets
+ *
+ * NIX AF Transmit Level 1 Yellow Sent Packets Registers
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN_PACKETS.
+ */
+union cavm_nixx_af_tl1x_yellow_packets {
+	u64 u;
+	struct cavm_nixx_af_tl1x_yellow_packets_s {
+		u64 count                            : 40;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_tl1x_yellow_packets_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1X_YELLOW_PACKETS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1X_YELLOW_PACKETS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 27))
+		return 0xd60 + 0x10000 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl1_const
+ *
+ * NIX AF Transmit Level 1 Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_tl1_const {
+	u64 u;
+	struct cavm_nixx_af_tl1_const_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_tl1_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL1_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL1_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x70;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_cir
+ *
+ * NIX AF Transmit Level 2 Committed Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_tl2x_cir {
+	u64 u;
+	struct cavm_nixx_af_tl2x_cir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl2x_cir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_CIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_CIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe20 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_green
+ *
+ * INTERNAL: NIX Transmit Level 2 Green State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL1()_GREEN.
+ */
+union cavm_nixx_af_tl2x_green {
+	u64 u;
+	struct cavm_nixx_af_tl2x_green_s {
+		u64 tail                             : 8;
+		u64 reserved_8_9                     : 2;
+		u64 head                             : 8;
+		u64 reserved_18_19                   : 2;
+		u64 active_vec                       : 20;
+		u64 rr_active                        : 1;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl2x_green_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_GREEN(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_GREEN(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe90 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug0
+ *
+ * NIX AF Transmit Level 2 Meta Descriptor Debug 0 Registers
+ * See NIX_AF_TL1()_MD_DEBUG0
+ */
+union cavm_nixx_af_tl2x_md_debug0 {
+	u64 u;
+	struct cavm_nixx_af_tl2x_md_debug0_s {
+		u64 pmd0_length                      : 16;
+		u64 pmd1_length                      : 16;
+		u64 pmd0_vld                         : 1;
+		u64 pmd1_vld                         : 1;
+		u64 reserved_34_45                   : 12;
+		u64 drain_pri                        : 1;
+		u64 drain                            : 1;
+		u64 c_con                            : 1;
+		u64 p_con                            : 1;
+		u64 reserved_50_51                   : 2;
+		u64 child                            : 10;
+		u64 reserved_62                      : 1;
+		u64 pmd_count                        : 1;
+	} s;
+	/* struct cavm_nixx_af_tl2x_md_debug0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xec0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug1
+ *
+ * NIX AF Transmit Level 2 Meta Descriptor Debug 1 Registers
+ * Packet meta descriptor 0 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl2x_md_debug1 {
+	u64 u;
+	struct cavm_nixx_af_tl2x_md_debug1_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl2x_md_debug1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xec8 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug2
+ *
+ * NIX AF Transmit Level 2 Meta Descriptor Debug 2 Registers
+ * Packet meta descriptor 1 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl2x_md_debug2 {
+	u64 u;
+	struct cavm_nixx_af_tl2x_md_debug2_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl2x_md_debug2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xed0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug3
+ *
+ * NIX AF Transmit Level 2 Meta Descriptor Debug 3 Registers
+ * Flush meta descriptor debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl2x_md_debug3 {
+	u64 u;
+	struct cavm_nixx_af_tl2x_md_debug3_s {
+		u64 reserved_0_36                    : 37;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl2x_md_debug3_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_MD_DEBUG3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xed8 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_parent
+ *
+ * NIX AF Transmit Level 2 Parent Registers
+ */
+union cavm_nixx_af_tl2x_parent {
+	u64 u;
+	struct cavm_nixx_af_tl2x_parent_s {
+		u64 reserved_0_15                    : 16;
+		u64 parent                           : 5;
+		u64 reserved_21_63                   : 43;
+	} s;
+	/* struct cavm_nixx_af_tl2x_parent_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_PARENT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_PARENT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe88 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_pir
+ *
+ * NIX AF Transmit Level 2 Peak Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_tl2x_pir {
+	u64 u;
+	struct cavm_nixx_af_tl2x_pir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl2x_pir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_PIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_PIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe30 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_pointers
+ *
+ * INTERNAL: NIX Transmit Level 2 Linked List Pointers Debug Register
+ */
+union cavm_nixx_af_tl2x_pointers {
+	u64 u;
+	struct cavm_nixx_af_tl2x_pointers_s {
+		u64 next                             : 8;
+		u64 reserved_8_15                    : 8;
+		u64 prev                             : 8;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_tl2x_pointers_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_POINTERS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_POINTERS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe60 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_red
+ *
+ * INTERNAL: NIX Transmit Level 2 Red State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL1()_RED.
+ */
+union cavm_nixx_af_tl2x_red {
+	u64 u;
+	struct cavm_nixx_af_tl2x_red_s {
+		u64 tail                             : 8;
+		u64 reserved_8_9                     : 2;
+		u64 head                             : 8;
+		u64 reserved_18_63                   : 46;
+	} s;
+	/* struct cavm_nixx_af_tl2x_red_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_RED(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_RED(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xeb0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_sched_state
+ *
+ * NIX AF Transmit Level 2 Scheduling Control State Registers
+ */
+union cavm_nixx_af_tl2x_sched_state {
+	u64 u;
+	struct cavm_nixx_af_tl2x_sched_state_s {
+		u64 rr_count                         : 25;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_tl2x_sched_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_SCHED_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_SCHED_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe40 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_schedule
+ *
+ * NIX AF Transmit Level 2 Scheduling Control Registers
+ */
+union cavm_nixx_af_tl2x_schedule {
+	u64 u;
+	struct cavm_nixx_af_tl2x_schedule_s {
+		u64 rr_quantum                       : 24;
+		u64 prio                             : 4;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_nixx_af_tl2x_schedule_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_SCHEDULE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_SCHEDULE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe00 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_shape
+ *
+ * NIX AF Transmit Level 2 Shaping Control Registers
+ */
+union cavm_nixx_af_tl2x_shape {
+	u64 u;
+	struct cavm_nixx_af_tl2x_shape_s {
+		u64 adjust                           : 9;
+		u64 red_algo                         : 2;
+		u64 red_disable                      : 1;
+		u64 yellow_disable                   : 1;
+		u64 reserved_13_23                   : 11;
+		u64 length_disable                   : 1;
+		u64 schedule_list                    : 2;
+		u64 reserved_27_63                   : 37;
+	} s;
+	/* struct cavm_nixx_af_tl2x_shape_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_SHAPE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_SHAPE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe10 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_shape_state
+ *
+ * NIX AF Transmit Level 2 Shape State Registers
+ * This register must not be written during normal operation.
+ */
+union cavm_nixx_af_tl2x_shape_state {
+	u64 u;
+	struct cavm_nixx_af_tl2x_shape_state_s {
+		u64 cir_accum                        : 26;
+		u64 pir_accum                        : 26;
+		u64 color                            : 2;
+		u64 reserved_54_63                   : 10;
+	} s;
+	/* struct cavm_nixx_af_tl2x_shape_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_SHAPE_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_SHAPE_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe50 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_sw_xoff
+ *
+ * NIX AF Transmit Level 2 Software Controlled XOFF Registers
+ * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF.
+ */
+union cavm_nixx_af_tl2x_sw_xoff {
+	u64 u;
+	struct cavm_nixx_af_tl2x_sw_xoff_s {
+		u64 xoff                             : 1;
+		u64 drain                            : 1;
+		u64 reserved_2                       : 1;
+		u64 drain_irq                        : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_tl2x_sw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_SW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_SW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe70 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_topology
+ *
+ * NIX AF Transmit Level 2 Topology Registers
+ */
+union cavm_nixx_af_tl2x_topology {
+	u64 u;
+	struct cavm_nixx_af_tl2x_topology_s {
+		u64 reserved_0                       : 1;
+		u64 rr_prio                          : 4;
+		u64 reserved_5_31                    : 27;
+		u64 prio_anchor                      : 8;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_nixx_af_tl2x_topology_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_TOPOLOGY(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_TOPOLOGY(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xe80 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2#_yellow
+ *
+ * INTERNAL: NIX Transmit Level 2 Yellow State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL1()_YELLOW.
+ */
+union cavm_nixx_af_tl2x_yellow {
+	u64 u;
+	struct cavm_nixx_af_tl2x_yellow_s {
+		u64 tail                             : 8;
+		u64 reserved_8_9                     : 2;
+		u64 head                             : 8;
+		u64 reserved_18_63                   : 46;
+	} s;
+	/* struct cavm_nixx_af_tl2x_yellow_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2X_YELLOW(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2X_YELLOW(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0xea0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl2_const
+ *
+ * NIX AF Transmit Level 2 Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_tl2_const {
+	u64 u;
+	struct cavm_nixx_af_tl2_const_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_tl2_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL2_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL2_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x78;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_cir
+ *
+ * NIX AF Transmit Level 3 Committed Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_tl3x_cir {
+	u64 u;
+	struct cavm_nixx_af_tl3x_cir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl3x_cir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_CIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_CIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1020 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_green
+ *
+ * INTERNAL: NIX Transmit Level 3 Green State Debug Register
+ */
+union cavm_nixx_af_tl3x_green {
+	u64 u;
+	struct cavm_nixx_af_tl3x_green_s {
+		u64 tail                             : 9;
+		u64 reserved_9                       : 1;
+		u64 head                             : 9;
+		u64 reserved_19                      : 1;
+		u64 active_vec                       : 20;
+		u64 rr_active                        : 1;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl3x_green_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_GREEN(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_GREEN(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1090 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug0
+ *
+ * NIX AF Transmit Level 3 Meta Descriptor Debug 0 Registers
+ * See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl3x_md_debug0 {
+	u64 u;
+	struct cavm_nixx_af_tl3x_md_debug0_s {
+		u64 pmd0_length                      : 16;
+		u64 pmd1_length                      : 16;
+		u64 pmd0_vld                         : 1;
+		u64 pmd1_vld                         : 1;
+		u64 reserved_34_45                   : 12;
+		u64 drain_pri                        : 1;
+		u64 drain                            : 1;
+		u64 c_con                            : 1;
+		u64 p_con                            : 1;
+		u64 reserved_50_51                   : 2;
+		u64 child                            : 10;
+		u64 reserved_62                      : 1;
+		u64 pmd_count                        : 1;
+	} s;
+	/* struct cavm_nixx_af_tl3x_md_debug0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x10c0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug1
+ *
+ * NIX AF Transmit Level 3 Meta Descriptor Debug 1 Registers
+ * Packet meta descriptor 0 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl3x_md_debug1 {
+	u64 u;
+	struct cavm_nixx_af_tl3x_md_debug1_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl3x_md_debug1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x10c8 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug2
+ *
+ * NIX AF Transmit Level 3 Meta Descriptor Debug 2 Registers
+ * Packet meta descriptor 1 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl3x_md_debug2 {
+	u64 u;
+	struct cavm_nixx_af_tl3x_md_debug2_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl3x_md_debug2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x10d0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug3
+ *
+ * NIX AF Transmit Level 3 Meta Descriptor Debug 3 Registers
+ * Flush meta descriptor debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl3x_md_debug3 {
+	u64 u;
+	struct cavm_nixx_af_tl3x_md_debug3_s {
+		u64 reserved_0_36                    : 37;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl3x_md_debug3_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_MD_DEBUG3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x10d8 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_parent
+ *
+ * NIX AF Transmit Level 3 Parent Registers
+ */
+union cavm_nixx_af_tl3x_parent {
+	u64 u;
+	struct cavm_nixx_af_tl3x_parent_s {
+		u64 reserved_0_15                    : 16;
+		u64 parent                           : 8;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_tl3x_parent_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_PARENT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_PARENT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1088 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_pir
+ *
+ * NIX AF Transmit Level 3 Peak Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_tl3x_pir {
+	u64 u;
+	struct cavm_nixx_af_tl3x_pir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl3x_pir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_PIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_PIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1030 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_pointers
+ *
+ * INTERNAL: NIX Transmit Level 3 Linked List Pointers Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL2()_POINTERS.
+ */
+union cavm_nixx_af_tl3x_pointers {
+	u64 u;
+	struct cavm_nixx_af_tl3x_pointers_s {
+		u64 next                             : 8;
+		u64 reserved_8_15                    : 8;
+		u64 prev                             : 8;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_tl3x_pointers_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_POINTERS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_POINTERS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1060 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_red
+ *
+ * INTERNAL: NIX Transmit Level 3 Red State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL3()_YELLOW.
+ */
+union cavm_nixx_af_tl3x_red {
+	u64 u;
+	struct cavm_nixx_af_tl3x_red_s {
+		u64 tail                             : 9;
+		u64 reserved_9                       : 1;
+		u64 head                             : 9;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_tl3x_red_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_RED(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_RED(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x10b0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_sched_state
+ *
+ * NIX AF Transmit Level 3 Scheduling Control State Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SCHED_STATE.
+ */
+union cavm_nixx_af_tl3x_sched_state {
+	u64 u;
+	struct cavm_nixx_af_tl3x_sched_state_s {
+		u64 rr_count                         : 25;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_tl3x_sched_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_SCHED_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_SCHED_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1040 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_schedule
+ *
+ * NIX AF Transmit Level 3 Scheduling Control Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SCHEDULE.
+ */
+union cavm_nixx_af_tl3x_schedule {
+	u64 u;
+	struct cavm_nixx_af_tl3x_schedule_s {
+		u64 rr_quantum                       : 24;
+		u64 prio                             : 4;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_nixx_af_tl3x_schedule_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_SCHEDULE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_SCHEDULE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1000 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_shape
+ *
+ * NIX AF Transmit Level 3 Shaping Control Registers
+ */
+union cavm_nixx_af_tl3x_shape {
+	u64 u;
+	struct cavm_nixx_af_tl3x_shape_s {
+		u64 adjust                           : 9;
+		u64 red_algo                         : 2;
+		u64 red_disable                      : 1;
+		u64 yellow_disable                   : 1;
+		u64 reserved_13_23                   : 11;
+		u64 length_disable                   : 1;
+		u64 schedule_list                    : 2;
+		u64 reserved_27_63                   : 37;
+	} s;
+	/* struct cavm_nixx_af_tl3x_shape_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_SHAPE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_SHAPE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1010 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_shape_state
+ *
+ * NIX AF Transmit Level 3 Shaping State Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SHAPE_STATE.
+ * This register must not be written during normal operation.
+ */
+union cavm_nixx_af_tl3x_shape_state {
+	u64 u;
+	struct cavm_nixx_af_tl3x_shape_state_s {
+		u64 cir_accum                        : 26;
+		u64 pir_accum                        : 26;
+		u64 color                            : 2;
+		u64 reserved_54_63                   : 10;
+	} s;
+	/* struct cavm_nixx_af_tl3x_shape_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_SHAPE_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_SHAPE_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1050 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_sw_xoff
+ *
+ * NIX AF Transmit Level 3 Software Controlled XOFF Registers
+ * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF
+ */
+union cavm_nixx_af_tl3x_sw_xoff {
+	u64 u;
+	struct cavm_nixx_af_tl3x_sw_xoff_s {
+		u64 xoff                             : 1;
+		u64 drain                            : 1;
+		u64 reserved_2                       : 1;
+		u64 drain_irq                        : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_tl3x_sw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_SW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_SW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1070 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_topology
+ *
+ * NIX AF Transmit Level 3 Topology Registers
+ */
+union cavm_nixx_af_tl3x_topology {
+	u64 u;
+	struct cavm_nixx_af_tl3x_topology_s {
+		u64 reserved_0                       : 1;
+		u64 rr_prio                          : 4;
+		u64 reserved_5_31                    : 27;
+		u64 prio_anchor                      : 9;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl3x_topology_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_TOPOLOGY(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_TOPOLOGY(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1080 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3#_yellow
+ *
+ * INTERNAL: NIX Transmit Level 3 Yellow State Debug Register
+ */
+union cavm_nixx_af_tl3x_yellow {
+	u64 u;
+	struct cavm_nixx_af_tl3x_yellow_s {
+		u64 tail                             : 9;
+		u64 reserved_9                       : 1;
+		u64 head                             : 9;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_tl3x_yellow_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3X_YELLOW(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3X_YELLOW(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x10a0 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3_const
+ *
+ * NIX AF Transmit Level 3 Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_tl3_const {
+	u64 u;
+	struct cavm_nixx_af_tl3_const_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_tl3_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x80;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3_tl2#_bp_status
+ *
+ * NIX AF Transmit Level 3/2 Backpressure Status Registers
+ */
+union cavm_nixx_af_tl3_tl2x_bp_status {
+	u64 u;
+	struct cavm_nixx_af_tl3_tl2x_bp_status_s {
+		u64 hw_xoff                          : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_tl3_tl2x_bp_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3_TL2X_BP_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3_TL2X_BP_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1610 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3_tl2#_cfg
+ *
+ * NIX AF Transmit Level 3/2 Configuration Registers
+ */
+union cavm_nixx_af_tl3_tl2x_cfg {
+	u64 u;
+	struct cavm_nixx_af_tl3_tl2x_cfg_s {
+		u64 express                          : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_tl3_tl2x_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3_TL2X_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3_TL2X_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x1600 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl3_tl2#_link#_cfg
+ *
+ * NIX AF Transmit Level 3/2 Link Configuration Registers
+ * These registers specify the links and associated channels that a given TL3 or
+ * TL2 queue (depending on NIX_AF_PSE_CHANNEL_LEVEL[BP_LEVEL]) can transmit on.
+ * Each TL3/TL2 queue can be enabled to transmit on and be backpressured by one or
+ * more links and associated channels.
+ */
+union cavm_nixx_af_tl3_tl2x_linkx_cfg {
+	u64 u;
+	struct cavm_nixx_af_tl3_tl2x_linkx_cfg_s {
+		u64 relchan                          : 8;
+		u64 reserved_8_11                    : 4;
+		u64 ena                              : 1;
+		u64 bp_ena                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_nixx_af_tl3_tl2x_linkx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL3_TL2X_LINKX_CFG(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL3_TL2X_LINKX_CFG(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 255) && (b <= 12)))
+		return 0x1700 + 0x10000 * ((a) & 0xff) + 8 * ((b) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_bp_status
+ *
+ * NIX AF Transmit Level 4 Backpressure Status Registers
+ */
+union cavm_nixx_af_tl4x_bp_status {
+	u64 u;
+	struct cavm_nixx_af_tl4x_bp_status_s {
+		u64 hw_xoff                          : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_af_tl4x_bp_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_BP_STATUS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_BP_STATUS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0xb00 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_cir
+ *
+ * NIX AF Transmit Level 4 Committed Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_tl4x_cir {
+	u64 u;
+	struct cavm_nixx_af_tl4x_cir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl4x_cir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_CIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_CIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1220 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_green
+ *
+ * INTERNAL: NIX Transmit Level 4 Green State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL3()_GREEN.
+ */
+union cavm_nixx_af_tl4x_green {
+	u64 u;
+	struct cavm_nixx_af_tl4x_green_s {
+		u64 tail                             : 9;
+		u64 reserved_9                       : 1;
+		u64 head                             : 9;
+		u64 reserved_19                      : 1;
+		u64 active_vec                       : 20;
+		u64 rr_active                        : 1;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl4x_green_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_GREEN(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_GREEN(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1290 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug0
+ *
+ * NIX AF Transmit Level 4 Meta Descriptor Debug 0 Registers
+ * See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl4x_md_debug0 {
+	u64 u;
+	struct cavm_nixx_af_tl4x_md_debug0_s {
+		u64 pmd0_length                      : 16;
+		u64 pmd1_length                      : 16;
+		u64 pmd0_vld                         : 1;
+		u64 pmd1_vld                         : 1;
+		u64 reserved_34_45                   : 12;
+		u64 drain_pri                        : 1;
+		u64 drain                            : 1;
+		u64 c_con                            : 1;
+		u64 p_con                            : 1;
+		u64 reserved_50_51                   : 2;
+		u64 child                            : 10;
+		u64 reserved_62                      : 1;
+		u64 pmd_count                        : 1;
+	} s;
+	/* struct cavm_nixx_af_tl4x_md_debug0_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x12c0 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug1
+ *
+ * NIX AF Transmit Level 4 Meta Descriptor Debug 1 Registers
+ * Packet meta descriptor 0 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl4x_md_debug1 {
+	u64 u;
+	struct cavm_nixx_af_tl4x_md_debug1_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl4x_md_debug1_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG1(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG1(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x12c8 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug2
+ *
+ * NIX AF Transmit Level 4 Meta Descriptor Debug 2 Registers
+ * Packet meta descriptor 1 debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl4x_md_debug2 {
+	u64 u;
+	struct cavm_nixx_af_tl4x_md_debug2_s {
+		u64 reserved_0_5                     : 6;
+		u64 red_algo_override                : 2;
+		u64 cir_dis                          : 1;
+		u64 pir_dis                          : 1;
+		u64 adjust                           : 9;
+		u64 uid                              : 4;
+		u64 drain                            : 1;
+		u64 bubble                           : 1;
+		u64 color                            : 2;
+		u64 pse_pkt_id                       : 9;
+		u64 reserved_36                      : 1;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl4x_md_debug2_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG2(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG2(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x12d0 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug3
+ *
+ * NIX AF Transmit Level 4 Meta Descriptor Debug 3 Registers
+ * Flush meta descriptor debug. See NIX_AF_TL1()_MD_DEBUG0.
+ */
+union cavm_nixx_af_tl4x_md_debug3 {
+	u64 u;
+	struct cavm_nixx_af_tl4x_md_debug3_s {
+		u64 reserved_0_36                    : 37;
+		u64 tx_pkt_p2x                       : 2;
+		u64 sqm_pkt_id                       : 13;
+		u64 mdq_idx                          : 10;
+		u64 reserved_62                      : 1;
+		u64 vld                              : 1;
+	} s;
+	/* struct cavm_nixx_af_tl4x_md_debug3_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG3(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_MD_DEBUG3(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x12d8 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_parent
+ *
+ * NIX AF Transmit Level 4 Parent Registers
+ */
+union cavm_nixx_af_tl4x_parent {
+	u64 u;
+	struct cavm_nixx_af_tl4x_parent_s {
+		u64 reserved_0_15                    : 16;
+		u64 parent                           : 8;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_tl4x_parent_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_PARENT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_PARENT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1288 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_pir
+ *
+ * NIX AF Transmit Level 4 Peak Information Rate Registers
+ * This register has the same bit fields as NIX_AF_TL1()_CIR.
+ */
+union cavm_nixx_af_tl4x_pir {
+	u64 u;
+	struct cavm_nixx_af_tl4x_pir_s {
+		u64 enable                           : 1;
+		u64 rate_mantissa                    : 8;
+		u64 rate_exponent                    : 4;
+		u64 rate_divider_exponent            : 4;
+		u64 reserved_17_28                   : 12;
+		u64 burst_mantissa                   : 8;
+		u64 burst_exponent                   : 4;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl4x_pir_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_PIR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_PIR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1230 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_pointers
+ *
+ * INTERNAL: NIX Transmit Level 4 Linked List Pointers Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL2()_POINTERS.
+ */
+union cavm_nixx_af_tl4x_pointers {
+	u64 u;
+	struct cavm_nixx_af_tl4x_pointers_s {
+		u64 next                             : 9;
+		u64 reserved_9_15                    : 7;
+		u64 prev                             : 9;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_tl4x_pointers_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_POINTERS(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_POINTERS(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1260 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_red
+ *
+ * INTERNAL: NIX Transmit Level 4 Red State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL3()_YELLOW.
+ */
+union cavm_nixx_af_tl4x_red {
+	u64 u;
+	struct cavm_nixx_af_tl4x_red_s {
+		u64 tail                             : 9;
+		u64 reserved_9                       : 1;
+		u64 head                             : 9;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_tl4x_red_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_RED(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_RED(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x12b0 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_sched_state
+ *
+ * NIX AF Transmit Level 4 Scheduling Control State Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SCHED_STATE.
+ */
+union cavm_nixx_af_tl4x_sched_state {
+	u64 u;
+	struct cavm_nixx_af_tl4x_sched_state_s {
+		u64 rr_count                         : 25;
+		u64 reserved_25_63                   : 39;
+	} s;
+	/* struct cavm_nixx_af_tl4x_sched_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_SCHED_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_SCHED_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1240 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_schedule
+ *
+ * NIX AF Transmit Level 4 Scheduling Control Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SCHEDULE.
+ */
+union cavm_nixx_af_tl4x_schedule {
+	u64 u;
+	struct cavm_nixx_af_tl4x_schedule_s {
+		u64 rr_quantum                       : 24;
+		u64 prio                             : 4;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_nixx_af_tl4x_schedule_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_SCHEDULE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_SCHEDULE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1200 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_sdp_link_cfg
+ *
+ * NIX AF Transmit Level 4 Link Configuration Registers
+ * These registers specify which TL4 queues transmit to and are optionally
+ * backpressured by SDP.
+ */
+union cavm_nixx_af_tl4x_sdp_link_cfg {
+	u64 u;
+	struct cavm_nixx_af_tl4x_sdp_link_cfg_s {
+		u64 relchan                          : 8;
+		u64 reserved_8_11                    : 4;
+		u64 ena                              : 1;
+		u64 bp_ena                           : 1;
+		u64 reserved_14_63                   : 50;
+	} s;
+	/* struct cavm_nixx_af_tl4x_sdp_link_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_SDP_LINK_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_SDP_LINK_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0xb10 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_shape
+ *
+ * NIX AF Transmit Level 4 Shaping Control Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SHAPE.
+ */
+union cavm_nixx_af_tl4x_shape {
+	u64 u;
+	struct cavm_nixx_af_tl4x_shape_s {
+		u64 adjust                           : 9;
+		u64 red_algo                         : 2;
+		u64 red_disable                      : 1;
+		u64 yellow_disable                   : 1;
+		u64 reserved_13_23                   : 11;
+		u64 length_disable                   : 1;
+		u64 schedule_list                    : 2;
+		u64 reserved_27_63                   : 37;
+	} s;
+	/* struct cavm_nixx_af_tl4x_shape_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_SHAPE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_SHAPE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1210 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_shape_state
+ *
+ * NIX AF Transmit Level 4 Shaping State Registers
+ * This register has the same bit fields as NIX_AF_TL2()_SHAPE_STATE.
+ * This register must not be written during normal operation.
+ */
+union cavm_nixx_af_tl4x_shape_state {
+	u64 u;
+	struct cavm_nixx_af_tl4x_shape_state_s {
+		u64 cir_accum                        : 26;
+		u64 pir_accum                        : 26;
+		u64 color                            : 2;
+		u64 reserved_54_63                   : 10;
+	} s;
+	/* struct cavm_nixx_af_tl4x_shape_state_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_SHAPE_STATE(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_SHAPE_STATE(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1250 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_sw_xoff
+ *
+ * NIX AF Transmit Level 4 Software Controlled XOFF Registers
+ * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF
+ */
+union cavm_nixx_af_tl4x_sw_xoff {
+	u64 u;
+	struct cavm_nixx_af_tl4x_sw_xoff_s {
+		u64 xoff                             : 1;
+		u64 drain                            : 1;
+		u64 reserved_2                       : 1;
+		u64 drain_irq                        : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_nixx_af_tl4x_sw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_SW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_SW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1270 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_topology
+ *
+ * NIX AF Transmit Level 4 Topology Registers
+ */
+union cavm_nixx_af_tl4x_topology {
+	u64 u;
+	struct cavm_nixx_af_tl4x_topology_s {
+		u64 reserved_0                       : 1;
+		u64 rr_prio                          : 4;
+		u64 reserved_5_31                    : 27;
+		u64 prio_anchor                      : 9;
+		u64 reserved_41_63                   : 23;
+	} s;
+	/* struct cavm_nixx_af_tl4x_topology_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_TOPOLOGY(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_TOPOLOGY(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1280 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4#_yellow
+ *
+ * INTERNAL: NIX Transmit Level 4 Yellow State Debug Register
+ *
+ * This register has the same bit fields as NIX_AF_TL3()_YELLOW
+ */
+union cavm_nixx_af_tl4x_yellow {
+	u64 u;
+	struct cavm_nixx_af_tl4x_yellow_s {
+		u64 tail                             : 9;
+		u64 reserved_9                       : 1;
+		u64 head                             : 9;
+		u64 reserved_19_63                   : 45;
+	} s;
+	/* struct cavm_nixx_af_tl4x_yellow_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4X_YELLOW(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4X_YELLOW(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x12a0 + 0x10000 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tl4_const
+ *
+ * NIX AF Transmit Level 4 Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_nixx_af_tl4_const {
+	u64 u;
+	struct cavm_nixx_af_tl4_const_s {
+		u64 count                            : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_nixx_af_tl4_const_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TL4_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TL4_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x88;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_link#_expr_credit
+ *
+ * NIX AF Transmit Link Express Credit Registers
+ * These registers track credits per link for express packets that may potentially
+ * preempt normal packets. Link index enumerated by NIX_LINK_E.
+ */
+union cavm_nixx_af_tx_linkx_expr_credit {
+	u64 u;
+	struct cavm_nixx_af_tx_linkx_expr_credit_s {
+		u64 reserved_0                       : 1;
+		u64 cc_enable                        : 1;
+		u64 cc_packet_cnt                    : 10;
+		u64 cc_unit_cnt                      : 20;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_tx_linkx_expr_credit_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_LINKX_EXPR_CREDIT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_LINKX_EXPR_CREDIT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 12))
+		return 0xa10 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_link#_hw_xoff
+ *
+ * NIX AF Transmit Link Hardware Controlled XOFF Registers
+ * Link index enumerated by NIX_LINK_E.
+ */
+union cavm_nixx_af_tx_linkx_hw_xoff {
+	u64 u;
+	struct cavm_nixx_af_tx_linkx_hw_xoff_s {
+		u64 chan_xoff                        : 64;
+	} s;
+	/* struct cavm_nixx_af_tx_linkx_hw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_LINKX_HW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_LINKX_HW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 12))
+		return 0xa30 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_link#_norm_credit
+ *
+ * NIX AF Transmit Link Normal Credit Registers
+ * These registers track credits per link for normal (potentially preemptable)
+ * packets sent to CGX and LBK. Link index enumerated by NIX_LINK_E.
+ */
+union cavm_nixx_af_tx_linkx_norm_credit {
+	u64 u;
+	struct cavm_nixx_af_tx_linkx_norm_credit_s {
+		u64 reserved_0                       : 1;
+		u64 cc_enable                        : 1;
+		u64 cc_packet_cnt                    : 10;
+		u64 cc_unit_cnt                      : 20;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_tx_linkx_norm_credit_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_LINKX_NORM_CREDIT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_LINKX_NORM_CREDIT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 12))
+		return 0xa00 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_link#_sw_xoff
+ *
+ * NIX AF Transmit Link Software Controlled XOFF Registers
+ * Link index enumerated by NIX_LINK_E.
+ */
+union cavm_nixx_af_tx_linkx_sw_xoff {
+	u64 u;
+	struct cavm_nixx_af_tx_linkx_sw_xoff_s {
+		u64 chan_xoff                        : 64;
+	} s;
+	/* struct cavm_nixx_af_tx_linkx_sw_xoff_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_LINKX_SW_XOFF(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_LINKX_SW_XOFF(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 12))
+		return 0xa20 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_mcast#
+ *
+ * NIX AF Transmit Multicast Registers
+ * These registers access transmit multicast table entries used to specify multicast replication
+ * lists. Each list consists of linked entries with [EOL] = 1 in the last entry.
+ *
+ * A transmit packet is multicast when the action returned by NPC has NIX_TX_ACTION_S[OP] =
+ * NIX_TX_ACTIONOP_E::MCAST. NIX_TX_ACTION_S[INDEX] points to the start of the multicast
+ * replication list, and [EOL] = 1 indicates the end of list.
+ */
+union cavm_nixx_af_tx_mcastx {
+	u64 u;
+	struct cavm_nixx_af_tx_mcastx_s {
+		u64 channel                          : 12;
+		u64 eol                              : 1;
+		u64 reserved_13_15                   : 3;
+		u64 next                             : 16;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_nixx_af_tx_mcastx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_MCASTX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_MCASTX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 2047))
+		return 0x1900 + 0x8000 * ((a) & 0x7ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_npc_capture_config
+ *
+ * NIX AF Transmit NPC Response Capture Configuration Register
+ * Configures the NPC response capture logic for transmit packets. When enabled,
+ * allows NPC responses for selected packets to be captured in
+ * NIX_AF_TX_NPC_CAPTURE_INFO and NIX_AF_TX_NPC_CAPTURE_RESP().
+ */
+union cavm_nixx_af_tx_npc_capture_config {
+	u64 u;
+	struct cavm_nixx_af_tx_npc_capture_config_s {
+		u64 en                               : 1;
+		u64 continuous                       : 1;
+		u64 lso_segnum_en                    : 1;
+		u64 sqe_id_en                        : 1;
+		u64 sq_id_en                         : 1;
+		u64 lf_id_en                         : 1;
+		u64 reserved_6_11                    : 6;
+		u64 lso_segnum                       : 8;
+		u64 sqe_id                           : 16;
+		u64 sq_id                            : 20;
+		u64 lf_id                            : 8;
+	} s;
+	/* struct cavm_nixx_af_tx_npc_capture_config_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_NPC_CAPTURE_CONFIG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_NPC_CAPTURE_CONFIG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x660;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_npc_capture_info
+ *
+ * NIX AF Transmit NPC Response Capture Information Register
+ * This register contains captured NPC response information for a transmit packet.
+ * See NIX_AF_TX_NPC_CAPTURE_CONFIG.
+ */
+union cavm_nixx_af_tx_npc_capture_info {
+	u64 u;
+	struct cavm_nixx_af_tx_npc_capture_info_s {
+		u64 vld                              : 1;
+		u64 reserved_1_11                    : 11;
+		u64 lso_segnum                       : 8;
+		u64 sqe_id                           : 16;
+		u64 sq_id                            : 20;
+		u64 lf_id                            : 8;
+	} s;
+	/* struct cavm_nixx_af_tx_npc_capture_info_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_NPC_CAPTURE_INFO
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_NPC_CAPTURE_INFO
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x668;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_npc_capture_resp#
+ *
+ * NIX AF Transmit NPC Capture Response Registers
+ * These registers contain the captured NPC response for a transmit packet when
+ * NIX_AF_TX_NPC_CAPTURE_INFO[VLD] is set. See also NIX_AF_TX_NPC_CAPTURE_CONFIG.
+ */
+union cavm_nixx_af_tx_npc_capture_respx {
+	u64 u;
+	struct cavm_nixx_af_tx_npc_capture_respx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_nixx_af_tx_npc_capture_respx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_NPC_CAPTURE_RESPX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_NPC_CAPTURE_RESPX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 7))
+		return 0x680 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_tstmp_cfg
+ *
+ * NIX AF Transmit Timestamp Configuration Register
+ */
+union cavm_nixx_af_tx_tstmp_cfg {
+	u64 u;
+	struct cavm_nixx_af_tx_tstmp_cfg_s {
+		u64 tstmp_wd_period                  : 4;
+		u64 reserved_4_7                     : 4;
+		u64 express                          : 16;
+		u64 reserved_24_63                   : 40;
+	} s;
+	/* struct cavm_nixx_af_tx_tstmp_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_TSTMP_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_TSTMP_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_vtag_def#_ctl
+ *
+ * NIX AF Transmit Vtag Definition Control Registers
+ * The transmit Vtag definition table specifies Vtag layers (e.g. VLAN, E-TAG) to
+ * optionally insert or replace in the TX packet header. Indexed by
+ * NIX_TX_VTAG_ACTION_S[VTAG*_DEF].
+ */
+union cavm_nixx_af_tx_vtag_defx_ctl {
+	u64 u;
+	struct cavm_nixx_af_tx_vtag_defx_ctl_s {
+		u64 size                             : 3;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_nixx_af_tx_vtag_defx_ctl_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_VTAG_DEFX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_VTAG_DEFX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1023))
+		return 0x1a00 + 0x10000 * ((a) & 0x3ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_af_tx_vtag_def#_data
+ *
+ * NIX AF Transmit Vtag Definition Data Registers
+ * See NIX_AF_TX_VTAG_DEF()_CTL.
+ */
+union cavm_nixx_af_tx_vtag_defx_data {
+	u64 u;
+	struct cavm_nixx_af_tx_vtag_defx_data_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_nixx_af_tx_vtag_defx_data_s cn; */
+};
+
+static inline u64 CAVM_NIXX_AF_TX_VTAG_DEFX_DATA(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_AF_TX_VTAG_DEFX_DATA(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1023))
+		return 0x1a10 + 0x10000 * ((a) & 0x3ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cfg
+ *
+ * NIX LF Configuration Register
+ */
+union cavm_nixx_lf_cfg {
+	u64 u;
+	struct cavm_nixx_lf_cfg_s {
+		u64 tcp_timer_int_ena                : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x100;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cint#_cnt
+ *
+ * NIX LF Completion Interrupt Count Registers
+ */
+union cavm_nixx_lf_cintx_cnt {
+	u64 u;
+	struct cavm_nixx_lf_cintx_cnt_s {
+		u64 ecount                           : 32;
+		u64 qcount                           : 16;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_lf_cintx_cnt_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CINTX_CNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CINTX_CNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xd00 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cint#_ena_w1c
+ *
+ * NIX LF Completion Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_lf_cintx_ena_w1c {
+	u64 u;
+	struct cavm_nixx_lf_cintx_ena_w1c_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_cintx_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CINTX_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CINTX_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xd50 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cint#_ena_w1s
+ *
+ * NIX LF Completion Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_lf_cintx_ena_w1s {
+	u64 u;
+	struct cavm_nixx_lf_cintx_ena_w1s_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_cintx_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CINTX_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CINTX_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xd40 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cint#_int
+ *
+ * NIX LF Completion Interrupt Registers
+ */
+union cavm_nixx_lf_cintx_int {
+	u64 u;
+	struct cavm_nixx_lf_cintx_int_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_cintx_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CINTX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CINTX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xd20 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cint#_int_w1s
+ *
+ * NIX LF Completion Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_lf_cintx_int_w1s {
+	u64 u;
+	struct cavm_nixx_lf_cintx_int_w1s_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_cintx_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CINTX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CINTX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xd30 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cint#_wait
+ *
+ * NIX LF Completion Interrupt Count Registers
+ */
+union cavm_nixx_lf_cintx_wait {
+	u64 u;
+	struct cavm_nixx_lf_cintx_wait_s {
+		u64 ecount_wait                      : 32;
+		u64 qcount_wait                      : 16;
+		u64 time_wait                        : 8;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_nixx_lf_cintx_wait_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CINTX_WAIT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CINTX_WAIT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xd10 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cq_op_door
+ *
+ * NIX LF CQ Doorbell Operation Register
+ * A write to this register dequeues CQEs from a CQ ring within the LF.
+ * A read is RAZ.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_cq_op_door {
+	u64 u;
+	struct cavm_nixx_lf_cq_op_door_s {
+		u64 count                            : 16;
+		u64 reserved_16_31                   : 16;
+		u64 cq                               : 20;
+		u64 reserved_52_63                   : 12;
+	} s;
+	/* struct cavm_nixx_lf_cq_op_door_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CQ_OP_DOOR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CQ_OP_DOOR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xb30;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cq_op_int
+ *
+ * NIX LF Completion Queue Interrupt Operation Register
+ * A 64-bit atomic load-and-add to this register reads CQ interrupts and
+ * interrupt enables.
+ * A write optionally sets or clears interrupts and interrupt enables.
+ * A read is RAZ.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_cq_op_int {
+	u64 u;
+	struct cavm_nixx_lf_cq_op_int_s {
+		u64 cq_err_int                       : 8;
+		u64 cq_err_int_ena                   : 8;
+		u64 reserved_16_41                   : 26;
+		u64 op_err                           : 1;
+		u64 setop                            : 1;
+		u64 cq                               : 20;
+	} s;
+	/* struct cavm_nixx_lf_cq_op_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CQ_OP_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CQ_OP_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xb00;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_cq_op_status
+ *
+ * NIX LF Completion Queue Status Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_CQ_CTX_S[HEAD,TAIL].
+ * The atomic write data has format NIX_OP_Q_WDATA_S and selects the CQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_cq_op_status {
+	u64 u;
+	struct cavm_nixx_lf_cq_op_status_s {
+		u64 tail                             : 20;
+		u64 head                             : 20;
+		u64 reserved_40_45                   : 6;
+		u64 cq_err                           : 1;
+		u64 reserved_47_62                   : 16;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_cq_op_status_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_CQ_OP_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_CQ_OP_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xb40;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_err_int
+ *
+ * NIX LF Error Interrupt Register
+ */
+union cavm_nixx_lf_err_int {
+	u64 u;
+	struct cavm_nixx_lf_err_int_s {
+		u64 sqb_fault                        : 1;
+		u64 sq_ctx_fault                     : 1;
+		u64 rq_ctx_fault                     : 1;
+		u64 cq_ctx_fault                     : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_fault                       : 1;
+		u64 ipsec_dyno_fault                 : 1;
+		u64 sq_disabled                      : 1;
+		u64 sq_oor                           : 1;
+		u64 send_jump_fault                  : 1;
+		u64 send_sg_fault                    : 1;
+		u64 rq_disabled                      : 1;
+		u64 rq_oor                           : 1;
+		u64 rx_wqe_fault                     : 1;
+		u64 rss_err                          : 1;
+		u64 reserved_15_19                   : 5;
+		u64 dyno_err                         : 1;
+		u64 reserved_21_23                   : 3;
+		u64 cq_disabled                      : 1;
+		u64 cq_oor                           : 1;
+		u64 reserved_26_27                   : 2;
+		u64 qint_fault                       : 1;
+		u64 cint_fault                       : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_nixx_lf_err_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_ERR_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_ERR_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x220;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_err_int_ena_w1c
+ *
+ * NIX LF Error Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_lf_err_int_ena_w1c {
+	u64 u;
+	struct cavm_nixx_lf_err_int_ena_w1c_s {
+		u64 sqb_fault                        : 1;
+		u64 sq_ctx_fault                     : 1;
+		u64 rq_ctx_fault                     : 1;
+		u64 cq_ctx_fault                     : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_fault                       : 1;
+		u64 ipsec_dyno_fault                 : 1;
+		u64 sq_disabled                      : 1;
+		u64 sq_oor                           : 1;
+		u64 send_jump_fault                  : 1;
+		u64 send_sg_fault                    : 1;
+		u64 rq_disabled                      : 1;
+		u64 rq_oor                           : 1;
+		u64 rx_wqe_fault                     : 1;
+		u64 rss_err                          : 1;
+		u64 reserved_15_19                   : 5;
+		u64 dyno_err                         : 1;
+		u64 reserved_21_23                   : 3;
+		u64 cq_disabled                      : 1;
+		u64 cq_oor                           : 1;
+		u64 reserved_26_27                   : 2;
+		u64 qint_fault                       : 1;
+		u64 cint_fault                       : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_nixx_lf_err_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_ERR_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_ERR_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x230;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_err_int_ena_w1s
+ *
+ * NIX LF Error Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_lf_err_int_ena_w1s {
+	u64 u;
+	struct cavm_nixx_lf_err_int_ena_w1s_s {
+		u64 sqb_fault                        : 1;
+		u64 sq_ctx_fault                     : 1;
+		u64 rq_ctx_fault                     : 1;
+		u64 cq_ctx_fault                     : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_fault                       : 1;
+		u64 ipsec_dyno_fault                 : 1;
+		u64 sq_disabled                      : 1;
+		u64 sq_oor                           : 1;
+		u64 send_jump_fault                  : 1;
+		u64 send_sg_fault                    : 1;
+		u64 rq_disabled                      : 1;
+		u64 rq_oor                           : 1;
+		u64 rx_wqe_fault                     : 1;
+		u64 rss_err                          : 1;
+		u64 reserved_15_19                   : 5;
+		u64 dyno_err                         : 1;
+		u64 reserved_21_23                   : 3;
+		u64 cq_disabled                      : 1;
+		u64 cq_oor                           : 1;
+		u64 reserved_26_27                   : 2;
+		u64 qint_fault                       : 1;
+		u64 cint_fault                       : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_nixx_lf_err_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_ERR_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_ERR_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x238;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_err_int_w1s
+ *
+ * NIX LF Error Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_lf_err_int_w1s {
+	u64 u;
+	struct cavm_nixx_lf_err_int_w1s_s {
+		u64 sqb_fault                        : 1;
+		u64 sq_ctx_fault                     : 1;
+		u64 rq_ctx_fault                     : 1;
+		u64 cq_ctx_fault                     : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_fault                       : 1;
+		u64 ipsec_dyno_fault                 : 1;
+		u64 sq_disabled                      : 1;
+		u64 sq_oor                           : 1;
+		u64 send_jump_fault                  : 1;
+		u64 send_sg_fault                    : 1;
+		u64 rq_disabled                      : 1;
+		u64 rq_oor                           : 1;
+		u64 rx_wqe_fault                     : 1;
+		u64 rss_err                          : 1;
+		u64 reserved_15_19                   : 5;
+		u64 dyno_err                         : 1;
+		u64 reserved_21_23                   : 3;
+		u64 cq_disabled                      : 1;
+		u64 cq_oor                           : 1;
+		u64 reserved_26_27                   : 2;
+		u64 qint_fault                       : 1;
+		u64 cint_fault                       : 1;
+		u64 reserved_30_63                   : 34;
+	} s;
+	/* struct cavm_nixx_lf_err_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_ERR_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_ERR_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x228;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_gint
+ *
+ * NIX LF General Interrupt Register
+ */
+union cavm_nixx_lf_gint {
+	u64 u;
+	struct cavm_nixx_lf_gint_s {
+		u64 drop                             : 1;
+		u64 tcp_timer                        : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_nixx_lf_gint_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_GINT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_GINT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x200;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_gint_ena_w1c
+ *
+ * NIX LF General Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_lf_gint_ena_w1c {
+	u64 u;
+	struct cavm_nixx_lf_gint_ena_w1c_s {
+		u64 drop                             : 1;
+		u64 tcp_timer                        : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_nixx_lf_gint_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_GINT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_GINT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x210;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_gint_ena_w1s
+ *
+ * NIX LF General Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_lf_gint_ena_w1s {
+	u64 u;
+	struct cavm_nixx_lf_gint_ena_w1s_s {
+		u64 drop                             : 1;
+		u64 tcp_timer                        : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_nixx_lf_gint_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_GINT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_GINT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x218;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_gint_w1s
+ *
+ * NIX LF General Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_lf_gint_w1s {
+	u64 u;
+	struct cavm_nixx_lf_gint_w1s_s {
+		u64 drop                             : 1;
+		u64 tcp_timer                        : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_nixx_lf_gint_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_GINT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_GINT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x208;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_mnq_err_dbg
+ *
+ * NIX LF Meta-descriptor Enqueue Error Debug Register
+ * This register captures debug info for an error detected during send
+ * meta-descriptor enqueue from an SQ to an SMQ.
+ * Hardware sets [VALID] when the debug info is captured, and subsequent errors
+ * are not captured until software clears [VALID] by writing a one to it.
+ */
+union cavm_nixx_lf_mnq_err_dbg {
+	u64 u;
+	struct cavm_nixx_lf_mnq_err_dbg_s {
+		u64 errcode                          : 8;
+		u64 sq                               : 20;
+		u64 sqe_id                           : 16;
+		u64 valid                            : 1;
+		u64 reserved_45_63                   : 19;
+	} s;
+	/* struct cavm_nixx_lf_mnq_err_dbg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_MNQ_ERR_DBG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_MNQ_ERR_DBG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x270;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_op_ipsec_dyno_cnt
+ *
+ * NIX LF IPSEC Dynamic Ordering Counter Operation Register
+ * A 64-bit atomic load-and-add to this register reads an IPSEC dynamic ordering
+ * counter (NIX_IPSEC_DYNO_S). A write decrements NIX_IPSEC_DYNO_S[COUNT]. A read
+ * is RAZ. See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA].
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_op_ipsec_dyno_cnt {
+	u64 u;
+	struct cavm_nixx_lf_op_ipsec_dyno_cnt_s {
+		u64 count                            : 32;
+		u64 reserved_32_46                   : 15;
+		u64 storeop                          : 1;
+		u64 dyno_sel                         : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_op_ipsec_dyno_cnt_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_OP_IPSEC_DYNO_CNT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_OP_IPSEC_DYNO_CNT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x980;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_op_send#
+ *
+ * NIX LF Send Operation Registers
+ * An LMTST (or large store from CPT) to this address enqueues one or more SQEs to
+ * a send queue. NIX_SEND_HDR_S[SQ] in the first SQE selects the send queue.The
+ * maximum size of each SQE is specified by NIX_SQ_CTX_S[MAX_SQE_SIZE].
+ *
+ * A read to this address is RAZ.
+ *
+ * An RSL access to this address will fault.
+ *
+ * The endianness of the instruction write data is controlled by NIX_AF_LF()_CFG[BE].
+ *
+ * When a NIX_SEND_JUMP_S is not present in the SQE, the SQE consists of the
+ * entire send descriptor.
+ *
+ * When a NIX_SEND_JUMP_S is present in the SQE, the SQE must contain exactly the
+ * portion of the send descriptor up to and including the NIX_SEND_JUMP_S, and the
+ * remainder of the send descriptor must be at LF IOVA NIX_SEND_JUMP_S[ADDR] in
+ * LLC/DRAM.
+ *
+ * Software must ensure that all LLC/DRAM locations that will be referenced by NIX while
+ * processing this descriptor, including all packet data and post-jump subdescriptors
+ * contain the latest updates before issuing the LMTST. A DMB instruction may be required prior
+ * to the LMTST to ensure this. A DMB following the LMTST may be useful if SQ descriptor ordering
+ * matters and more than one CPU core is simultaneously enqueueing to the same SQ. For more
+ * information on ordering, refer to the HRM "Core Memory Reference Ordering" section in the CPU
+ * cores chapter.
+ */
+union cavm_nixx_lf_op_sendx {
+	u64 u;
+	struct cavm_nixx_lf_op_sendx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_nixx_lf_op_sendx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_OP_SENDX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_OP_SENDX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x800 + 8 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_qint#_cnt
+ *
+ * NIX LF Queue Interrupt Count Registers
+ */
+union cavm_nixx_lf_qintx_cnt {
+	u64 u;
+	struct cavm_nixx_lf_qintx_cnt_s {
+		u64 count                            : 22;
+		u64 reserved_22_63                   : 42;
+	} s;
+	/* struct cavm_nixx_lf_qintx_cnt_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_QINTX_CNT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_QINTX_CNT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xc00 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_qint#_ena_w1c
+ *
+ * NIX LF Queue Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_lf_qintx_ena_w1c {
+	u64 u;
+	struct cavm_nixx_lf_qintx_ena_w1c_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_qintx_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_QINTX_ENA_W1C(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_QINTX_ENA_W1C(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xc30 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_qint#_ena_w1s
+ *
+ * NIX LF Queue Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_lf_qintx_ena_w1s {
+	u64 u;
+	struct cavm_nixx_lf_qintx_ena_w1s_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_qintx_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_QINTX_ENA_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_QINTX_ENA_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xc20 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_qint#_int
+ *
+ * NIX LF Queue Interrupt Registers
+ */
+union cavm_nixx_lf_qintx_int {
+	u64 u;
+	struct cavm_nixx_lf_qintx_int_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_qintx_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_QINTX_INT(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_QINTX_INT(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xc10 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_qint#_int_w1s
+ *
+ * INTERNAL: NIX LF Queue Interrupt Set Registers
+ */
+union cavm_nixx_lf_qintx_int_w1s {
+	u64 u;
+	struct cavm_nixx_lf_qintx_int_w1s_s {
+		u64 intr                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_nixx_lf_qintx_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_QINTX_INT_W1S(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_QINTX_INT_W1S(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0xc18 + 0x1000 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_ras
+ *
+ * NIX LF RAS Interrupt Register
+ */
+union cavm_nixx_lf_ras {
+	u64 u;
+	struct cavm_nixx_lf_ras_s {
+		u64 sqb_poison                       : 1;
+		u64 sq_ctx_poison                    : 1;
+		u64 rq_ctx_poison                    : 1;
+		u64 cq_ctx_poison                    : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_poison                      : 1;
+		u64 ipsec_dyno_poison                : 1;
+		u64 send_jump_poison                 : 1;
+		u64 send_sg_poison                   : 1;
+		u64 qint_poison                      : 1;
+		u64 cint_poison                      : 1;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_lf_ras_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RAS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RAS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x240;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_ras_ena_w1c
+ *
+ * NIX LF RAS Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_nixx_lf_ras_ena_w1c {
+	u64 u;
+	struct cavm_nixx_lf_ras_ena_w1c_s {
+		u64 sqb_poison                       : 1;
+		u64 sq_ctx_poison                    : 1;
+		u64 rq_ctx_poison                    : 1;
+		u64 cq_ctx_poison                    : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_poison                      : 1;
+		u64 ipsec_dyno_poison                : 1;
+		u64 send_jump_poison                 : 1;
+		u64 send_sg_poison                   : 1;
+		u64 qint_poison                      : 1;
+		u64 cint_poison                      : 1;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_lf_ras_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RAS_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RAS_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x250;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_ras_ena_w1s
+ *
+ * NIX LF RAS Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_nixx_lf_ras_ena_w1s {
+	u64 u;
+	struct cavm_nixx_lf_ras_ena_w1s_s {
+		u64 sqb_poison                       : 1;
+		u64 sq_ctx_poison                    : 1;
+		u64 rq_ctx_poison                    : 1;
+		u64 cq_ctx_poison                    : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_poison                      : 1;
+		u64 ipsec_dyno_poison                : 1;
+		u64 send_jump_poison                 : 1;
+		u64 send_sg_poison                   : 1;
+		u64 qint_poison                      : 1;
+		u64 cint_poison                      : 1;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_lf_ras_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RAS_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RAS_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x258;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_ras_w1s
+ *
+ * NIX LF RAS Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_nixx_lf_ras_w1s {
+	u64 u;
+	struct cavm_nixx_lf_ras_w1s_s {
+		u64 sqb_poison                       : 1;
+		u64 sq_ctx_poison                    : 1;
+		u64 rq_ctx_poison                    : 1;
+		u64 cq_ctx_poison                    : 1;
+		u64 reserved_4                       : 1;
+		u64 rsse_poison                      : 1;
+		u64 ipsec_dyno_poison                : 1;
+		u64 send_jump_poison                 : 1;
+		u64 send_sg_poison                   : 1;
+		u64 qint_poison                      : 1;
+		u64 cint_poison                      : 1;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_nixx_lf_ras_w1s_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RAS_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RAS_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x248;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_drop_octs
+ *
+ * NIX LF Receive Queue Dropped Octets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[DROP_OCTS].
+ * The atomic write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_rq_op_drop_octs {
+	u64 u;
+	struct cavm_nixx_lf_rq_op_drop_octs_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_rq_op_drop_octs_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RQ_OP_DROP_OCTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RQ_OP_DROP_OCTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x930;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_drop_pkts
+ *
+ * NIX LF Receive Queue Dropped Packets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[DROP_PKTS].
+ * The atomic write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_rq_op_drop_pkts {
+	u64 u;
+	struct cavm_nixx_lf_rq_op_drop_pkts_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_rq_op_drop_pkts_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RQ_OP_DROP_PKTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RQ_OP_DROP_PKTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x940;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_int
+ *
+ * NIX LF Receive Queue Interrupt Operation Register
+ * A 64-bit atomic load-and-add to this register reads RQ interrupts and
+ * interrupt enables.
+ * A 64-bit write optionally sets or clears interrupts and interrupt enables.
+ *
+ * All other accesses to this register (e.g. reads, 128-bit accesses) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_rq_op_int {
+	u64 u;
+	struct cavm_nixx_lf_rq_op_int_s {
+		u64 rq_int                           : 8;
+		u64 rq_int_ena                       : 8;
+		u64 reserved_16_41                   : 26;
+		u64 op_err                           : 1;
+		u64 setop                            : 1;
+		u64 rq                               : 20;
+	} s;
+	/* struct cavm_nixx_lf_rq_op_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RQ_OP_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RQ_OP_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x900;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_octs
+ *
+ * NIX LF Receive Queue Octets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[OCTS]. The atomic
+ * write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_rq_op_octs {
+	u64 u;
+	struct cavm_nixx_lf_rq_op_octs_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_rq_op_octs_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RQ_OP_OCTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RQ_OP_OCTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x910;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_pkts
+ *
+ * NIX LF Receive Queue Packets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[PKTS]. The atomic
+ * write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_rq_op_pkts {
+	u64 u;
+	struct cavm_nixx_lf_rq_op_pkts_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_rq_op_pkts_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RQ_OP_PKTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RQ_OP_PKTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x920;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_re_pkts
+ *
+ * NIX LF Receive Queue Errored Packets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[RE_PKTS].
+ * The atomic write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_rq_op_re_pkts {
+	u64 u;
+	struct cavm_nixx_lf_rq_op_re_pkts_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_rq_op_re_pkts_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RQ_OP_RE_PKTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RQ_OP_RE_PKTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x950;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rx_secret#
+ *
+ * NIX LF Receive Secret Key Registers
+ */
+union cavm_nixx_lf_rx_secretx {
+	u64 u;
+	struct cavm_nixx_lf_rx_secretx_s {
+		u64 key                              : 64;
+	} s;
+	/* struct cavm_nixx_lf_rx_secretx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RX_SECRETX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RX_SECRETX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 5))
+		return 0 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_rx_stat#
+ *
+ * NIX LF Receive Statistics Registers
+ * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_RX_E.
+ */
+union cavm_nixx_lf_rx_statx {
+	u64 u;
+	struct cavm_nixx_lf_rx_statx_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_lf_rx_statx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_RX_STATX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_RX_STATX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 11))
+		return 0x400 + 8 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_send_err_dbg
+ *
+ * NIX LF Send Error Debug Register
+ * This register captures debug info an error detected on packet send after a
+ * meta-descriptor is granted by PSE.
+ * Hardware sets [VALID] when the debug info is captured, and subsequent errors
+ * are not captured until software clears [VALID] by writing a one to it.
+ */
+union cavm_nixx_lf_send_err_dbg {
+	u64 u;
+	struct cavm_nixx_lf_send_err_dbg_s {
+		u64 errcode                          : 8;
+		u64 sq                               : 20;
+		u64 sqe_id                           : 16;
+		u64 valid                            : 1;
+		u64 reserved_45_63                   : 19;
+	} s;
+	/* struct cavm_nixx_lf_send_err_dbg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SEND_ERR_DBG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SEND_ERR_DBG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x280;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_drop_octs
+ *
+ * NIX LF Send Queue Dropped Octets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_SQ_CTX_S[DROP_OCTS].
+ * The atomic write data has format NIX_OP_Q_WDATA_S and selects the SQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_sq_op_drop_octs {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_drop_octs_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_sq_op_drop_octs_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_DROP_OCTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_DROP_OCTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa40;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_drop_pkts
+ *
+ * NIX LF Send Queue Dropped Packets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_SQ_CTX_S[DROP_PKTS].
+ * The atomic write data has format NIX_OP_Q_WDATA_S and selects the SQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_sq_op_drop_pkts {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_drop_pkts_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_sq_op_drop_pkts_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_DROP_PKTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_DROP_PKTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa50;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_err_dbg
+ *
+ * NIX LF SQ Operation Error Debug Register
+ * This register captures debug info for an error detected on LMT store to
+ * NIX_LF_OP_SEND() or when a NIX_LF_SQ_OP_* register is accessed.
+ * Hardware sets [VALID] when the debug info is captured, and subsequent errors
+ * are not captured until software clears [VALID] by writing a one to it.
+ */
+union cavm_nixx_lf_sq_op_err_dbg {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_err_dbg_s {
+		u64 errcode                          : 8;
+		u64 sq                               : 20;
+		u64 sqe_id                           : 16;
+		u64 valid                            : 1;
+		u64 reserved_45_63                   : 19;
+	} s;
+	/* struct cavm_nixx_lf_sq_op_err_dbg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_ERR_DBG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_ERR_DBG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x260;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_int
+ *
+ * NIX LF Send Queue Interrupt Operation Register
+ * A 64-bit atomic load-and-add to this register reads SQ interrupts,
+ * interrupt enables and XOFF status.
+ * A write optionally sets or clears interrupts, interrupt enables and XOFF
+ * status. A read is RAZ.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_sq_op_int {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_int_s {
+		u64 sq_int                           : 8;
+		u64 sq_int_ena                       : 8;
+		u64 xoff                             : 1;
+		u64 reserved_17_41                   : 25;
+		u64 op_err                           : 1;
+		u64 setop                            : 1;
+		u64 sq                               : 20;
+	} s;
+	/* struct cavm_nixx_lf_sq_op_int_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa00;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_octs
+ *
+ * NIX LF Send Queue Octets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_SQ_CTX_S[OCTS]. The atomic
+ * write data has format NIX_OP_Q_WDATA_S and selects the SQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_sq_op_octs {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_octs_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_sq_op_octs_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_OCTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_OCTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa10;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_pkts
+ *
+ * NIX LF Send Queue Packets Operation Register
+ * A 64-bit atomic load-and-add to this register reads NIX_SQ_CTX_S[PKTS]. The atomic
+ * write data has format NIX_OP_Q_WDATA_S and selects the SQ within LF.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_sq_op_pkts {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_pkts_s {
+		u64 cnt                              : 48;
+		u64 reserved_48_62                   : 15;
+		u64 op_err                           : 1;
+	} s;
+	/* struct cavm_nixx_lf_sq_op_pkts_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_PKTS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_PKTS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa20;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_status
+ *
+ * NIX LF Send Queue Status Operation Register
+ * A 64-bit atomic load-and-add to this register reads status fields in
+ * NIX_SQ_CTX_S. The atomic write data has format NIX_OP_Q_WDATA_S and
+ * selects the SQ within LF.
+ * Completion of the load-and-add operation also ensures that all previously
+ * issued LMT stores to NIX_LF_OP_SEND() have completed.
+ *
+ * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
+ *
+ * RSL accesses to this register are RAZ/WI.
+ */
+union cavm_nixx_lf_sq_op_status {
+	u64 u;
+	struct cavm_nixx_lf_sq_op_status_s {
+		u64 sqb_count                        : 16;
+		u64 reserved_16_19                   : 4;
+		u64 head_offset                      : 6;
+		u64 reserved_26_27                   : 2;
+		u64 tail_offset                      : 6;
+		u64 reserved_34_62                   : 29;
+		u64 op_err                           : 1;
+	} s;
+	struct cavm_nixx_lf_sq_op_status_cn {
+		u64 sqb_count                        : 16;
+		u64 reserved_16_19                   : 4;
+		u64 head_offset                      : 6;
+		u64 reserved_26_27                   : 2;
+		u64 tail_offset                      : 6;
+		u64 reserved_34_35                   : 2;
+		u64 reserved_36_62                   : 27;
+		u64 op_err                           : 1;
+	} cn;
+};
+
+static inline u64 CAVM_NIXX_LF_SQ_OP_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_SQ_OP_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa30;
+	return -1;
+}
+
+/**
+ * Register (RVU_PFVF_BAR2) nix#_lf_tx_stat#
+ *
+ * NIX LF Transmit Statistics Registers
+ * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_TX_E.
+ */
+union cavm_nixx_lf_tx_statx {
+	u64 u;
+	struct cavm_nixx_lf_tx_statx_s {
+		u64 stat                             : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_nixx_lf_tx_statx_s cn; */
+};
+
+static inline u64 CAVM_NIXX_LF_TX_STATX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_LF_TX_STATX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 4))
+		return 0x300 + 8 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_priv_af_int_cfg
+ *
+ * NIX Privileged Admin Function Interrupt Configuration Register
+ */
+union cavm_nixx_priv_af_int_cfg {
+	u64 u;
+	struct cavm_nixx_priv_af_int_cfg_s {
+		u64 msix_offset                      : 11;
+		u64 reserved_11                      : 1;
+		u64 msix_size                        : 8;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_nixx_priv_af_int_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_PRIV_AF_INT_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_PRIV_AF_INT_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8000000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_priv_lf#_cfg
+ *
+ * NIX Privileged Local Function Configuration Registers
+ * These registers allow each NIX local function (LF) to be provisioned to a VF/PF
+ * for RVU. See also NIX_AF_RVU_LF_CFG_DEBUG.
+ *
+ * Software should read this register after write to ensure that the LF is mapped to
+ * [PF_FUNC] before issuing transactions to the mapped PF and function.
+ *
+ * [SLOT] must be zero.
+ *
+ * Internal:
+ * Hardware ignores [SLOT] and always assumes 0x0.
+ */
+union cavm_nixx_priv_lfx_cfg {
+	u64 u;
+	struct cavm_nixx_priv_lfx_cfg_s {
+		u64 slot                             : 8;
+		u64 pf_func                          : 16;
+		u64 reserved_24_62                   : 39;
+		u64 ena                              : 1;
+	} s;
+	/* struct cavm_nixx_priv_lfx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_PRIV_LFX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_PRIV_LFX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x8000010 + 0x100 * ((a) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) nix#_priv_lf#_int_cfg
+ *
+ * NIX Privileged LF Interrupt Configuration Registers
+ */
+union cavm_nixx_priv_lfx_int_cfg {
+	u64 u;
+	struct cavm_nixx_priv_lfx_int_cfg_s {
+		u64 msix_offset                      : 11;
+		u64 reserved_11                      : 1;
+		u64 msix_size                        : 8;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_nixx_priv_lfx_int_cfg_s cn; */
+};
+
+static inline u64 CAVM_NIXX_PRIV_LFX_INT_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NIXX_PRIV_LFX_INT_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 127))
+		return 0x8000020 + 0x100 * ((a) & 0x7f);
+	return -1;
+}
+
+#endif /* __CAVM_CSRS_NIX_H__ */
diff --git a/drivers/net/cavium/octeontx2/cavm-csrs-npc.h b/drivers/net/cavium/octeontx2/cavm-csrs-npc.h
new file mode 100644
index 0000000000..4b17ca52f3
--- /dev/null
+++ b/drivers/net/cavium/octeontx2/cavm-csrs-npc.h
@@ -0,0 +1,1761 @@
+#ifndef __CAVM_CSRS_NPC_H__
+#define __CAVM_CSRS_NPC_H__
+/* This file is auto-generated. Do not edit */
+
+/***********************license start***************
+ * Copyright (c) 2003-2018  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM  NETWORKS MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * @file
+ *
+ * Configuration and status register (CSR) address and type definitions for
+ * Cavium NPC.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ */
+
+/**
+ * Enumeration npc_errlev_e
+ *
+ * NPC Error Level Enumeration
+ * Enumerates the lowest protocol layer containing an error.
+ */
+#define CAVM_NPC_ERRLEV_E_LA (1)
+#define CAVM_NPC_ERRLEV_E_LB (2)
+#define CAVM_NPC_ERRLEV_E_LC (3)
+#define CAVM_NPC_ERRLEV_E_LD (4)
+#define CAVM_NPC_ERRLEV_E_LE (5)
+#define CAVM_NPC_ERRLEV_E_LF (6)
+#define CAVM_NPC_ERRLEV_E_LG (7)
+#define CAVM_NPC_ERRLEV_E_LH (8)
+#define CAVM_NPC_ERRLEV_E_NIX (0xf)
+#define CAVM_NPC_ERRLEV_E_RX(a) (0 + (a))
+#define CAVM_NPC_ERRLEV_E_RE (0)
+
+/**
+ * Enumeration npc_intf_e
+ *
+ * NPC Interface Enumeration
+ * Enumerates the NPC interfaces.
+ */
+#define CAVM_NPC_INTF_E_NIXX_RX(a) (0 + 2 * (a))
+#define CAVM_NPC_INTF_E_NIXX_TX(a) (1 + 2 * (a))
+
+/**
+ * Enumeration npc_lid_e
+ *
+ * NPC Layer ID Enumeration
+ * Enumerates layers parsed by NPC.
+ */
+#define CAVM_NPC_LID_E_LA (0)
+#define CAVM_NPC_LID_E_LB (1)
+#define CAVM_NPC_LID_E_LC (2)
+#define CAVM_NPC_LID_E_LD (3)
+#define CAVM_NPC_LID_E_LE (4)
+#define CAVM_NPC_LID_E_LF (5)
+#define CAVM_NPC_LID_E_LG (6)
+#define CAVM_NPC_LID_E_LH (7)
+
+/**
+ * Enumeration npc_lkupop_e
+ *
+ * NPC Lookup Operation Enumeration
+ * Enumerates the lookup operation for NPC_AF_LKUP_CTL[OP].
+ */
+#define CAVM_NPC_LKUPOP_E_KEY (1)
+#define CAVM_NPC_LKUPOP_E_PKT (0)
+
+/**
+ * Enumeration npc_mcamkeyw_e
+ *
+ * NPC MCAM Search Key Width Enumeration
+ */
+#define CAVM_NPC_MCAMKEYW_E_X1 (0)
+#define CAVM_NPC_MCAMKEYW_E_X2 (1)
+#define CAVM_NPC_MCAMKEYW_E_X4 (2)
+
+/**
+ * Structure npc_layer_info_s
+ *
+ * NPC Layer Parse Information Structure
+ * This structure specifies the format of NPC_RESULT_S[LA,LB,...,LH].
+ */
+union cavm_npc_layer_info_s {
+	u32 u;
+	struct cavm_npc_layer_info_s_s {
+		u32 lptr                             : 8;
+		u32 flags                            : 8;
+		u32 ltype                            : 4;
+		u32 reserved_20_31                   : 12;
+	} s;
+	/* struct cavm_npc_layer_info_s_s cn; */
+};
+
+/**
+ * Structure npc_layer_kex_s
+ *
+ * NPC Layer MCAM Search Key Extract Structure
+ * This structure specifies the format of each of the
+ * NPC_PARSE_KEX_S[LA,LB,...,LH] fields. It contains the subset of
+ * NPC_LAYER_INFO_S fields that can be included in the MCAM search key.
+ * See NPC_PARSE_KEX_S and NPC_AF_INTF()_KEX_CFG.
+ */
+union cavm_npc_layer_kex_s {
+	u32 u;
+	struct cavm_npc_layer_kex_s_s {
+		u32 flags                            : 8;
+		u32 ltype                            : 4;
+		u32 reserved_12_31                   : 20;
+	} s;
+	/* struct cavm_npc_layer_kex_s_s cn; */
+};
+
+/**
+ * Structure npc_mcam_key_x1_s
+ *
+ * NPC MCAM Search Key X1 Structure
+ * This structure specifies the MCAM search key format used by an interface
+ * when NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X1.
+ */
+union cavm_npc_mcam_key_x1_s {
+	u64 u[3];
+	struct cavm_npc_mcam_key_x1_s_s {
+		u64 intf                             : 2;
+		u64 reserved_2_63                    : 62;
+		u64 kw0                              : 64;
+		u64 kw1                              : 48;
+		u64 reserved_176_191                 : 16;
+	} s;
+	/* struct cavm_npc_mcam_key_x1_s_s cn; */
+};
+
+/**
+ * Structure npc_mcam_key_x2_s
+ *
+ * NPC MCAM Search Key X2 Structure
+ * This structure specifies the MCAM search key format used by an interface
+ * when NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X2.
+ */
+union cavm_npc_mcam_key_x2_s {
+	u64 u[5];
+	struct cavm_npc_mcam_key_x2_s_s {
+		u64 intf                             : 2;
+		u64 reserved_2_63                    : 62;
+		u64 kw0                              : 64;
+		u64 kw1                              : 64;
+		u64 kw2                              : 64;
+		u64 kw3                              : 32;
+		u64 reserved_288_319                 : 32;
+	} s;
+	/* struct cavm_npc_mcam_key_x2_s_s cn; */
+};
+
+/**
+ * Structure npc_mcam_key_x4_s
+ *
+ * NPC MCAM Search Key X4 Structure
+ * This structure specifies the MCAM search key format used by an interface
+ * when NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X4.
+ */
+union cavm_npc_mcam_key_x4_s {
+	u64 u[8];
+	struct cavm_npc_mcam_key_x4_s_s {
+		u64 intf                             : 2;
+		u64 reserved_2_63                    : 62;
+		u64 kw0                              : 64;
+		u64 kw1                              : 64;
+		u64 kw2                              : 64;
+		u64 kw3                              : 64;
+		u64 kw4                              : 64;
+		u64 kw5                              : 64;
+		u64 kw6                              : 64;
+	} s;
+	/* struct cavm_npc_mcam_key_x4_s_s cn; */
+};
+
+/**
+ * Structure npc_parse_kex_s
+ *
+ * NPC Parse Key Extract Structure
+ * This structure contains the subset of NPC_RESULT_S fields that can be included
+ * in the MCAM search key. See NPC_AF_INTF()_KEX_CFG.
+ */
+union cavm_npc_parse_kex_s {
+	u64 u[2];
+	struct cavm_npc_parse_kex_s_s {
+		u64 chan                             : 12;
+		u64 errlev                           : 4;
+		u64 errcode                          : 8;
+		u64 l2m                              : 1;
+		u64 l2b                              : 1;
+		u64 l3m                              : 1;
+		u64 l3b                              : 1;
+		u64 la                               : 12;
+		u64 lb                               : 12;
+		u64 lc                               : 12;
+		u64 ld                               : 12;
+		u64 le                               : 12;
+		u64 lf                               : 12;
+		u64 lg                               : 12;
+		u64 lh                               : 12;
+		u64 reserved_124_127                 : 4;
+	} s;
+	/* struct cavm_npc_parse_kex_s_s cn; */
+};
+
+/**
+ * Structure npc_result_s
+ *
+ * NPC Result Structure
+ * This structure contains a packet's parse and flow identification information.
+ */
+union cavm_npc_result_s {
+	u64 u[6];
+	struct cavm_npc_result_s_s {
+		u64 intf                             : 2;
+		u64 pkind                            : 6;
+		u64 chan                             : 12;
+		u64 errlev                           : 4;
+		u64 errcode                          : 8;
+		u64 l2m                              : 1;
+		u64 l2b                              : 1;
+		u64 l3m                              : 1;
+		u64 l3b                              : 1;
+		u64 eoh_ptr                          : 8;
+		u64 reserved_44_63                   : 20;
+		u64 action                           : 64;
+		u64 vtag_action                      : 64;
+		u64 la                               : 20;
+		u64 lb                               : 20;
+		u64 lc                               : 20;
+		u64 reserved_252_255                 : 4;
+		u64 ld                               : 20;
+		u64 le                               : 20;
+		u64 lf                               : 20;
+		u64 reserved_316_319                 : 4;
+		u64 lg                               : 20;
+		u64 lh                               : 20;
+		u64 reserved_360_383                 : 24;
+	} s;
+	/* struct cavm_npc_result_s_s cn; */
+};
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_active_pc
+ *
+ * NPC Interrupt-Timer Configuration Register
+ */
+union cavm_npc_af_active_pc {
+	u64 u;
+	struct cavm_npc_af_active_pc_s {
+		u64 active_pc                        : 64;
+	} s;
+	/* struct cavm_npc_af_active_pc_s cn; */
+};
+
+#define CAVM_NPC_AF_ACTIVE_PC_RVU_BAR0 CAVM_NPC_AF_ACTIVE_PC_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_ACTIVE_PC_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_ACTIVE_PC_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_blk_rst
+ *
+ * NPC AF Block Reset Register
+ */
+union cavm_npc_af_blk_rst {
+	u64 u;
+	struct cavm_npc_af_blk_rst_s {
+		u64 rst                              : 1;
+		u64 reserved_1_62                    : 62;
+		u64 busy                             : 1;
+	} s;
+	/* struct cavm_npc_af_blk_rst_s cn; */
+};
+
+#define CAVM_NPC_AF_BLK_RST_RVU_BAR0 CAVM_NPC_AF_BLK_RST_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_BLK_RST_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_BLK_RST_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x40;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_cfg
+ *
+ * NPC AF General Configuration Register
+ */
+union cavm_npc_af_cfg {
+	u64 u;
+	struct cavm_npc_af_cfg_s {
+		u64 reserved_0_1                     : 2;
+		u64 cclk_force                       : 1;
+		u64 force_intf_clk_en                : 1;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_npc_af_cfg_s cn; */
+};
+
+#define CAVM_NPC_AF_CFG_RVU_BAR0 CAVM_NPC_AF_CFG_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_CFG_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_CFG_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_const
+ *
+ * NPC AF Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_npc_af_const {
+	u64 u;
+	struct cavm_npc_af_const_s {
+		u64 intfs                            : 4;
+		u64 lids                             : 4;
+		u64 kpus                             : 5;
+		u64 reserved_13_15                   : 3;
+		u64 mcam_bank_width                  : 10;
+		u64 reserved_26_27                   : 2;
+		u64 mcam_bank_depth                  : 16;
+		u64 mcam_banks                       : 4;
+		u64 match_stats                      : 16;
+	} s;
+	/* struct cavm_npc_af_const_s cn; */
+};
+
+#define CAVM_NPC_AF_CONST_RVU_BAR0 CAVM_NPC_AF_CONST_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_CONST_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_CONST_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x20;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_const1
+ *
+ * NPC AF Constants 1 Register
+ * This register contains constants for software discovery.
+ */
+union cavm_npc_af_const1 {
+	u64 u;
+	struct cavm_npc_af_const1_s {
+		u64 kpu_entries                      : 12;
+		u64 pkinds                           : 8;
+		u64 cpi_size                         : 16;
+		u64 reserved_36_63                   : 28;
+	} s;
+	/* struct cavm_npc_af_const1_s cn; */
+};
+
+#define CAVM_NPC_AF_CONST1_RVU_BAR0 CAVM_NPC_AF_CONST1_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_CONST1_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_CONST1_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x30;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_cpi#_cfg
+ *
+ * NPC AF Channel Parse Index Table Registers
+ */
+union cavm_npc_af_cpix_cfg {
+	u64 u;
+	struct cavm_npc_af_cpix_cfg_s {
+		u64 padd                             : 4;
+		u64 reserved_4_63                    : 60;
+	} s;
+	/* struct cavm_npc_af_cpix_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_CPIX_CFG_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_CPIX_CFG_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1023))
+		return 0x200000 + 8 * ((a) & 0x3ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_dbg_ctl
+ *
+ * NPC AF Debug Control Register
+ * This register controls the capture of debug information in NPC_AF_KPU()_DBG,
+ * NPC_AF_MCAM_DBG, NPC_AF_DBG_DATA() and NPC_AF_DBG_RESULT().
+ */
+union cavm_npc_af_dbg_ctl {
+	u64 u;
+	struct cavm_npc_af_dbg_ctl_s {
+		u64 continuous                       : 1;
+		u64 lkup_dbg                         : 1;
+		u64 intf_dbg                         : 4;
+		u64 reserved_6_63                    : 58;
+	} s;
+	/* struct cavm_npc_af_dbg_ctl_s cn; */
+};
+
+#define CAVM_NPC_AF_DBG_CTL_RVU_BAR0 CAVM_NPC_AF_DBG_CTL_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_DBG_CTL_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_DBG_CTL_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3000000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_dbg_data#
+ *
+ * NPC AF Debug Data Registers
+ * This register contains packet header data for the last packet/lookup  for which
+ * debug information is captured by NPC_AF_DBG_CTL[INTF_DBG,LKUP_DBG].
+ */
+union cavm_npc_af_dbg_datax {
+	u64 u;
+	struct cavm_npc_af_dbg_datax_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_npc_af_dbg_datax_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_DBG_DATAX_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_DBG_DATAX_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 31))
+		return 0x3001400 + 0x10 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_dbg_result#
+ *
+ * NPC AF Debug Result Registers
+ * This register contains packet result data with format NPC_RESULT_S, for the
+ * last packet/lookup for which debug information is captured by
+ * NPC_AF_DBG_CTL[INTF_DBG,LKUP_DBG].
+ */
+union cavm_npc_af_dbg_resultx {
+	u64 u;
+	struct cavm_npc_af_dbg_resultx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_npc_af_dbg_resultx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_DBG_RESULTX_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_DBG_RESULTX_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 5))
+		return 0x3001800 + 0x10 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_dbg_status
+ *
+ * NPC AF Debug Status Register
+ * This register controls the capture of debug information in NPC_AF_KPU()_DBG,
+ * NPC_AF_MCAM_DBG, NPC_AF_LKUP_DATA() and NPC_AF_LKUP_RESULT().
+ */
+union cavm_npc_af_dbg_status {
+	u64 u;
+	struct cavm_npc_af_dbg_status_s {
+		u64 done                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_npc_af_dbg_status_s cn; */
+};
+
+#define CAVM_NPC_AF_DBG_STATUS_RVU_BAR0 CAVM_NPC_AF_DBG_STATUS_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_DBG_STATUS_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_DBG_STATUS_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3000010;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_dv_fc_scratch
+ *
+ * INTERNAL: NPC AF Scratch Register
+ *
+ * Internal:
+ * This register is for internal DV purpose.
+ */
+union cavm_npc_af_dv_fc_scratch {
+	u64 u;
+	struct cavm_npc_af_dv_fc_scratch_s {
+		u64 it                               : 64;
+	} s;
+	/* struct cavm_npc_af_dv_fc_scratch_s cn; */
+};
+
+#define CAVM_NPC_AF_DV_FC_SCRATCH_RVU_BAR0 CAVM_NPC_AF_DV_FC_SCRATCH_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_DV_FC_SCRATCH_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_DV_FC_SCRATCH_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x60;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_eco0
+ *
+ * INTERNAL: ECO 0 Register
+ */
+union cavm_npc_af_eco0 {
+	u64 u;
+	struct cavm_npc_af_eco0_s {
+		u64 eco_rw                           : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_npc_af_eco0_s cn; */
+};
+
+#define CAVM_NPC_AF_ECO0_RVU_BAR0 CAVM_NPC_AF_ECO0_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_ECO0_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_ECO0_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x200;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_ikpu_err_ctl
+ *
+ * NPC AF Initial KPU Error Control Registers
+ * Similar to NPC_AF_KPU()_ERR_CTL, but specifies values captured in
+ * NPC_RESULT_S[ERRLEV,ERRCODE] for errors detected by the PKIND-based initial
+ * actions from NPC_AF_PKIND()_ACTION0 and NPC_AF_PKIND()_ACTION1.
+ * [DP_OFFSET_ERRCODE] from this register is never used.
+ */
+union cavm_npc_af_ikpu_err_ctl {
+	u64 u;
+	struct cavm_npc_af_ikpu_err_ctl_s {
+		u64 errlev                           : 4;
+		u64 dp_offset_errcode                : 8;
+		u64 ptr_advance_errcode              : 8;
+		u64 var_len_offset_errcode           : 8;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_npc_af_ikpu_err_ctl_s cn; */
+};
+
+#define CAVM_NPC_AF_IKPU_ERR_CTL_RVU_BAR0 CAVM_NPC_AF_IKPU_ERR_CTL_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_IKPU_ERR_CTL_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_IKPU_ERR_CTL_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3000080;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_kex_cfg
+ *
+ * NPC AF Interface Key Extract Configuration Registers
+ */
+union cavm_npc_af_intfx_kex_cfg {
+	u64 u;
+	struct cavm_npc_af_intfx_kex_cfg_s {
+		u64 parse_nibble_ena                 : 31;
+		u64 reserved_31                      : 1;
+		u64 keyw                             : 3;
+		u64 reserved_35_63                   : 29;
+	} s;
+	/* struct cavm_npc_af_intfx_kex_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_KEX_CFG_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_KEX_CFG_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x1010 + 0x100 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_ldata#_flags#_cfg
+ *
+ * NPC AF Interface Layer Data Flags Configuration Registers
+ * These registers control the extraction of layer data (LDATA) into the MCAM search key for each
+ * interface based on the FLAGS\<3:0\> bits of two layers selected by NPC_AF_KEX_LDATA()_FLAGS_CFG.
+ */
+union cavm_npc_af_intfx_ldatax_flagsx_cfg {
+	u64 u;
+	struct cavm_npc_af_intfx_ldatax_flagsx_cfg_s {
+		u64 key_offset                       : 6;
+		u64 reserved_6                       : 1;
+		u64 ena                              : 1;
+		u64 hdr_offset                       : 8;
+		u64 bytesm1                          : 4;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_npc_af_intfx_ldatax_flagsx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_LDATAX_FLAGSX_CFG_RVU_BAR0(u64 a, u64 b, u64 c)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_LDATAX_FLAGSX_CFG_RVU_BAR0(u64 a, u64 b, u64 c)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1) && (b <= 1) && (c <= 15)))
+		return 0x980000 + 0x10000 * ((a) & 0x1) + 0x1000 * ((b) & 0x1) + 8 * ((c) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_lid#_lt#_ld#_cfg
+ *
+ * NPC AF Interface Layer Data Extract Configuration Registers
+ * These registers control the extraction of layer data (LDATA) into the MCAM search key for each
+ * interface. Up to two LDATA fields can be extracted per layer (LID(0..7) indexed by NPC_LID_E),
+ * with up to 16 bytes per LDATA field. For each layer, the corresponding
+ * NPC_LAYER_INFO_S[LTYPE] value in NPC_RESULT_S is used as the LTYPE(0..15) index and select the
+ * associated LDATA(0..1) registers.
+ *
+ * NPC_LAYER_INFO_S[LTYPE]=0x0 means the corresponding layer not parsed (invalid), so
+ * software should keep NPC_AF_INTF()_LID()_LT(0)_LD()_CFG[ENA] clear to disable extraction
+ * when LTYPE is zero.
+ */
+union cavm_npc_af_intfx_lidx_ltx_ldx_cfg {
+	u64 u;
+	struct cavm_npc_af_intfx_lidx_ltx_ldx_cfg_s {
+		u64 key_offset                       : 6;
+		u64 flags_ena                        : 1;
+		u64 ena                              : 1;
+		u64 hdr_offset                       : 8;
+		u64 bytesm1                          : 4;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_npc_af_intfx_lidx_ltx_ldx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_LIDX_LTX_LDX_CFG_RVU_BAR0(u64 a, u64 b, u64 c, u64 d)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_LIDX_LTX_LDX_CFG_RVU_BAR0(u64 a, u64 b, u64 c, u64 d)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1) && (b <= 7) && (c <= 15) && (d <= 1)))
+		return 0x900000 + 0x10000 * ((a) & 0x1) + 0x1000 * ((b) & 0x7) + 0x20 * ((c) & 0xf) + 8 * ((d) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_miss_act
+ *
+ * NPC AF Interface MCAM Miss Action Data Registers
+ * When a combination of NPC_AF_MCAME()_BANK()_CAM()_* and
+ * NPC_AF_MCAME()_BANK()_CFG[ENA] yields an MCAM miss for a packet, this
+ * register specifies the packet's match action captured in NPC_RESULT_S[ACTION].
+ */
+union cavm_npc_af_intfx_miss_act {
+	u64 u;
+	struct cavm_npc_af_intfx_miss_act_s {
+		u64 action                           : 64;
+	} s;
+	/* struct cavm_npc_af_intfx_miss_act_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_MISS_ACT_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_MISS_ACT_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x1a00000 + 0x10 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_miss_stat_act
+ *
+ * NPC AF Interface MCAM Miss Stat Action Data Registers
+ * Used to optionally increment a NPC_AF_MATCH_STAT() counter when a packet
+ * misses an MCAM entry.
+ */
+union cavm_npc_af_intfx_miss_stat_act {
+	u64 u;
+	struct cavm_npc_af_intfx_miss_stat_act_s {
+		u64 stat_sel                         : 9;
+		u64 ena                              : 1;
+		u64 reserved_10_63                   : 54;
+	} s;
+	/* struct cavm_npc_af_intfx_miss_stat_act_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_MISS_STAT_ACT_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_MISS_STAT_ACT_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x1880040 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_miss_tag_act
+ *
+ * NPC AF Interface MCAM Miss VTag Action Data Registers
+ * When a combination of NPC_AF_MCAME()_BANK()_CAM()_* and
+ * NPC_AF_MCAME()_BANK()_CFG[ENA] yields an MCAM miss for a packet, this
+ * register specifies the packet's match Vtag action captured in
+ * NPC_RESULT_S[VTAG_ACTION].
+ */
+union cavm_npc_af_intfx_miss_tag_act {
+	u64 u;
+	struct cavm_npc_af_intfx_miss_tag_act_s {
+		u64 vtag_action                      : 64;
+	} s;
+	/* struct cavm_npc_af_intfx_miss_tag_act_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_MISS_TAG_ACT_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_MISS_TAG_ACT_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x1b00008 + 0x10 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_intf#_stat
+ *
+ * NPC AF Interface Statistics Registers
+ * Statistics per interface. Index enumerated by NPC_INTF_E.
+ */
+union cavm_npc_af_intfx_stat {
+	u64 u;
+	struct cavm_npc_af_intfx_stat_s {
+		u64 count                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_npc_af_intfx_stat_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_INTFX_STAT_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_INTFX_STAT_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x2000800 + 0x10 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kcam_scrub_ctl
+ *
+ * NPC AF KCAM Scrub Control Register
+ */
+union cavm_npc_af_kcam_scrub_ctl {
+	u64 u;
+	struct cavm_npc_af_kcam_scrub_ctl_s {
+		u64 ena                              : 1;
+		u64 reserved_1_7                     : 7;
+		u64 lp_dis                           : 1;
+		u64 reserved_9_15                    : 7;
+		u64 toth                             : 4;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_npc_af_kcam_scrub_ctl_s cn; */
+};
+
+#define CAVM_NPC_AF_KCAM_SCRUB_CTL_RVU_BAR0 CAVM_NPC_AF_KCAM_SCRUB_CTL_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_KCAM_SCRUB_CTL_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KCAM_SCRUB_CTL_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xb0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kex_ldata#_flags_cfg
+ *
+ * NPC AF Key Extract Layer Data Flags Configuration Register
+ */
+union cavm_npc_af_kex_ldatax_flags_cfg {
+	u64 u;
+	struct cavm_npc_af_kex_ldatax_flags_cfg_s {
+		u64 lid                              : 3;
+		u64 reserved_3_63                    : 61;
+	} s;
+	/* struct cavm_npc_af_kex_ldatax_flags_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KEX_LDATAX_FLAGS_CFG_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KEX_LDATAX_FLAGS_CFG_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x800 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_cfg
+ *
+ * NPC AF KPU Configuration Registers
+ */
+union cavm_npc_af_kpux_cfg {
+	u64 u;
+	struct cavm_npc_af_kpux_cfg_s {
+		u64 ena                              : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_npc_af_kpux_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_CFG_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_CFG_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x500 + 8 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_dbg
+ *
+ * NPC AF KPU Debug Registers
+ * This register contains information for the last packet/lookup for which debug
+ * is enabled by NPC_AF_DBG_CTL[INTF_DBG,LKUP_DBG].
+ */
+union cavm_npc_af_kpux_dbg {
+	u64 u;
+	struct cavm_npc_af_kpux_dbg_s {
+		u64 hit_entry                        : 8;
+		u64 byp                              : 1;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_npc_af_kpux_dbg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_DBG_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_DBG_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x3000020 + 0x100 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_entry#_action0
+ *
+ * NPC AF KPU Entry Action Data 0 Registers
+ * When a KPU's search data matches a KPU CAM entry in
+ * NPC_AF_KPU()_ENTRY()_CAM(), the corresponding entry action in
+ * NPC_AF_KPU()_ENTRY()_ACTION0 and NPC_AF_KPU()_ENTRY()_ACTION1 specifies the
+ * next state and operations to perform before exiting the KPU.
+ */
+union cavm_npc_af_kpux_entryx_action0 {
+	u64 u;
+	struct cavm_npc_af_kpux_entryx_action0_s {
+		u64 var_len_shift                    : 3;
+		u64 var_len_right                    : 1;
+		u64 var_len_mask                     : 8;
+		u64 var_len_offset                   : 8;
+		u64 ptr_advance                      : 8;
+		u64 capture_flags                    : 8;
+		u64 capture_ltype                    : 4;
+		u64 capture_lid                      : 3;
+		u64 reserved_43                      : 1;
+		u64 next_state                       : 8;
+		u64 parse_done                       : 1;
+		u64 capture_ena                      : 1;
+		u64 byp_count                        : 3;
+		u64 reserved_57_63                   : 7;
+	} s;
+	/* struct cavm_npc_af_kpux_entryx_action0_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_ENTRYX_ACTION0_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_ENTRYX_ACTION0_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b <= 127)))
+		return 0x100020 + 0x4000 * ((a) & 0xf) + 0x40 * ((b) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_entry#_action1
+ *
+ * NPC AF KPU Entry Action Data 0 Registers
+ * See NPC_AF_KPU()_ENTRY()_ACTION0.
+ */
+union cavm_npc_af_kpux_entryx_action1 {
+	u64 u;
+	struct cavm_npc_af_kpux_entryx_action1_s {
+		u64 dp0_offset                       : 8;
+		u64 dp1_offset                       : 8;
+		u64 dp2_offset                       : 8;
+		u64 errcode                          : 8;
+		u64 errlev                           : 4;
+		u64 reserved_36_63                   : 28;
+	} s;
+	/* struct cavm_npc_af_kpux_entryx_action1_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_ENTRYX_ACTION1_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_ENTRYX_ACTION1_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b <= 127)))
+		return 0x100028 + 0x4000 * ((a) & 0xf) + 0x40 * ((b) & 0x7f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_entry#_cam#
+ *
+ * NPC AF KPU Entry CAM Registers
+ * KPU comparison ternary data. The field values in
+ * NPC_AF_KPU()_ENTRY()_CAM() are ternary, where  each data bit of the
+ * search key matches as follows:
+ * _ [CAM(1)]\<n\>=0, [CAM(0)]\<n\>=0: Always match; search key data\<n\> don't care.
+ * _ [CAM(1)]\<n\>=0, [CAM(0)]\<n\>=1: Match when search key data\<n\> == 0.
+ * _ [CAM(1)]\<n\>=1, [CAM(0)]\<n\>=0: Match when search key data\<n\> == 1.
+ * _ [CAM(1)]\<n\>=1, [CAM(0)]\<n\>=1: Reserved.
+ *
+ * The reserved combination is not allowed. Hardware suppresses any write to
+ * CAM(0) or CAM(1) that would result in the reserved combination for any CAM bit.
+ *
+ * Software must program a default entry for each KPU, e.g. by programming each
+ * KPU's last entry {b} (NPC_AF_KPU()_ENTRY({b})_CAM()) to always match all
+ * bits.
+ */
+union cavm_npc_af_kpux_entryx_camx {
+	u64 u;
+	struct cavm_npc_af_kpux_entryx_camx_s {
+		u64 dp0_data                         : 16;
+		u64 dp1_data                         : 16;
+		u64 dp2_data                         : 16;
+		u64 state                            : 8;
+		u64 reserved_56_63                   : 8;
+	} s;
+	/* struct cavm_npc_af_kpux_entryx_camx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_ENTRYX_CAMX_RVU_BAR0(u64 a, u64 b, u64 c)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_ENTRYX_CAMX_RVU_BAR0(u64 a, u64 b, u64 c)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b <= 127) && (c <= 1)))
+		return 0x100000 + 0x4000 * ((a) & 0xf) + 0x40 * ((b) & 0x7f) + 8 * ((c) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_entry_dis#
+ *
+ * NPC AF KPU Entry Disable Registers
+ * See NPC_AF_KPU()_ENTRY()_ACTION0.
+ */
+union cavm_npc_af_kpux_entry_disx {
+	u64 u;
+	struct cavm_npc_af_kpux_entry_disx_s {
+		u64 dis                              : 64;
+	} s;
+	/* struct cavm_npc_af_kpux_entry_disx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_ENTRY_DISX_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_ENTRY_DISX_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b <= 1)))
+		return 0x180000 + 0x40 * ((a) & 0xf) + 8 * ((b) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_kpu#_err_ctl
+ *
+ * NPC AF KPU Error Control Registers
+ * This register specifies values captured in NPC_RESULT_S[ERRLEV,ERRCODE] when errors
+ * are detected by a KPU.
+ */
+union cavm_npc_af_kpux_err_ctl {
+	u64 u;
+	struct cavm_npc_af_kpux_err_ctl_s {
+		u64 errlev                           : 4;
+		u64 dp_offset_errcode                : 8;
+		u64 ptr_advance_errcode              : 8;
+		u64 var_len_offset_errcode           : 8;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_npc_af_kpux_err_ctl_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_KPUX_ERR_CTL_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_KPUX_ERR_CTL_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x30000a0 + 0x100 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_lkup_ctl
+ *
+ * NPC AF Software Lookup Control Registers
+ */
+union cavm_npc_af_lkup_ctl {
+	u64 u;
+	struct cavm_npc_af_lkup_ctl_s {
+		u64 intf                             : 2;
+		u64 pkind                            : 6;
+		u64 chan                             : 12;
+		u64 hdr_sizem1                       : 8;
+		u64 op                               : 3;
+		u64 exec                             : 1;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_npc_af_lkup_ctl_s cn; */
+};
+
+#define CAVM_NPC_AF_LKUP_CTL_RVU_BAR0 CAVM_NPC_AF_LKUP_CTL_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_LKUP_CTL_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_LKUP_CTL_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2000000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_lkup_data#
+ *
+ * NPC AF Software Lookup Data Registers
+ */
+union cavm_npc_af_lkup_datax {
+	u64 u;
+	struct cavm_npc_af_lkup_datax_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_npc_af_lkup_datax_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_LKUP_DATAX_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_LKUP_DATAX_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 31))
+		return 0x2000200 + 0x10 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_lkup_result#
+ *
+ * NPC AF Software Lookup Result Registers
+ */
+union cavm_npc_af_lkup_resultx {
+	u64 u;
+	struct cavm_npc_af_lkup_resultx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_npc_af_lkup_resultx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_LKUP_RESULTX_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_LKUP_RESULTX_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 5))
+		return 0x2000400 + 0x10 * ((a) & 0x7);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_match_stat#
+ *
+ * NPC AF Match Statistics Registers
+ */
+union cavm_npc_af_match_statx {
+	u64 u;
+	struct cavm_npc_af_match_statx_s {
+		u64 count                            : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_npc_af_match_statx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MATCH_STATX_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MATCH_STATX_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 511))
+		return 0x1880008 + 0x100 * ((a) & 0x1ff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcam_bank#_hit#
+ *
+ * NPC AF MCAM Bank Hit Registers
+ */
+union cavm_npc_af_mcam_bankx_hitx {
+	u64 u;
+	struct cavm_npc_af_mcam_bankx_hitx_s {
+		u64 hit                              : 64;
+	} s;
+	/* struct cavm_npc_af_mcam_bankx_hitx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAM_BANKX_HITX_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAM_BANKX_HITX_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 3) && (b <= 15)))
+		return 0x1c80000 + 0x100 * ((a) & 0x3) + 0x10 * ((b) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcam_dbg
+ *
+ * NPC AF MCAM Debug Register
+ * This register contains information for the last packet/lookup for which debug
+ * is enabled by NPC_AF_DBG_CTL[INTF_DBG,LKUP_DBG].
+ */
+union cavm_npc_af_mcam_dbg {
+	u64 u;
+	struct cavm_npc_af_mcam_dbg_s {
+		u64 hit_entry                        : 10;
+		u64 reserved_10_11                   : 2;
+		u64 hit_bank                         : 2;
+		u64 reserved_14_15                   : 2;
+		u64 miss                             : 1;
+		u64 reserved_17_63                   : 47;
+	} s;
+	/* struct cavm_npc_af_mcam_dbg_s cn; */
+};
+
+#define CAVM_NPC_AF_MCAM_DBG_RVU_BAR0 CAVM_NPC_AF_MCAM_DBG_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_MCAM_DBG_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAM_DBG_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x3001000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcam_scrub_ctl
+ *
+ * NPC AF MCAM Scrub Control Register
+ */
+union cavm_npc_af_mcam_scrub_ctl {
+	u64 u;
+	struct cavm_npc_af_mcam_scrub_ctl_s {
+		u64 ena                              : 1;
+		u64 reserved_1_7                     : 7;
+		u64 lp_dis                           : 1;
+		u64 reserved_9_15                    : 7;
+		u64 toth                             : 4;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_npc_af_mcam_scrub_ctl_s cn; */
+};
+
+#define CAVM_NPC_AF_MCAM_SCRUB_CTL_RVU_BAR0 CAVM_NPC_AF_MCAM_SCRUB_CTL_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_MCAM_SCRUB_CTL_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAM_SCRUB_CTL_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xa0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_action
+ *
+ * NPC AF MCAM Entry Bank Action Data Registers
+ * Specifies a packet's match action captured in NPC_RESULT_S[ACTION].
+ *
+ * When an interface is configured to use the NPC_MCAM_KEY_X2_S search key
+ * format (NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X2),
+ * * NPC_AF_MCAME()_BANK(0)_ACTION/_TAG_ACT/_STAT_ACT are used if the search key
+ * matches NPC_AF_MCAME()_BANK(0..1)_CAM()_W*.
+ * * NPC_AF_MCAME()_BANK(2)_ACTION/_TAG_ACT/_STAT_ACT are used if the search key
+ * matches NPC_AF_MCAME()_BANK(2..3)_CAM()_W*.
+ * * NPC_AF_MCAME()_BANK(1,3)_ACTION/_TAG_ACT/_STAT_ACT are not used.
+ *
+ * When an interface is configured to use the NPC_MCAM_KEY_X4_S search key
+ * format (NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X4):
+ * * NPC_AF_MCAME()_BANK(0)_ACTION/_TAG_ACT/_STAT_ACT are used if the search key
+ * matches NPC_AF_MCAME()_BANK(0..3)_CAM()_W*.
+ * * NPC_AF_MCAME()_BANK(1..3)_ACTION/_TAG_ACT/_STAT_ACT are not used.
+ */
+union cavm_npc_af_mcamex_bankx_action {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_action_s {
+		u64 action                           : 64;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_action_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_ACTION_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_ACTION_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3)))
+		return 0x1900000 + 0x100 * ((a) & 0x3ff) + 0x10 * ((b) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_cam#_intf
+ *
+ * NPC AF MCAM Entry Bank CAM Data Interface Registers
+ * MCAM comparison ternary data interface word. The field values in
+ * NPC_AF_MCAME()_BANK()_CAM()_INTF, NPC_AF_MCAME()_BANK()_CAM()_W0 and
+ * NPC_AF_MCAME()_BANK()_CAM()_W1 are ternary, where  each data bit of the
+ * search key matches as follows:
+ * _ [CAM(1)]\<n\>=0, [CAM(0)]\<n\>=0: Always match; search key data\<n\> don't care.
+ * _ [CAM(1)]\<n\>=0, [CAM(0)]\<n\>=1: Match when search key data\<n\> == 0.
+ * _ [CAM(1)]\<n\>=1, [CAM(0)]\<n\>=0: Match when search key data\<n\> == 1.
+ * _ [CAM(1)]\<n\>=1, [CAM(0)]\<n\>=1: Reserved.
+ *
+ * The reserved combination is not allowed. Hardware suppresses any write to
+ * CAM(0) or CAM(1) that would result in the reserved combination for any CAM bit.
+ *
+ * When an interface is configured to use the NPC_MCAM_KEY_X1_S search key
+ * format (NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X1), the four banks
+ * of every MCAM entry are used as individual entries, each of which is
+ * independently compared with the search key as follows:
+ * _ NPC_AF_MCAME()_BANK()_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X1_S[INTF].
+ * _ NPC_AF_MCAME()_BANK()_CAM()_W0[MD] corresponds to NPC_MCAM_KEY_X1_S[KW0].
+ * _ NPC_AF_MCAME()_BANK()_CAM()_W1[MD] corresponds to NPC_MCAM_KEY_X1_S[KW1].
+ *
+ * When an interface is configured to use the NPC_MCAM_KEY_X2_S search key
+ * format (NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X2), banks 0-1 of
+ * every MCAM entry are used as one double-wide entry, banks 2-3 as a second
+ * double-wide entry, and each double-wide entry is independently compared
+ * with the search key as follows:
+ * _ NPC_AF_MCAME()_BANK(0,2)_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X2_S[INTF].
+ * _ NPC_AF_MCAME()_BANK(0,2)_CAM()_W0[MD] corresponds to NPC_MCAM_KEY_X2_S[KW0].
+ * _ NPC_AF_MCAME()_BANK(0,2)_CAM()_W1[MD] corresponds to NPC_MCAM_KEY_X2_S[KW1]\<47:0\>.
+ * _ NPC_AF_MCAME()_BANK(1,3)_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X2_S[INTF].
+ * _ NPC_AF_MCAME()_BANK(1,3)_CAM()_W0[MD]\<15:0\> corresponds to NPC_MCAM_KEY_X2_S[KW1]\<63:48\>.
+ * _ NPC_AF_MCAME()_BANK(1,3)_CAM()_W0[MD]\<63:16\> corresponds to NPC_MCAM_KEY_X2_S[KW2]\<47:0\>.
+ * _ NPC_AF_MCAME()_BANK(1,3)_CAM()_W1[MD]\<15:0\> corresponds to NPC_MCAM_KEY_X2_S[KW2]\<63:48\>.
+ * _ NPC_AF_MCAME()_BANK(1,3)_CAM()_W1[MD]\<47:16\> corresponds to NPC_MCAM_KEY_X2_S[KW3]\<31:0\>.
+ *
+ * When an interface is configured to use the NPC_MCAM_KEY_X4_S search key
+ * format (NPC_AF_INTF()_KEX_CFG[KEYW] = NPC_MCAMKEYW_E::X4), the four banks of every MCAM entry
+ * are used as a single quad-wide entry that is compared with the search key as follows:
+ * _ NPC_AF_MCAME()_BANK(0)_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X4_S[INTF].
+ * _ NPC_AF_MCAME()_BANK(0)_CAM()_W0[MD] corresponds to NPC_MCAM_KEY_X4_S[KW0].
+ * _ NPC_AF_MCAME()_BANK(0)_CAM()_W1[MD] corresponds to NPC_MCAM_KEY_X4_S[KW1]\<47:0\>.
+ * _ NPC_AF_MCAME()_BANK(1)_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X4_S[INTF].
+ * _ NPC_AF_MCAME()_BANK(1)_CAM()_W0[MD]\<15:0\> corresponds to NPC_MCAM_KEY_X4_S[KW1]\<63:48\>.
+ * _ NPC_AF_MCAME()_BANK(1)_CAM()_W0[MD]\<63:16\> corresponds to NPC_MCAM_KEY_X4_S[KW2]\<47:0\>.
+ * _ NPC_AF_MCAME()_BANK(1)_CAM()_W1[MD]\<15:0\> corresponds to NPC_MCAM_KEY_X4_S[KW2]\<63:48\>.
+ * _ NPC_AF_MCAME()_BANK(1)_CAM()_W1[MD]\<47:16\> corresponds to NPC_MCAM_KEY_X4_S[KW3]\<31:0\>.
+ * _ NPC_AF_MCAME()_BANK(2)_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X4_S[INTF].
+ * _ NPC_AF_MCAME()_BANK(2)_CAM()_W0[MD]\<31:0\> corresponds to NPC_MCAM_KEY_X4_S[KW3]\<63:32\>.
+ * _ NPC_AF_MCAME()_BANK(2)_CAM()_W0[MD]\<63:32\> corresponds to NPC_MCAM_KEY_X4_S[KW4]\<31:0\>.
+ * _ NPC_AF_MCAME()_BANK(2)_CAM()_W1[MD]\<31:0\> corresponds to NPC_MCAM_KEY_X4_S[KW4]\<63:32\>.
+ * _ NPC_AF_MCAME()_BANK(2)_CAM()_W1[MD]\<47:32\> corresponds to NPC_MCAM_KEY_X4_S[KW5]\<15:0\>.
+ * _ NPC_AF_MCAME()_BANK(3)_CAM()_INTF[INTF] corresponds to NPC_MCAM_KEY_X4_S[INTF].
+ * _ NPC_AF_MCAME()_BANK(3)_CAM()_W0[MD]\<47:0\> corresponds to NPC_MCAM_KEY_X4_S[KW5]\<63:16\>.
+ * _ NPC_AF_MCAME()_BANK(3)_CAM()_W0[MD]\<63:48\> corresponds to NPC_MCAM_KEY_X4_S[KW6]\<15:0\>.
+ * _ NPC_AF_MCAME()_BANK(3)_CAM()_W1[MD] corresponds to NPC_MCAM_KEY_X4_S[KW6]\<63:16\>.
+ *
+ * Note that for the X2 and X4 formats, a wide entry will not match unless
+ * the INTF fields from the associated two or four banks match the INTF
+ * value from the search key.
+ *
+ * For the X1 and X2 formats, a match in a lower-numbered bank takes priority over
+ * a match in any higher numbered banks. Within each bank, the lowest numbered
+ * matching entry takes priority over any higher numbered entry.
+ */
+union cavm_npc_af_mcamex_bankx_camx_intf {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_camx_intf_s {
+		u64 intf                             : 2;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_camx_intf_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CAMX_INTF_RVU_BAR0(u64 a, u64 b, u64 c)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CAMX_INTF_RVU_BAR0(u64 a, u64 b, u64 c)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3) && (c <= 1)))
+		return 0x1000000 + 0x400 * ((a) & 0x3ff) + 0x40 * ((b) & 0x3) + 8 * ((c) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_cam#_w0
+ *
+ * NPC AF MCAM Entry Bank CAM Data Word 0 Registers
+ * MCAM comparison ternary data word 0. See NPC_AF_MCAME()_BANK()_CAM()_INTF.
+ */
+union cavm_npc_af_mcamex_bankx_camx_w0 {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_camx_w0_s {
+		u64 md                               : 64;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_camx_w0_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CAMX_W0_RVU_BAR0(u64 a, u64 b, u64 c)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CAMX_W0_RVU_BAR0(u64 a, u64 b, u64 c)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3) && (c <= 1)))
+		return 0x1000010 + 0x400 * ((a) & 0x3ff) + 0x40 * ((b) & 0x3) + 8 * ((c) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_cam#_w1
+ *
+ * NPC AF MCAM Entry Bank Data Word 1 Registers
+ * MCAM comparison ternary data word 1. See NPC_AF_MCAME()_BANK()_CAM()_INTF.
+ */
+union cavm_npc_af_mcamex_bankx_camx_w1 {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_camx_w1_s {
+		u64 md                               : 48;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_camx_w1_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CAMX_W1_RVU_BAR0(u64 a, u64 b, u64 c)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CAMX_W1_RVU_BAR0(u64 a, u64 b, u64 c)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3) && (c <= 1)))
+		return 0x1000020 + 0x400 * ((a) & 0x3ff) + 0x40 * ((b) & 0x3) + 8 * ((c) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_cfg
+ *
+ * NPC AF MCAM Entry Bank Configuration Registers
+ */
+union cavm_npc_af_mcamex_bankx_cfg {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_cfg_s {
+		u64 ena                              : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_cfg_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CFG_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_CFG_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3)))
+		return 0x1800000 + 0x100 * ((a) & 0x3ff) + 0x10 * ((b) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_stat_act
+ *
+ * NPC AF MCAM Entry Bank Statistics Action Registers
+ * Used to optionally increment a NPC_AF_MATCH_STAT() counter when a packet
+ * matches an MCAM entry. See also NPC_AF_MCAME()_BANK()_ACTION.
+ */
+union cavm_npc_af_mcamex_bankx_stat_act {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_stat_act_s {
+		u64 stat_sel                         : 9;
+		u64 ena                              : 1;
+		u64 reserved_10_63                   : 54;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_stat_act_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_STAT_ACT_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_STAT_ACT_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3)))
+		return 0x1880000 + 0x100 * ((a) & 0x3ff) + 0x10 * ((b) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_mcame#_bank#_tag_act
+ *
+ * NPC AF MCAM Entry Bank VTag Action Data Registers
+ * Specifies a packet's match Vtag action captured in NPC_RESULT_S[VTAG_ACTION].
+ * See also NPC_AF_MCAME()_BANK()_ACTION.
+ */
+union cavm_npc_af_mcamex_bankx_tag_act {
+	u64 u;
+	struct cavm_npc_af_mcamex_bankx_tag_act_s {
+		u64 vtag_action                      : 64;
+	} s;
+	/* struct cavm_npc_af_mcamex_bankx_tag_act_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_TAG_ACT_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_MCAMEX_BANKX_TAG_ACT_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 1023) && (b <= 3)))
+		return 0x1900008 + 0x100 * ((a) & 0x3ff) + 0x10 * ((b) & 0x3);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pck_cfg
+ *
+ * NPC AF Protocol Check Configuration Register
+ */
+union cavm_npc_af_pck_cfg {
+	u64 u;
+	struct cavm_npc_af_pck_cfg_s {
+		u64 reserved_0                       : 1;
+		u64 iip4_cksum                       : 1;
+		u64 oip4_cksum                       : 1;
+		u64 reserved_3                       : 1;
+		u64 l3b                              : 1;
+		u64 l3m                              : 1;
+		u64 l2b                              : 1;
+		u64 l2m                              : 1;
+		u64 reserved_8_23                    : 16;
+		u64 iip4_cksum_errcode               : 8;
+		u64 oip4_cksum_errcode               : 8;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_npc_af_pck_cfg_s cn; */
+};
+
+#define CAVM_NPC_AF_PCK_CFG_RVU_BAR0 CAVM_NPC_AF_PCK_CFG_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_PCK_CFG_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PCK_CFG_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x600;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pck_def_iip4
+ *
+ * NPC AF Protocol Check Outer IPv4 Definition Register
+ * Provides layer information used by the protocol checker to identify an inner IPv4 header.
+ */
+union cavm_npc_af_pck_def_iip4 {
+	u64 u;
+	struct cavm_npc_af_pck_def_iip4_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_npc_af_pck_def_iip4_s cn; */
+};
+
+#define CAVM_NPC_AF_PCK_DEF_IIP4_RVU_BAR0 CAVM_NPC_AF_PCK_DEF_IIP4_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_PCK_DEF_IIP4_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PCK_DEF_IIP4_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x640;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pck_def_oip4
+ *
+ * NPC AF Protocol Check Outer IPv4 Definition Register
+ * Provides layer information used by the protocol checker to identify an outer IPv4 header.
+ */
+union cavm_npc_af_pck_def_oip4 {
+	u64 u;
+	struct cavm_npc_af_pck_def_oip4_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_npc_af_pck_def_oip4_s cn; */
+};
+
+#define CAVM_NPC_AF_PCK_DEF_OIP4_RVU_BAR0 CAVM_NPC_AF_PCK_DEF_OIP4_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_PCK_DEF_OIP4_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PCK_DEF_OIP4_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x620;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pck_def_oip6
+ *
+ * NPC AF Protocol Check Outer IPv4 Definition Register
+ * Provides layer information used by the protocol checker to identify an outer
+ * IPv6 header. [LID] must have the same value as NPC_AF_PCK_DEF_OIP4[LID].
+ */
+union cavm_npc_af_pck_def_oip6 {
+	u64 u;
+	struct cavm_npc_af_pck_def_oip6_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_npc_af_pck_def_oip6_s cn; */
+};
+
+#define CAVM_NPC_AF_PCK_DEF_OIP6_RVU_BAR0 CAVM_NPC_AF_PCK_DEF_OIP6_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_PCK_DEF_OIP6_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PCK_DEF_OIP6_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x630;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pck_def_ol2
+ *
+ * NPC AF Protocol Check Outer L2 Definition Register
+ * Provides layer information used by the protocol checker to identify an outer L2 header.
+ */
+union cavm_npc_af_pck_def_ol2 {
+	u64 u;
+	struct cavm_npc_af_pck_def_ol2_s {
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_11_63                   : 53;
+	} s;
+	/* struct cavm_npc_af_pck_def_ol2_s cn; */
+};
+
+#define CAVM_NPC_AF_PCK_DEF_OL2_RVU_BAR0 CAVM_NPC_AF_PCK_DEF_OL2_RVU_BAR0_FUNC()
+static inline u64 CAVM_NPC_AF_PCK_DEF_OL2_RVU_BAR0_FUNC(void)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PCK_DEF_OL2_RVU_BAR0_FUNC(void)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x610;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pkind#_action0
+ *
+ * NPC AF Port Kind Action Data 0 Registers
+ * NPC_AF_PKIND()_ACTION0 and NPC_AF_PKIND()_ACTION1 specify the initial parse state and
+ * operations to perform before entering KPU 0.
+ */
+union cavm_npc_af_pkindx_action0 {
+	u64 u;
+	struct cavm_npc_af_pkindx_action0_s {
+		u64 var_len_shift                    : 3;
+		u64 var_len_right                    : 1;
+		u64 var_len_mask                     : 8;
+		u64 var_len_offset                   : 8;
+		u64 ptr_advance                      : 8;
+		u64 capture_flags                    : 8;
+		u64 capture_ltype                    : 4;
+		u64 capture_lid                      : 3;
+		u64 reserved_43                      : 1;
+		u64 next_state                       : 8;
+		u64 parse_done                       : 1;
+		u64 capture_ena                      : 1;
+		u64 byp_count                        : 3;
+		u64 reserved_57_63                   : 7;
+	} s;
+	/* struct cavm_npc_af_pkindx_action0_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_PKINDX_ACTION0_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PKINDX_ACTION0_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0x80000 + 0x40 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pkind#_action1
+ *
+ * NPC AF Port Kind Action Data 1 Registers
+ * NPC_AF_PKIND()_ACTION0 and NPC_AF_PKIND()_ACTION1 specify the initial parse state and
+ * operations to perform before entering KPU 0.
+ */
+union cavm_npc_af_pkindx_action1 {
+	u64 u;
+	struct cavm_npc_af_pkindx_action1_s {
+		u64 dp0_offset                       : 8;
+		u64 dp1_offset                       : 8;
+		u64 dp2_offset                       : 8;
+		u64 errcode                          : 8;
+		u64 errlev                           : 4;
+		u64 reserved_36_63                   : 28;
+	} s;
+	/* struct cavm_npc_af_pkindx_action1_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_PKINDX_ACTION1_RVU_BAR0(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PKINDX_ACTION1_RVU_BAR0(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 63))
+		return 0x80008 + 0x40 * ((a) & 0x3f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) npc_af_pkind#_cpi_def#
+ *
+ * NPC AF Port Kind Channel Parse Index Definition Registers
+ * These registers specify the layer information and algorithm to compute a
+ * packet's channel parse index (CPI), which provides a port to channel adder for
+ * calculating NPC_RESULT_S[CHAN].
+ *
+ * There are two CPI definitions per port kind, allowing the CPI computation to
+ * use two possible layer definitions in the parsed packet, e.g. DiffServ DSCP
+ * from either IPv4 or IPv6 header.
+ *
+ * CPI pseudocode:
+ * \<pre\>
+ * for (i = 0; i \< 2; i++) {
+ *    cpi_def = NPC_AF_PKIND()_CPI_DEF(i);
+ *    LX = LA, LB, ..., or LH as selected by cpi_def[LID];
+ *
+ *    if (cpi_def[VALID]
+ *        && ((cpi_def[LTYPE_MATCH] & cpi_def[LTYPE_MASK])
+ *             == (NPC_RESULT_S[LX[LTYPE]] & cpi_def[LTYPE_MASK]))
+ *        && ((cpi_def[FLAGS_MATCH] & cpi_def[FLAGS_MASK])
+ *             == (NPC_RESULT_S[LX[FLAGS]] & cpi_def[FLAGS_MASK])))
+ *    {
+ *       // Found matching layer
+ *       nibble_offset = (2*NPC_RESULT_S[LX[LPTR]]) + cpi_def[ADD_OFFSET];
+ *       add_byte = byte at nibble_offset from start of packet;
+ *       cpi_add = (add_byte & cpi_def[ADD_MASK]) \>\> cpi_def[ADD_SHIFT];
+ *       cpi = cpi_def[CPI_BASE] + cpi_add;
+ *       NPC_RESULT_S[CHAN] += NPC_AF_CPI(cpi)_CFG[PADD];
+ *       break;
+ *    }
+ * }
+ * \</pre\>
+ */
+union cavm_npc_af_pkindx_cpi_defx {
+	u64 u;
+	struct cavm_npc_af_pkindx_cpi_defx_s {
+		u64 cpi_base                         : 10;
+		u64 reserved_10_11                   : 2;
+		u64 add_shift                        : 3;
+		u64 reserved_15                      : 1;
+		u64 add_mask                         : 8;
+		u64 add_offset                       : 8;
+		u64 flags_mask                       : 8;
+		u64 flags_match                      : 8;
+		u64 ltype_mask                       : 4;
+		u64 ltype_match                      : 4;
+		u64 lid                              : 3;
+		u64 reserved_59_62                   : 4;
+		u64 ena                              : 1;
+	} s;
+	/* struct cavm_npc_af_pkindx_cpi_defx_s cn; */
+};
+
+static inline u64 CAVM_NPC_AF_PKINDX_CPI_DEFX_RVU_BAR0(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_NPC_AF_PKINDX_CPI_DEFX_RVU_BAR0(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 63) && (b <= 1)))
+		return 0x80020 + 0x40 * ((a) & 0x3f) + 8 * ((b) & 0x1);
+	return -1;
+}
+
+#endif /* __CAVM_CSRS_NPC_H__ */
diff --git a/drivers/net/cavium/octeontx2/cavm-csrs-rvu.h b/drivers/net/cavium/octeontx2/cavm-csrs-rvu.h
new file mode 100644
index 0000000000..db7c3c6268
--- /dev/null
+++ b/drivers/net/cavium/octeontx2/cavm-csrs-rvu.h
@@ -0,0 +1,2465 @@
+#ifndef __CAVM_CSRS_RVU_H__
+#define __CAVM_CSRS_RVU_H__
+/* This file is auto-generated. Do not edit */
+
+/***********************license start***************
+ * Copyright (c) 2003-2018  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM  NETWORKS MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Configuration and status register (CSR) address and type definitions for
+ * Cavium RVU.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ */
+
+/**
+ * Enumeration rvu_af_int_vec_e
+ *
+ * RVU Admin Function Interrupt Vector Enumeration
+ * Enumerates the MSI-X interrupt vectors.
+ * Internal:
+ * RVU maintains the state of these vectors internally, and generates GIB
+ * messages for it without accessing the MSI-X table region in LLC/DRAM.
+ */
+#define CAVM_RVU_AF_INT_VEC_E_GEN (3)
+#define CAVM_RVU_AF_INT_VEC_E_MBOX (4)
+#define CAVM_RVU_AF_INT_VEC_E_PFFLR (1)
+#define CAVM_RVU_AF_INT_VEC_E_PFME (2)
+#define CAVM_RVU_AF_INT_VEC_E_POISON (0)
+
+/**
+ * Enumeration rvu_bar_e
+ *
+ * RVU Base Address Register Enumeration
+ * Enumerates the base address registers.
+ * Internal:
+ * For documentation only.
+ */
+#define CAVM_RVU_BAR_E_RVU_PFX_BAR0(a) (0x840000000000ll + 0x1000000000ll * (a))
+#define CAVM_RVU_BAR_E_RVU_PFX_BAR0_SIZE 0x10000000ull
+#define CAVM_RVU_BAR_E_RVU_PFX_FUNCX_BAR2(a, b)	\
+	(0x840200000000ll + 0x1000000000ll * (a) + 0x2000000ll * (b))
+#define CAVM_RVU_BAR_E_RVU_PFX_FUNCX_BAR2_SIZE 0x100000ull
+
+/**
+ * Enumeration rvu_block_addr_e
+ *
+ * RVU Block Address Enumeration
+ * Enumerates addressing of RVU resource blocks within each RVU BAR, i.e. values
+ * of RVU_FUNC_ADDR_S[BLOCK] and RVU_AF_ADDR_S[BLOCK].
+ */
+#define CAVM_RVU_BLOCK_ADDR_E_CPTX(a) (0xa + (a))
+#define CAVM_RVU_BLOCK_ADDR_E_LMT (1)
+#define CAVM_RVU_BLOCK_ADDR_E_NDCX(a) (0xc + (a))
+#define CAVM_RVU_BLOCK_ADDR_E_NIXX(a) (4 + (a))
+#define CAVM_RVU_BLOCK_ADDR_E_NPA (3)
+#define CAVM_RVU_BLOCK_ADDR_E_NPC (6)
+#define CAVM_RVU_BLOCK_ADDR_E_RX(a) (0 + (a))
+#define CAVM_RVU_BLOCK_ADDR_E_RVUM (0)
+#define CAVM_RVU_BLOCK_ADDR_E_SSO (7)
+#define CAVM_RVU_BLOCK_ADDR_E_SSOW (8)
+#define CAVM_RVU_BLOCK_ADDR_E_TIM (9)
+
+/**
+ * Enumeration rvu_block_type_e
+ *
+ * RVU Block Type Enumeration
+ * Enumerates values of RVU_PF/RVU_VF_BLOCK_ADDR()_DISC[BTYPE].
+ */
+#define CAVM_RVU_BLOCK_TYPE_E_CPT (9)
+#define CAVM_RVU_BLOCK_TYPE_E_DDF (0xb)
+#define CAVM_RVU_BLOCK_TYPE_E_DFA (0xe)
+#define CAVM_RVU_BLOCK_TYPE_E_HNA (0xf)
+#define CAVM_RVU_BLOCK_TYPE_E_LMT (2)
+#define CAVM_RVU_BLOCK_TYPE_E_NDC (0xa)
+#define CAVM_RVU_BLOCK_TYPE_E_NIX (3)
+#define CAVM_RVU_BLOCK_TYPE_E_NPA (4)
+#define CAVM_RVU_BLOCK_TYPE_E_NPC (5)
+#define CAVM_RVU_BLOCK_TYPE_E_RAD (0xd)
+#define CAVM_RVU_BLOCK_TYPE_E_RVUM (0)
+#define CAVM_RVU_BLOCK_TYPE_E_SSO (6)
+#define CAVM_RVU_BLOCK_TYPE_E_SSOW (7)
+#define CAVM_RVU_BLOCK_TYPE_E_TIM (8)
+#define CAVM_RVU_BLOCK_TYPE_E_ZIP (0xc)
+
+/**
+ * Enumeration rvu_bus_lf_e
+ *
+ * INTERNAL: RVU Bus LF Range Enumeration
+ *
+ * Enumerates the LF range for the RVU bus.
+ * Internal:
+ * This is an enum used in csr3 virtual equations.
+ */
+#define CAVM_RVU_BUS_LF_E_RVU_BUS_LFX(a) (0 + 0x2000000 * (a))
+
+/**
+ * Enumeration rvu_bus_lf_slot_e
+ *
+ * INTERNAL: RVU Bus LF Slot Range Enumeration
+ *
+ * Enumerates the LF and Slot range for the RVU bus.
+ * Internal:
+ * This is an enum used in csr3 virtual equations.
+ */
+#define CAVM_RVU_BUS_LF_SLOT_E_RVU_BUS_LFX_SLOTX(a, b)	\
+	(0 + 0x2000000 * (a) + 0x1000 * (b))
+
+/**
+ * Enumeration rvu_bus_pf_e
+ *
+ * INTERNAL: RVU Bus PF Range Enumeration
+ *
+ * Enumerates the PF range for the RVU bus.
+ * Internal:
+ * This is an enum used in csr3 virtual equations.
+ */
+#define CAVM_RVU_BUS_PF_E_RVU_BUS_PFX(a) (0ll + 0x1000000000ll * (a))
+
+/**
+ * Enumeration rvu_bus_pfvf_e
+ *
+ * INTERNAL: RVU Bus PFVF Range Enumeration
+ *
+ * Enumerates the PF and VF ranges for the RVU bus.
+ * Internal:
+ * This is an enum used in csr3 virtual equations.
+ */
+#define CAVM_RVU_BUS_PFVF_E_RVU_BUS_PFX(a) (0 + 0x2000000 * (a))
+#define CAVM_RVU_BUS_PFVF_E_RVU_BUS_VFX(a) (0 + 0x2000000 * (a))
+
+/**
+ * Enumeration rvu_busbar_e
+ *
+ * INTERNAL: RVU Bus Base Address Region Enumeration
+ *
+ * Enumerates the base address region for the RVU bus.
+ * Internal:
+ * This is an enum used in csr3 virtual equations.
+ */
+#define CAVM_RVU_BUSBAR_E_RVU_BUSBAR0 (0)
+#define CAVM_RVU_BUSBAR_E_RVU_BUSBAR2 (0x200000000ll)
+
+/**
+ * Enumeration rvu_busdid_e
+ *
+ * INTERNAL: RVU Bus DID Enumeration
+ *
+ * Enumerates the DID offset for the RVU bus.
+ * Internal:
+ * This is an enum used in csr3 virtual equations.
+ */
+#define CAVM_RVU_BUSDID_E_RVU_BUSDID (0x840000000000ll)
+
+/**
+ * Enumeration rvu_pf_int_vec_e
+ *
+ * RVU PF Interrupt Vector Enumeration
+ * Enumerates the MSI-X interrupt vectors.
+ */
+#define CAVM_RVU_PF_INT_VEC_E_AFPF_MBOX (6)
+#define CAVM_RVU_PF_INT_VEC_E_VFFLRX(a) (0 + (a))
+#define CAVM_RVU_PF_INT_VEC_E_VFMEX(a) (2 + (a))
+#define CAVM_RVU_PF_INT_VEC_E_VFPF_MBOXX(a) (4 + (a))
+
+/**
+ * Enumeration rvu_vf_int_vec_e
+ *
+ * RVU VF Interrupt Vector Enumeration
+ * Enumerates the MSI-X interrupt vectors.
+ */
+#define CAVM_RVU_VF_INT_VEC_E_MBOX (0)
+
+/**
+ * Structure rvu_af_addr_s
+ *
+ * RVU Admin Function Register Address Structure
+ * Address format for accessing shared Admin Function (AF) registers in
+ * RVU PF BAR0. These registers may be accessed by all RVU PFs whose
+ * RVU_PRIV_PF()_CFG[AF_ENA] bit is set.
+ */
+union cavm_rvu_af_addr_s {
+	u64 u;
+	struct cavm_rvu_af_addr_s_s {
+		u64 addr                             : 28;
+		u64 block                            : 5;
+		u64 reserved_33_63                   : 31;
+	} s;
+	/* struct cavm_rvu_af_addr_s_s cn; */
+};
+
+/**
+ * Structure rvu_func_addr_s
+ *
+ * RVU Function-unique Address Structure
+ * Address format for accessing function-unique registers in RVU PF/FUNC BAR2.
+ */
+union cavm_rvu_func_addr_s {
+	u32 u;
+	struct cavm_rvu_func_addr_s_s {
+		u32 addr                             : 12;
+		u32 lf_slot                          : 8;
+		u32 block                            : 5;
+		u32 reserved_25_31                   : 7;
+	} s;
+	/* struct cavm_rvu_func_addr_s_s cn; */
+};
+
+/**
+ * Structure rvu_msix_vec_s
+ *
+ * RVU MSI-X Vector Structure
+ * Format of entries in the RVU MSI-X table region in LLC/DRAM. See
+ * RVU_PRIV_PF()_MSIX_CFG.
+ */
+union cavm_rvu_msix_vec_s {
+	u64 u[2];
+	struct cavm_rvu_msix_vec_s_s {
+		u64 addr                             : 64;
+		u64 data                             : 32;
+		u64 mask                             : 1;
+		u64 pend                             : 1;
+		u64 reserved_98_127                  : 30;
+	} s;
+	/* struct cavm_rvu_msix_vec_s_s cn; */
+};
+
+/**
+ * Structure rvu_pf_func_s
+ *
+ * RVU PF Function Identification Structure
+ * Identifies an RVU PF/VF, and format of *_PRIV_LF()_CFG[PF_FUNC] in RVU
+ * resource blocks, e.g. NPA_PRIV_LF()_CFG[PF_FUNC].
+ *
+ * Internal:
+ * Also used for PF/VF identification on inter-coprocessor hardware
+ * interfaces (NPA, SSO, CPT, ...).
+ */
+union cavm_rvu_pf_func_s {
+	u32 u;
+	struct cavm_rvu_pf_func_s_s {
+		u32 func                             : 10;
+		u32 pf                               : 6;
+		u32 reserved_16_31                   : 16;
+	} s;
+	/* struct cavm_rvu_pf_func_s_s cn; */
+};
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_afpf#_mbox#
+ *
+ * RVU Admin Function AF/PF Mailbox Registers
+ */
+union cavm_rvu_af_afpfx_mboxx {
+	u64 u;
+	struct cavm_rvu_af_afpfx_mboxx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_rvu_af_afpfx_mboxx_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_AFPFX_MBOXX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_AFPFX_MBOXX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b <= 1)))
+		return 0x2000 + 0x10 * ((a) & 0xf) + 8 * ((b) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_bar2_alias#
+ *
+ * INTERNAL: RVU Admin Function  BAR2 Alias Registers
+ *
+ * These registers alias to the RVU BAR2 registers for the PF and function
+ * selected by RVU_AF_BAR2_SEL[PF_FUNC].
+ *
+ * Internal:
+ * Not implemented. Placeholder for bug33464.
+ */
+union cavm_rvu_af_bar2_aliasx {
+	u64 u;
+	struct cavm_rvu_af_bar2_aliasx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_rvu_af_bar2_aliasx_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_BAR2_ALIASX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_BAR2_ALIASX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 131071))
+		return 0x9100000 + 8 * ((a) & 0x1ffff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_bar2_sel
+ *
+ * INTERNAL: RVU Admin Function BAR2 Select Register
+ *
+ * This register configures BAR2 accesses from the RVU_AF_BAR2_ALIAS() registers in BAR0.
+ * Internal:
+ * Not implemented. Placeholder for bug33464.
+ */
+union cavm_rvu_af_bar2_sel {
+	u64 u;
+	struct cavm_rvu_af_bar2_sel_s {
+		u64 alias_pf_func                    : 16;
+		u64 alias_ena                        : 1;
+		u64 reserved_17_63                   : 47;
+	} s;
+	/* struct cavm_rvu_af_bar2_sel_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_BAR2_SEL
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_BAR2_SEL
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x9000000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_blk_rst
+ *
+ * RVU Master Admin Function Block Reset Register
+ */
+union cavm_rvu_af_blk_rst {
+	u64 u;
+	struct cavm_rvu_af_blk_rst_s {
+		u64 rst                              : 1;
+		u64 reserved_1_62                    : 62;
+		u64 busy                             : 1;
+	} s;
+	/* struct cavm_rvu_af_blk_rst_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_BLK_RST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_BLK_RST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x30;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_eco
+ *
+ * INTERNAL: RVU Admin Function ECO Register
+ */
+union cavm_rvu_af_eco {
+	u64 u;
+	struct cavm_rvu_af_eco_s {
+		u64 eco_rw                           : 32;
+		u64 reserved_32_63                   : 32;
+	} s;
+	/* struct cavm_rvu_af_eco_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_ECO
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_ECO
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x20;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_gen_int
+ *
+ * RVU Admin Function General Interrupt Register
+ * This register contains General interrupt summary bits.
+ */
+union cavm_rvu_af_gen_int {
+	u64 u;
+	struct cavm_rvu_af_gen_int_s {
+		u64 unmapped                         : 1;
+		u64 msix_fault                       : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_rvu_af_gen_int_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_GEN_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_GEN_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x120;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_gen_int_ena_w1c
+ *
+ * RVU Admin Function General Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_af_gen_int_ena_w1c {
+	u64 u;
+	struct cavm_rvu_af_gen_int_ena_w1c_s {
+		u64 unmapped                         : 1;
+		u64 msix_fault                       : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_rvu_af_gen_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_GEN_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_GEN_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x138;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_gen_int_ena_w1s
+ *
+ * RVU Admin Function General Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_af_gen_int_ena_w1s {
+	u64 u;
+	struct cavm_rvu_af_gen_int_ena_w1s_s {
+		u64 unmapped                         : 1;
+		u64 msix_fault                       : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_rvu_af_gen_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_GEN_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_GEN_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x130;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_gen_int_w1s
+ *
+ * RVU Admin Function General Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_af_gen_int_w1s {
+	u64 u;
+	struct cavm_rvu_af_gen_int_w1s_s {
+		u64 unmapped                         : 1;
+		u64 msix_fault                       : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_rvu_af_gen_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_GEN_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_GEN_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x128;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_hwvf_rst
+ *
+ * RVU Admin Function Hardware VF Reset Register
+ */
+union cavm_rvu_af_hwvf_rst {
+	u64 u;
+	struct cavm_rvu_af_hwvf_rst_s {
+		u64 hwvf                             : 8;
+		u64 reserved_8_11                    : 4;
+		u64 exec                             : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_rvu_af_hwvf_rst_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_HWVF_RST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_HWVF_RST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2850;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_msixtr_base
+ *
+ * RVU Admin Function MSI-X Table Region Base-Address Register
+ */
+union cavm_rvu_af_msixtr_base {
+	u64 u;
+	struct cavm_rvu_af_msixtr_base_s {
+		u64 reserved_0_6                     : 7;
+		u64 addr                             : 46;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_rvu_af_msixtr_base_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_MSIXTR_BASE
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_MSIXTR_BASE
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pf_bar4_addr
+ *
+ * RVU Admin Function PF BAR4 Address Registers
+ */
+union cavm_rvu_af_pf_bar4_addr {
+	u64 u;
+	struct cavm_rvu_af_pf_bar4_addr_s {
+		u64 reserved_0_15                    : 16;
+		u64 addr                             : 48;
+	} s;
+	/* struct cavm_rvu_af_pf_bar4_addr_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PF_BAR4_ADDR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PF_BAR4_ADDR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x40;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pf_rst
+ *
+ * RVU Admin Function PF Reset Register
+ */
+union cavm_rvu_af_pf_rst {
+	u64 u;
+	struct cavm_rvu_af_pf_rst_s {
+		u64 pf                               : 4;
+		u64 reserved_4_11                    : 8;
+		u64 exec                             : 1;
+		u64 reserved_13_63                   : 51;
+	} s;
+	/* struct cavm_rvu_af_pf_rst_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PF_RST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PF_RST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2840;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfaf_mbox_int
+ *
+ * RVU Admin Function PF to AF Mailbox Interrupt Registers
+ */
+union cavm_rvu_af_pfaf_mbox_int {
+	u64 u;
+	struct cavm_rvu_af_pfaf_mbox_int_s {
+		u64 mbox                             : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfaf_mbox_int_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2880;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfaf_mbox_int_ena_w1c
+ *
+ * RVU Admin Function PF to AF Mailbox Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_af_pfaf_mbox_int_ena_w1c {
+	u64 u;
+	struct cavm_rvu_af_pfaf_mbox_int_ena_w1c_s {
+		u64 mbox                             : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfaf_mbox_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2898;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfaf_mbox_int_ena_w1s
+ *
+ * RVU Admin Function PF to AF Mailbox Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_af_pfaf_mbox_int_ena_w1s {
+	u64 u;
+	struct cavm_rvu_af_pfaf_mbox_int_ena_w1s_s {
+		u64 mbox                             : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfaf_mbox_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2890;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfaf_mbox_int_w1s
+ *
+ * RVU Admin Function PF to AF Mailbox Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_af_pfaf_mbox_int_w1s {
+	u64 u;
+	struct cavm_rvu_af_pfaf_mbox_int_w1s_s {
+		u64 mbox                             : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfaf_mbox_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFAF_MBOX_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2888;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfflr_int
+ *
+ * RVU Admin Function PF Function Level Reset Interrupt Registers
+ */
+union cavm_rvu_af_pfflr_int {
+	u64 u;
+	struct cavm_rvu_af_pfflr_int_s {
+		u64 flr                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfflr_int_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFFLR_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFFLR_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28a0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfflr_int_ena_w1c
+ *
+ * RVU Admin Function PF Function Level Reset Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_af_pfflr_int_ena_w1c {
+	u64 u;
+	struct cavm_rvu_af_pfflr_int_ena_w1c_s {
+		u64 flr                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfflr_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFFLR_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFFLR_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28b8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfflr_int_ena_w1s
+ *
+ * RVU Admin Function PF Function Level Reset Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_af_pfflr_int_ena_w1s {
+	u64 u;
+	struct cavm_rvu_af_pfflr_int_ena_w1s_s {
+		u64 flr                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfflr_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFFLR_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFFLR_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28b0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfflr_int_w1s
+ *
+ * RVU Admin Function PF Function Level Reset Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_af_pfflr_int_w1s {
+	u64 u;
+	struct cavm_rvu_af_pfflr_int_w1s_s {
+		u64 flr                              : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfflr_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFFLR_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFFLR_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28a8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfme_int
+ *
+ * RVU Admin Function PF Bus Master Enable Interrupt Registers
+ */
+union cavm_rvu_af_pfme_int {
+	u64 u;
+	struct cavm_rvu_af_pfme_int_s {
+		u64 me                               : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfme_int_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFME_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFME_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28c0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfme_int_ena_w1c
+ *
+ * RVU Admin Function PF Bus Master Enable Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_af_pfme_int_ena_w1c {
+	u64 u;
+	struct cavm_rvu_af_pfme_int_ena_w1c_s {
+		u64 me                               : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfme_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFME_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFME_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28d8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfme_int_ena_w1s
+ *
+ * RVU Admin Function PF Bus Master Enable Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_af_pfme_int_ena_w1s {
+	u64 u;
+	struct cavm_rvu_af_pfme_int_ena_w1s_s {
+		u64 me                               : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfme_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFME_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFME_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28d0;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfme_int_w1s
+ *
+ * RVU Admin Function PF Bus Master Enable Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_af_pfme_int_w1s {
+	u64 u;
+	struct cavm_rvu_af_pfme_int_w1s_s {
+		u64 me                               : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfme_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFME_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFME_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28c8;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pfme_status
+ *
+ * RVU Admin Function PF Bus Master Enable Status Registers
+ */
+union cavm_rvu_af_pfme_status {
+	u64 u;
+	struct cavm_rvu_af_pfme_status_s {
+		u64 me                               : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pfme_status_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFME_STATUS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFME_STATUS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2800;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pftrpend
+ *
+ * RVU Admin Function PF Transaction Pending Registers
+ */
+union cavm_rvu_af_pftrpend {
+	u64 u;
+	struct cavm_rvu_af_pftrpend_s {
+		u64 trpend                           : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pftrpend_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFTRPEND
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFTRPEND
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2810;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_pftrpend_w1s
+ *
+ * RVU Admin Function PF Transaction Pending Set Registers
+ * This register reads or sets bits.
+ */
+union cavm_rvu_af_pftrpend_w1s {
+	u64 u;
+	struct cavm_rvu_af_pftrpend_w1s_s {
+		u64 trpend                           : 16;
+		u64 reserved_16_63                   : 48;
+	} s;
+	/* struct cavm_rvu_af_pftrpend_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_PFTRPEND_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_PFTRPEND_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x2820;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_ras
+ *
+ * RVU Admin Function RAS Interrupt Register
+ * This register is intended for delivery of RAS events to the SCP, so should be
+ * ignored by OS drivers.
+ */
+union cavm_rvu_af_ras {
+	u64 u;
+	struct cavm_rvu_af_ras_s {
+		u64 msix_poison                      : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_af_ras_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_RAS
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_RAS
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x100;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_ras_ena_w1c
+ *
+ * RVU Admin Function RAS Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_af_ras_ena_w1c {
+	u64 u;
+	struct cavm_rvu_af_ras_ena_w1c_s {
+		u64 msix_poison                      : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_af_ras_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_RAS_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_RAS_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x118;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_ras_ena_w1s
+ *
+ * RVU Admin Function RAS Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_af_ras_ena_w1s {
+	u64 u;
+	struct cavm_rvu_af_ras_ena_w1s_s {
+		u64 msix_poison                      : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_af_ras_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_RAS_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_RAS_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x110;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_af_ras_w1s
+ *
+ * RVU Admin Function RAS Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_af_ras_w1s {
+	u64 u;
+	struct cavm_rvu_af_ras_w1s_s {
+		u64 msix_poison                      : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_af_ras_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_AF_RAS_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_AF_RAS_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x108;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_block_addr#_disc
+ *
+ * RVU PF Block Address Discovery Registers
+ * These registers allow each PF driver to discover block resources that are
+ * provisioned to its PF. The register's block address index is enumerated by
+ * RVU_BLOCK_ADDR_E.
+ */
+union cavm_rvu_pf_block_addrx_disc {
+	u64 u;
+	struct cavm_rvu_pf_block_addrx_disc_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_10                    : 2;
+		u64 imp                              : 1;
+		u64 rid                              : 8;
+		u64 btype                            : 8;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_rvu_pf_block_addrx_disc_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_BLOCK_ADDRX_DISC(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_BLOCK_ADDRX_DISC(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 31))
+		return 0x200 + 8 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_int
+ *
+ * RVU PF Interrupt Registers
+ */
+union cavm_rvu_pf_int {
+	u64 u;
+	struct cavm_rvu_pf_int_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_pf_int_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc20;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_int_ena_w1c
+ *
+ * RVU PF Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_pf_int_ena_w1c {
+	u64 u;
+	struct cavm_rvu_pf_int_ena_w1c_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_pf_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc38;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_int_ena_w1s
+ *
+ * RVU PF Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_pf_int_ena_w1s {
+	u64 u;
+	struct cavm_rvu_pf_int_ena_w1s_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_pf_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc30;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_int_w1s
+ *
+ * RVU PF Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_pf_int_w1s {
+	u64 u;
+	struct cavm_rvu_pf_int_w1s_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_pf_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0xc28;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_msix_pba#
+ *
+ * RVU PF MSI-X Pending-Bit-Array Registers
+ * This register is the MSI-X PF PBA table.
+ */
+union cavm_rvu_pf_msix_pbax {
+	u64 u;
+	struct cavm_rvu_pf_msix_pbax_s {
+		u64 pend                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_msix_pbax_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_MSIX_PBAX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_MSIX_PBAX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0xf0000 + 8 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_msix_vec#_addr
+ *
+ * RVU PF MSI-X Vector-Table Address Registers
+ * These registers and RVU_PF_MSIX_VEC()_CTL form the PF MSI-X vector table.
+ * The number of MSI-X vectors for a given PF is specified by
+ * RVU_PRIV_PF()_MSIX_CFG[PF_MSIXT_SIZEM1] (plus 1).
+ *
+ * Internal:
+ * PF vector count of 256 is sized to allow up to 120 for AF, 4 for PF/VF
+ * mailboxes, and 128 for LF resources from various blocks that are directly
+ * provisioned to the PF.
+ */
+union cavm_rvu_pf_msix_vecx_addr {
+	u64 u;
+	struct cavm_rvu_pf_msix_vecx_addr_s {
+		u64 secvec                           : 1;
+		u64 reserved_1                       : 1;
+		u64 addr                             : 51;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_rvu_pf_msix_vecx_addr_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_MSIX_VECX_ADDR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_MSIX_VECX_ADDR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0x80000 + 0x10 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_msix_vec#_ctl
+ *
+ * RVU PF MSI-X Vector-Table Control and Data Registers
+ * These registers and RVU_PF_MSIX_VEC()_ADDR form the PF MSI-X vector table.
+ */
+union cavm_rvu_pf_msix_vecx_ctl {
+	u64 u;
+	struct cavm_rvu_pf_msix_vecx_ctl_s {
+		u64 data                             : 32;
+		u64 mask                             : 1;
+		u64 reserved_33_63                   : 31;
+	} s;
+	/* struct cavm_rvu_pf_msix_vecx_ctl_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_MSIX_VECX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_MSIX_VECX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0x80008 + 0x10 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_pfaf_mbox#
+ *
+ * RVU PF/AF Mailbox Registers
+ */
+union cavm_rvu_pf_pfaf_mboxx {
+	u64 u;
+	struct cavm_rvu_pf_pfaf_mboxx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_pfaf_mboxx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_PFAF_MBOXX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_PFAF_MBOXX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0xc00 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vf#_pfvf_mbox#
+ *
+ * RVU PF/VF Mailbox Registers
+ */
+union cavm_rvu_pf_vfx_pfvf_mboxx {
+	u64 u;
+	struct cavm_rvu_pf_vfx_pfvf_mboxx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfx_pfvf_mboxx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFX_PFVF_MBOXX(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFX_PFVF_MBOXX(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 127) && (b <= 1)))
+		return 0 + 0x1000 * ((a) & 0x7f) + 8 * ((b) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vf_bar4_addr
+ *
+ * RVU PF VF BAR4 Address Registers
+ */
+union cavm_rvu_pf_vf_bar4_addr {
+	u64 u;
+	struct cavm_rvu_pf_vf_bar4_addr_s {
+		u64 reserved_0_15                    : 16;
+		u64 addr                             : 48;
+	} s;
+	/* struct cavm_rvu_pf_vf_bar4_addr_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VF_BAR4_ADDR
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VF_BAR4_ADDR
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x10;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfflr_int#
+ *
+ * RVU PF VF Function Level Reset Interrupt Registers
+ */
+union cavm_rvu_pf_vfflr_intx {
+	u64 u;
+	struct cavm_rvu_pf_vfflr_intx_s {
+		u64 flr                              : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfflr_intx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFFLR_INTX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFFLR_INTX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x900 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfflr_int_ena_w1c#
+ *
+ * RVU PF VF Function Level Reset Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_pf_vfflr_int_ena_w1cx {
+	u64 u;
+	struct cavm_rvu_pf_vfflr_int_ena_w1cx_s {
+		u64 flr                              : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfflr_int_ena_w1cx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFFLR_INT_ENA_W1CX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFFLR_INT_ENA_W1CX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x960 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfflr_int_ena_w1s#
+ *
+ * RVU PF VF Function Level Reset Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_pf_vfflr_int_ena_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vfflr_int_ena_w1sx_s {
+		u64 flr                              : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfflr_int_ena_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFFLR_INT_ENA_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFFLR_INT_ENA_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x940 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfflr_int_w1s#
+ *
+ * RVU PF VF Function Level Reset Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_pf_vfflr_int_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vfflr_int_w1sx_s {
+		u64 flr                              : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfflr_int_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFFLR_INT_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFFLR_INT_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x920 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfme_int#
+ *
+ * RVU PF VF Bus Master Enable Interrupt Registers
+ */
+union cavm_rvu_pf_vfme_intx {
+	u64 u;
+	struct cavm_rvu_pf_vfme_intx_s {
+		u64 me                               : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfme_intx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFME_INTX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFME_INTX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x980 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfme_int_ena_w1c#
+ *
+ * RVU PF VF Bus Master Enable Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_pf_vfme_int_ena_w1cx {
+	u64 u;
+	struct cavm_rvu_pf_vfme_int_ena_w1cx_s {
+		u64 me                               : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfme_int_ena_w1cx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFME_INT_ENA_W1CX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFME_INT_ENA_W1CX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x9e0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfme_int_ena_w1s#
+ *
+ * RVU PF VF Bus Master Enable Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_pf_vfme_int_ena_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vfme_int_ena_w1sx_s {
+		u64 me                               : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfme_int_ena_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFME_INT_ENA_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFME_INT_ENA_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x9c0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfme_int_w1s#
+ *
+ * RVU PF VF Bus Master Enable Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_pf_vfme_int_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vfme_int_w1sx_s {
+		u64 me                               : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfme_int_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFME_INT_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFME_INT_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x9a0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfme_status#
+ *
+ * RVU PF VF Bus Master Enable Status Registers
+ */
+union cavm_rvu_pf_vfme_statusx {
+	u64 u;
+	struct cavm_rvu_pf_vfme_statusx_s {
+		u64 me                               : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfme_statusx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFME_STATUSX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFME_STATUSX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x800 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfpf_mbox_int#
+ *
+ * RVU VF to PF Mailbox Interrupt Registers
+ */
+union cavm_rvu_pf_vfpf_mbox_intx {
+	u64 u;
+	struct cavm_rvu_pf_vfpf_mbox_intx_s {
+		u64 mbox                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfpf_mbox_intx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INTX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INTX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x880 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfpf_mbox_int_ena_w1c#
+ *
+ * RVU VF to PF Mailbox Interrupt Enable Clear Registers
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_pf_vfpf_mbox_int_ena_w1cx {
+	u64 u;
+	struct cavm_rvu_pf_vfpf_mbox_int_ena_w1cx_s {
+		u64 mbox                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfpf_mbox_int_ena_w1cx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INT_ENA_W1CX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INT_ENA_W1CX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x8e0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfpf_mbox_int_ena_w1s#
+ *
+ * RVU VF to PF Mailbox Interrupt Enable Set Registers
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_pf_vfpf_mbox_int_ena_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vfpf_mbox_int_ena_w1sx_s {
+		u64 mbox                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfpf_mbox_int_ena_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INT_ENA_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INT_ENA_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x8c0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vfpf_mbox_int_w1s#
+ *
+ * RVU VF to PF Mailbox Interrupt Set Registers
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_pf_vfpf_mbox_int_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vfpf_mbox_int_w1sx_s {
+		u64 mbox                             : 64;
+	} s;
+	/* struct cavm_rvu_pf_vfpf_mbox_int_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INT_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFPF_MBOX_INT_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x8a0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vftrpend#
+ *
+ * RVU PF VF Transaction Pending Registers
+ */
+union cavm_rvu_pf_vftrpendx {
+	u64 u;
+	struct cavm_rvu_pf_vftrpendx_s {
+		u64 trpend                           : 64;
+	} s;
+	/* struct cavm_rvu_pf_vftrpendx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFTRPENDX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFTRPENDX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x820 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR2) rvu_pf_vftrpend_w1s#
+ *
+ * RVU PF VF Transaction Pending Set Registers
+ * This register reads or sets bits.
+ */
+union cavm_rvu_pf_vftrpend_w1sx {
+	u64 u;
+	struct cavm_rvu_pf_vftrpend_w1sx_s {
+		u64 trpend                           : 64;
+	} s;
+	/* struct cavm_rvu_pf_vftrpend_w1sx_s cn; */
+};
+
+static inline u64 CAVM_RVU_PF_VFTRPEND_W1SX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PF_VFTRPEND_W1SX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0x840 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_active_pc
+ *
+ * RVU Active Program Counter Register
+ */
+union cavm_rvu_priv_active_pc {
+	u64 u;
+	struct cavm_rvu_priv_active_pc_s {
+		u64 active_pc                        : 64;
+	} s;
+	/* struct cavm_rvu_priv_active_pc_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_ACTIVE_PC
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_ACTIVE_PC
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8000030;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_block_type#_rev
+ *
+ * RVU Privileged Block Type Revision Registers
+ * These registers are used by configuration software to specify the revision ID
+ * of each block type enumerated by RVU_BLOCK_TYPE_E, to assist VF/PF software
+ * discovery.
+ */
+union cavm_rvu_priv_block_typex_rev {
+	u64 u;
+	struct cavm_rvu_priv_block_typex_rev_s {
+		u64 rid                              : 8;
+		u64 reserved_8_63                    : 56;
+	} s;
+	/* struct cavm_rvu_priv_block_typex_rev_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_BLOCK_TYPEX_REV(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_BLOCK_TYPEX_REV(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000400 + 8 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_clk_cfg
+ *
+ * RVU Privileged General Configuration Register
+ */
+union cavm_rvu_priv_clk_cfg {
+	u64 u;
+	struct cavm_rvu_priv_clk_cfg_s {
+		u64 blk_clken                        : 1;
+		u64 ncbi_clken                       : 1;
+		u64 reserved_2_63                    : 62;
+	} s;
+	/* struct cavm_rvu_priv_clk_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_CLK_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_CLK_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8000020;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_const
+ *
+ * RVU Privileged Constants Register
+ * This register contains constants for software discovery.
+ */
+union cavm_rvu_priv_const {
+	u64 u;
+	struct cavm_rvu_priv_const_s {
+		u64 max_msix                         : 20;
+		u64 hwvfs                            : 12;
+		u64 pfs                              : 8;
+		u64 max_vfs_per_pf                   : 8;
+		u64 reserved_48_63                   : 16;
+	} s;
+	/* struct cavm_rvu_priv_const_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_CONST
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_CONST
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8000000;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_gen_cfg
+ *
+ * RVU Privileged General Configuration Register
+ */
+union cavm_rvu_priv_gen_cfg {
+	u64 u;
+	struct cavm_rvu_priv_gen_cfg_s {
+		u64 lock                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_priv_gen_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_GEN_CFG
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_GEN_CFG
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x8000010;
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_cpt#_cfg
+ *
+ * RVU Privileged Hardware VF CPT Configuration Registers
+ * Similar to RVU_PRIV_HWVF()_NIX()_CFG, but for CPT({a}) block.
+ */
+union cavm_rvu_priv_hwvfx_cptx_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_cptx_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_cptx_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_CPTX_CFG(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_CPTX_CFG(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 255) && (b == 0)))
+		return 0x8001350 + 0x10000 * ((a) & 0xff) + 8 * ((b) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_int_cfg
+ *
+ * RVU Privileged Hardware VF Interrupt Configuration Registers
+ */
+union cavm_rvu_priv_hwvfx_int_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_int_cfg_s {
+		u64 msix_offset                      : 11;
+		u64 reserved_11                      : 1;
+		u64 msix_size                        : 8;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_int_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_INT_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_INT_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x8001280 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_nix#_cfg
+ *
+ * RVU Privileged Hardware VF NIX Configuration Registers
+ * These registers are used to assist VF software discovery. For each HWVF, if the
+ * HWVF is mapped to a VF by RVU_PRIV_PF()_CFG[FIRST_HWVF,NVF], software
+ * writes NIX block's resource configuration for the VF in this register. The VF
+ * driver can read RVU_VF_BLOCK_ADDR()_DISC to discover the configuration.
+ */
+union cavm_rvu_priv_hwvfx_nixx_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_nixx_cfg_s {
+		u64 has_lf                           : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_nixx_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_NIXX_CFG(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_NIXX_CFG(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 255) && (b == 0)))
+		return 0x8001300 + 0x10000 * ((a) & 0xff) + 8 * ((b) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_npa_cfg
+ *
+ * RVU Privileged Hardware VF NPA Configuration Registers
+ * Similar to RVU_PRIV_HWVF()_NIX()_CFG, but for NPA block.
+ */
+union cavm_rvu_priv_hwvfx_npa_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_npa_cfg_s {
+		u64 has_lf                           : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_npa_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_NPA_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_NPA_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x8001310 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_sso_cfg
+ *
+ * RVU Privileged Hardware VF SSO Configuration Registers
+ * Similar to RVU_PRIV_HWVF()_NIX()_CFG, but for SSO block.
+ */
+union cavm_rvu_priv_hwvfx_sso_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_sso_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_sso_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_SSO_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_SSO_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x8001320 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_ssow_cfg
+ *
+ * RVU Privileged Hardware VF SSO Work Slot Configuration Registers
+ * Similar to RVU_PRIV_HWVF()_NIX()_CFG, but for SSOW block.
+ */
+union cavm_rvu_priv_hwvfx_ssow_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_ssow_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_ssow_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_SSOW_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_SSOW_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x8001330 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_hwvf#_tim_cfg
+ *
+ * RVU Privileged Hardware VF SSO Work Slot Configuration Registers
+ * Similar to RVU_PRIV_HWVF()_NIX()_CFG, but for TIM block.
+ */
+union cavm_rvu_priv_hwvfx_tim_cfg {
+	u64 u;
+	struct cavm_rvu_priv_hwvfx_tim_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_hwvfx_tim_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_HWVFX_TIM_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_HWVFX_TIM_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 255))
+		return 0x8001340 + 0x10000 * ((a) & 0xff);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_cfg
+ *
+ * RVU Privileged PF Configuration Registers
+ */
+union cavm_rvu_priv_pfx_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_cfg_s {
+		u64 first_hwvf                       : 12;
+		u64 nvf                              : 8;
+		u64 ena                              : 1;
+		u64 af_ena                           : 1;
+		u64 me_flr_ena                       : 1;
+		u64 reserved_23_63                   : 41;
+	} s;
+	/* struct cavm_rvu_priv_pfx_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000100 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_cpt#_cfg
+ *
+ * RVU Privileged PF CPT Configuration Registers
+ * Similar to RVU_PRIV_PF()_NIX()_CFG, but for CPT({a}) block.
+ */
+union cavm_rvu_priv_pfx_cptx_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_cptx_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_pfx_cptx_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_CPTX_CFG(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_CPTX_CFG(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b == 0)))
+		return 0x8000350 + 0x10000 * ((a) & 0xf) + 8 * ((b) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_id_cfg
+ *
+ * RVU Privileged PF ID Configuration Registers
+ */
+union cavm_rvu_priv_pfx_id_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_id_cfg_s {
+		u64 pf_devid                         : 8;
+		u64 vf_devid                         : 8;
+		u64 class_code                       : 24;
+		u64 reserved_40_63                   : 24;
+	} s;
+	/* struct cavm_rvu_priv_pfx_id_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_ID_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_ID_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000120 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_int_cfg
+ *
+ * RVU Privileged PF Interrupt Configuration Registers
+ */
+union cavm_rvu_priv_pfx_int_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_int_cfg_s {
+		u64 msix_offset                      : 11;
+		u64 reserved_11                      : 1;
+		u64 msix_size                        : 8;
+		u64 reserved_20_63                   : 44;
+	} s;
+	/* struct cavm_rvu_priv_pfx_int_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_INT_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_INT_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000200 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_msix_cfg
+ *
+ * RVU Privileged PF MSI-X Configuration Registers
+ * These registers specify MSI-X table sizes and locations for RVU PFs and
+ * associated VFs. Hardware maintains all RVU MSI-X tables in a contiguous memory
+ * region in LLC/DRAM called the MSI-X table region. The table region's base AF IOVA
+ * is specified by RVU_AF_MSIXTR_BASE, and its size as a multiple of
+ * 16-byte RVU_MSIX_VEC_S structures must be less than or equal to
+ * RVU_PRIV_CONST[MAX_MSIX].
+ *
+ * A PF's MSI-X table consists of the following range of RVU_MSIX_VEC_S structures
+ * in the table region:
+ * * First index: [PF_MSIXT_OFFSET].
+ * * Last index: [PF_MSIXT_OFFSET] + [PF_MSIXT_SIZEM1].
+ *
+ * If a PF has enabled VFs (associated RVU_PRIV_PF()_CFG[NVF] is nonzero),
+ * then each VF's MSI-X table consumes the following range of RVU_MSIX_VEC_S structures:
+ * * First index: [VF_MSIXT_OFFSET] + N*([VF_MSIXT_SIZEM1] + 1).
+ * * Last index: [VF_MSIXT_OFFSET] + N*([VF_MSIXT_SIZEM1] + 1) + [VF_MSIXT_SIZEM1].
+ *
+ * N=0 for the first VF, N=1 for the second VF, etc.
+ *
+ * Different PFs and VFs must have non-overlapping vector ranges, and the last
+ * index of any range must be less than RVU_PRIV_CONST[MAX_MSIX].
+ */
+union cavm_rvu_priv_pfx_msix_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_msix_cfg_s {
+		u64 vf_msixt_sizem1                  : 12;
+		u64 vf_msixt_offset                  : 20;
+		u64 pf_msixt_sizem1                  : 12;
+		u64 pf_msixt_offset                  : 20;
+	} s;
+	/* struct cavm_rvu_priv_pfx_msix_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_MSIX_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_MSIX_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000110 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_nix#_cfg
+ *
+ * RVU Privileged PF NIX Configuration Registers
+ * These registers are used to assist PF software discovery. For each enabled RVU
+ * PF, software writes the block's resource configuration for the PF in this
+ * register. The PF driver can read RVU_PF_BLOCK_ADDR()_DISC to discover the
+ * configuration.
+ */
+union cavm_rvu_priv_pfx_nixx_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_nixx_cfg_s {
+		u64 has_lf                           : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_priv_pfx_nixx_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_NIXX_CFG(u64 a, u64 b)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_NIXX_CFG(u64 a, u64 b)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a <= 15) && (b == 0)))
+		return 0x8000300 + 0x10000 * ((a) & 0xf) + 8 * ((b) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_npa_cfg
+ *
+ * RVU Privileged PF NPA Configuration Registers
+ * Similar to RVU_PRIV_PF()_NIX()_CFG, but for NPA block.
+ */
+union cavm_rvu_priv_pfx_npa_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_npa_cfg_s {
+		u64 has_lf                           : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_priv_pfx_npa_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_NPA_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_NPA_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000310 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_sso_cfg
+ *
+ * RVU Privileged PF SSO Configuration Registers
+ * Similar to RVU_PRIV_PF()_NIX()_CFG, but for SSO block.
+ */
+union cavm_rvu_priv_pfx_sso_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_sso_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_pfx_sso_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_SSO_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_SSO_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000320 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_ssow_cfg
+ *
+ * RVU Privileged PF SSO Work Slot Configuration Registers
+ * Similar to RVU_PRIV_PF()_NIX()_CFG, but for SSOW block.
+ */
+union cavm_rvu_priv_pfx_ssow_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_ssow_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_pfx_ssow_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_SSOW_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_SSOW_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000330 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_PF_BAR0) rvu_priv_pf#_tim_cfg
+ *
+ * RVU Privileged PF SSO Work Slot Configuration Registers
+ * Similar to RVU_PRIV_PF()_NIX()_CFG, but for TIM block.
+ */
+union cavm_rvu_priv_pfx_tim_cfg {
+	u64 u;
+	struct cavm_rvu_priv_pfx_tim_cfg_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_63                    : 55;
+	} s;
+	/* struct cavm_rvu_priv_pfx_tim_cfg_s cn; */
+};
+
+static inline u64 CAVM_RVU_PRIV_PFX_TIM_CFG(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_PRIV_PFX_TIM_CFG(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 15))
+		return 0x8000340 + 0x10000 * ((a) & 0xf);
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_block_addr#_disc
+ *
+ * RVU VF Block Address Discovery Registers
+ * These registers allow each VF driver to discover block resources that are
+ * provisioned to its VF. The register's block address index is enumerated by
+ * RVU_BLOCK_ADDR_E.
+ */
+union cavm_rvu_vf_block_addrx_disc {
+	u64 u;
+	struct cavm_rvu_vf_block_addrx_disc_s {
+		u64 num_lfs                          : 9;
+		u64 reserved_9_10                    : 2;
+		u64 imp                              : 1;
+		u64 rid                              : 8;
+		u64 btype                            : 8;
+		u64 reserved_28_63                   : 36;
+	} s;
+	/* struct cavm_rvu_vf_block_addrx_disc_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_BLOCK_ADDRX_DISC(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_BLOCK_ADDRX_DISC(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 31))
+		return 0x200 + 8 * ((a) & 0x1f);
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_int
+ *
+ * RVU VF Interrupt Registers
+ */
+union cavm_rvu_vf_int {
+	u64 u;
+	struct cavm_rvu_vf_int_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_vf_int_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_INT
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_INT
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x20;
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_int_ena_w1c
+ *
+ * RVU VF Interrupt Enable Clear Register
+ * This register clears interrupt enable bits.
+ */
+union cavm_rvu_vf_int_ena_w1c {
+	u64 u;
+	struct cavm_rvu_vf_int_ena_w1c_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_vf_int_ena_w1c_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_INT_ENA_W1C
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_INT_ENA_W1C
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x38;
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_int_ena_w1s
+ *
+ * RVU VF Interrupt Enable Set Register
+ * This register sets interrupt enable bits.
+ */
+union cavm_rvu_vf_int_ena_w1s {
+	u64 u;
+	struct cavm_rvu_vf_int_ena_w1s_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_vf_int_ena_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_INT_ENA_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_INT_ENA_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x30;
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_int_w1s
+ *
+ * RVU VF Interrupt Set Register
+ * This register sets interrupt bits.
+ */
+union cavm_rvu_vf_int_w1s {
+	u64 u;
+	struct cavm_rvu_vf_int_w1s_s {
+		u64 mbox                             : 1;
+		u64 reserved_1_63                    : 63;
+	} s;
+	/* struct cavm_rvu_vf_int_w1s_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_INT_W1S
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_INT_W1S
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX))
+		return 0x28;
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_msix_pba#
+ *
+ * RVU VF MSI-X Pending-Bit-Array Registers
+ * This register is the MSI-X VF PBA table.
+ */
+union cavm_rvu_vf_msix_pbax {
+	u64 u;
+	struct cavm_rvu_vf_msix_pbax_s {
+		u64 pend                             : 64;
+	} s;
+	/* struct cavm_rvu_vf_msix_pbax_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_MSIX_PBAX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_MSIX_PBAX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0xf0000 + 8 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_msix_vec#_addr
+ *
+ * RVU VF MSI-X Vector-Table Address Registers
+ * These registers and RVU_VF_MSIX_VEC()_CTL form the VF MSI-X vector table.
+ * The number of MSI-X vectors for a given VF is specified by
+ * RVU_PRIV_PF()_MSIX_CFG[VF_MSIXT_SIZEM1] (plus 1).
+ *
+ * Internal:
+ * VF vector count of 128 allows up to that number to be provisioned to the VF
+ * from LF resources of various blocks.
+ */
+union cavm_rvu_vf_msix_vecx_addr {
+	u64 u;
+	struct cavm_rvu_vf_msix_vecx_addr_s {
+		u64 secvec                           : 1;
+		u64 reserved_1                       : 1;
+		u64 addr                             : 51;
+		u64 reserved_53_63                   : 11;
+	} s;
+	/* struct cavm_rvu_vf_msix_vecx_addr_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_MSIX_VECX_ADDR(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_MSIX_VECX_ADDR(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0x80000 + 0x10 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_msix_vec#_ctl
+ *
+ * RVU VF MSI-X Vector-Table Control and Data Registers
+ * These registers and RVU_VF_MSIX_VEC()_ADDR form the VF MSI-X vector table.
+ */
+union cavm_rvu_vf_msix_vecx_ctl {
+	u64 u;
+	struct cavm_rvu_vf_msix_vecx_ctl_s {
+		u64 data                             : 32;
+		u64 mask                             : 1;
+		u64 reserved_33_63                   : 31;
+	} s;
+	/* struct cavm_rvu_vf_msix_vecx_ctl_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_MSIX_VECX_CTL(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_MSIX_VECX_CTL(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a == 0))
+		return 0x80008 + 0x10 * ((a) & 0x0);
+	return -1;
+}
+
+/**
+ * Register (RVU_VF_BAR2) rvu_vf_vfpf_mbox#
+ *
+ * RVU VF/PF Mailbox Registers
+ */
+union cavm_rvu_vf_vfpf_mboxx {
+	u64 u;
+	struct cavm_rvu_vf_vfpf_mboxx_s {
+		u64 data                             : 64;
+	} s;
+	/* struct cavm_rvu_vf_vfpf_mboxx_s cn; */
+};
+
+static inline u64 CAVM_RVU_VF_VFPF_MBOXX(u64 a)
+	__attribute__ ((pure, always_inline));
+static inline u64 CAVM_RVU_VF_VFPF_MBOXX(u64 a)
+{
+	if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a <= 1))
+		return 0 + 8 * ((a) & 0x1);
+	return -1;
+}
+
+#endif /* __CAVM_CSRS_RVU_H__ */
-- 
2.29.0

